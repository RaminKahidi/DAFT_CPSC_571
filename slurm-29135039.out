==============================================
Hostname: fg1
Job ID: 29135039
Node: fg1
True
Training completed.
MONAI version: 1.1.0
Numpy version: 1.19.2
Pytorch version: 1.13.1+cu117
MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False
MONAI rev id: a2ec3752f54bfc3b40e7952234fbeb5452ed63e3
MONAI __file__: /home/ramin.kahidi/software/miniconda3/envs/pytorch/lib/python3.7/site-packages/monai/__init__.py

Optional dependencies:
Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.
Nibabel version: NOT INSTALLED or UNKNOWN VERSION.
scikit-image version: NOT INSTALLED or UNKNOWN VERSION.
Pillow version: 9.4.0
Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.
gdown version: NOT INSTALLED or UNKNOWN VERSION.
TorchVision version: 0.13.1a0
tqdm version: 4.66.2
lmdb version: NOT INSTALLED or UNKNOWN VERSION.
psutil version: NOT INSTALLED or UNKNOWN VERSION.
pandas version: 1.3.5
einops version: NOT INSTALLED or UNKNOWN VERSION.
transformers version: NOT INSTALLED or UNKNOWN VERSION.
mlflow version: NOT INSTALLED or UNKNOWN VERSION.
pynrrd version: NOT INSTALLED or UNKNOWN VERSION.

For details about installing the optional dependencies, please visit:
    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies

DAFT
Tensor filenames:  ['Breast_MRI_001', 'Breast_MRI_002', 'Breast_MRI_003', 'Breast_MRI_004', 'Breast_MRI_005', 'Breast_MRI_006', 'Breast_MRI_007', 'Breast_MRI_008', 'Breast_MRI_009', 'Breast_MRI_010', 'Breast_MRI_011', 'Breast_MRI_012', 'Breast_MRI_013', 'Breast_MRI_014', 'Breast_MRI_015', 'Breast_MRI_016', 'Breast_MRI_017', 'Breast_MRI_018', 'Breast_MRI_019', 'Breast_MRI_020', 'Breast_MRI_021', 'Breast_MRI_022', 'Breast_MRI_023', 'Breast_MRI_024', 'Breast_MRI_025', 'Breast_MRI_026', 'Breast_MRI_027', 'Breast_MRI_028', 'Breast_MRI_029', 'Breast_MRI_030', 'Breast_MRI_031', 'Breast_MRI_032', 'Breast_MRI_033', 'Breast_MRI_034', 'Breast_MRI_035', 'Breast_MRI_036', 'Breast_MRI_037', 'Breast_MRI_038', 'Breast_MRI_039', 'Breast_MRI_040', 'Breast_MRI_041', 'Breast_MRI_042', 'Breast_MRI_043', 'Breast_MRI_044', 'Breast_MRI_045', 'Breast_MRI_046', 'Breast_MRI_047', 'Breast_MRI_048', 'Breast_MRI_049', 'Breast_MRI_050', 'Breast_MRI_051', 'Breast_MRI_052', 'Breast_MRI_053', 'Breast_MRI_054', 'Breast_MRI_055', 'Breast_MRI_056', 'Breast_MRI_057', 'Breast_MRI_058', 'Breast_MRI_059', 'Breast_MRI_060', 'Breast_MRI_061', 'Breast_MRI_062', 'Breast_MRI_063', 'Breast_MRI_064', 'Breast_MRI_065', 'Breast_MRI_066', 'Breast_MRI_067', 'Breast_MRI_068', 'Breast_MRI_069', 'Breast_MRI_070', 'Breast_MRI_071', 'Breast_MRI_072', 'Breast_MRI_073', 'Breast_MRI_074', 'Breast_MRI_075', 'Breast_MRI_076', 'Breast_MRI_077', 'Breast_MRI_078', 'Breast_MRI_079', 'Breast_MRI_080', 'Breast_MRI_081', 'Breast_MRI_082', 'Breast_MRI_083', 'Breast_MRI_084', 'Breast_MRI_085', 'Breast_MRI_086', 'Breast_MRI_087', 'Breast_MRI_088', 'Breast_MRI_089', 'Breast_MRI_090', 'Breast_MRI_091', 'Breast_MRI_092', 'Breast_MRI_093', 'Breast_MRI_094', 'Breast_MRI_095', 'Breast_MRI_096', 'Breast_MRI_097', 'Breast_MRI_098', 'Breast_MRI_099', 'Breast_MRI_100', 'Breast_MRI_101', 'Breast_MRI_102', 'Breast_MRI_103', 'Breast_MRI_104', 'Breast_MRI_105', 'Breast_MRI_106', 'Breast_MRI_107', 'Breast_MRI_108', 'Breast_MRI_109', 'Breast_MRI_110', 'Breast_MRI_111', 'Breast_MRI_112', 'Breast_MRI_113', 'Breast_MRI_114', 'Breast_MRI_115', 'Breast_MRI_116', 'Breast_MRI_117', 'Breast_MRI_118', 'Breast_MRI_119', 'Breast_MRI_121', 'Breast_MRI_122', 'Breast_MRI_123', 'Breast_MRI_124', 'Breast_MRI_125', 'Breast_MRI_126', 'Breast_MRI_127', 'Breast_MRI_128', 'Breast_MRI_129', 'Breast_MRI_131', 'Breast_MRI_132', 'Breast_MRI_133', 'Breast_MRI_134', 'Breast_MRI_135', 'Breast_MRI_136', 'Breast_MRI_137', 'Breast_MRI_138', 'Breast_MRI_139', 'Breast_MRI_140', 'Breast_MRI_141', 'Breast_MRI_142', 'Breast_MRI_143', 'Breast_MRI_144', 'Breast_MRI_145', 'Breast_MRI_146', 'Breast_MRI_147', 'Breast_MRI_148', 'Breast_MRI_149', 'Breast_MRI_150', 'Breast_MRI_151', 'Breast_MRI_152', 'Breast_MRI_153', 'Breast_MRI_154', 'Breast_MRI_155', 'Breast_MRI_156', 'Breast_MRI_157', 'Breast_MRI_158', 'Breast_MRI_159', 'Breast_MRI_160', 'Breast_MRI_161', 'Breast_MRI_162', 'Breast_MRI_163', 'Breast_MRI_164', 'Breast_MRI_165', 'Breast_MRI_166', 'Breast_MRI_168', 'Breast_MRI_169', 'Breast_MRI_170', 'Breast_MRI_171', 'Breast_MRI_172', 'Breast_MRI_173', 'Breast_MRI_174', 'Breast_MRI_175', 'Breast_MRI_176', 'Breast_MRI_177', 'Breast_MRI_178', 'Breast_MRI_179', 'Breast_MRI_180', 'Breast_MRI_181', 'Breast_MRI_182', 'Breast_MRI_183', 'Breast_MRI_184', 'Breast_MRI_185', 'Breast_MRI_186', 'Breast_MRI_187', 'Breast_MRI_188', 'Breast_MRI_189', 'Breast_MRI_190', 'Breast_MRI_191', 'Breast_MRI_192', 'Breast_MRI_193', 'Breast_MRI_194', 'Breast_MRI_195', 'Breast_MRI_196', 'Breast_MRI_197', 'Breast_MRI_198', 'Breast_MRI_199', 'Breast_MRI_200', 'Breast_MRI_201', 'Breast_MRI_202', 'Breast_MRI_203', 'Breast_MRI_204', 'Breast_MRI_205', 'Breast_MRI_206', 'Breast_MRI_207', 'Breast_MRI_208', 'Breast_MRI_209', 'Breast_MRI_210', 'Breast_MRI_211', 'Breast_MRI_212', 'Breast_MRI_213', 'Breast_MRI_214', 'Breast_MRI_215', 'Breast_MRI_216', 'Breast_MRI_217', 'Breast_MRI_218', 'Breast_MRI_219', 'Breast_MRI_220', 'Breast_MRI_221', 'Breast_MRI_222', 'Breast_MRI_223', 'Breast_MRI_224', 'Breast_MRI_225', 'Breast_MRI_226', 'Breast_MRI_227', 'Breast_MRI_228', 'Breast_MRI_229', 'Breast_MRI_230', 'Breast_MRI_231', 'Breast_MRI_232', 'Breast_MRI_233', 'Breast_MRI_234', 'Breast_MRI_235', 'Breast_MRI_236', 'Breast_MRI_237', 'Breast_MRI_238', 'Breast_MRI_239', 'Breast_MRI_240', 'Breast_MRI_241', 'Breast_MRI_242', 'Breast_MRI_243', 'Breast_MRI_244', 'Breast_MRI_245', 'Breast_MRI_246', 'Breast_MRI_247', 'Breast_MRI_248', 'Breast_MRI_249', 'Breast_MRI_251', 'Breast_MRI_252', 'Breast_MRI_253', 'Breast_MRI_254', 'Breast_MRI_255', 'Breast_MRI_256', 'Breast_MRI_257', 'Breast_MRI_258', 'Breast_MRI_259', 'Breast_MRI_260', 'Breast_MRI_261', 'Breast_MRI_262', 'Breast_MRI_263', 'Breast_MRI_264', 'Breast_MRI_265', 'Breast_MRI_266', 'Breast_MRI_267', 'Breast_MRI_268', 'Breast_MRI_269', 'Breast_MRI_270', 'Breast_MRI_271', 'Breast_MRI_272', 'Breast_MRI_273', 'Breast_MRI_274', 'Breast_MRI_275', 'Breast_MRI_276', 'Breast_MRI_277', 'Breast_MRI_278', 'Breast_MRI_279', 'Breast_MRI_280', 'Breast_MRI_281', 'Breast_MRI_282', 'Breast_MRI_283', 'Breast_MRI_284', 'Breast_MRI_285', 'Breast_MRI_286', 'Breast_MRI_287', 'Breast_MRI_288', 'Breast_MRI_289', 'Breast_MRI_290', 'Breast_MRI_291', 'Breast_MRI_292', 'Breast_MRI_293', 'Breast_MRI_294', 'Breast_MRI_295', 'Breast_MRI_296', 'Breast_MRI_297', 'Breast_MRI_298', 'Breast_MRI_299', 'Breast_MRI_301', 'Breast_MRI_302', 'Breast_MRI_303', 'Breast_MRI_304', 'Breast_MRI_305', 'Breast_MRI_306', 'Breast_MRI_307', 'Breast_MRI_308', 'Breast_MRI_309', 'Breast_MRI_310', 'Breast_MRI_311', 'Breast_MRI_312', 'Breast_MRI_313', 'Breast_MRI_314', 'Breast_MRI_315', 'Breast_MRI_316', 'Breast_MRI_317', 'Breast_MRI_318', 'Breast_MRI_319', 'Breast_MRI_509', 'Breast_MRI_320', 'Breast_MRI_321', 'Breast_MRI_322', 'Breast_MRI_323', 'Breast_MRI_324', 'Breast_MRI_325', 'Breast_MRI_326', 'Breast_MRI_327', 'Breast_MRI_328', 'Breast_MRI_329', 'Breast_MRI_330', 'Breast_MRI_331', 'Breast_MRI_332', 'Breast_MRI_333', 'Breast_MRI_334', 'Breast_MRI_335', 'Breast_MRI_336', 'Breast_MRI_337', 'Breast_MRI_338', 'Breast_MRI_339', 'Breast_MRI_340', 'Breast_MRI_341', 'Breast_MRI_342', 'Breast_MRI_343', 'Breast_MRI_344', 'Breast_MRI_345', 'Breast_MRI_346', 'Breast_MRI_347', 'Breast_MRI_348', 'Breast_MRI_349', 'Breast_MRI_350', 'Breast_MRI_351', 'Breast_MRI_352', 'Breast_MRI_353', 'Breast_MRI_354', 'Breast_MRI_355', 'Breast_MRI_356', 'Breast_MRI_357', 'Breast_MRI_358', 'Breast_MRI_359', 'Breast_MRI_360', 'Breast_MRI_361', 'Breast_MRI_362', 'Breast_MRI_363', 'Breast_MRI_364', 'Breast_MRI_365', 'Breast_MRI_366', 'Breast_MRI_367', 'Breast_MRI_368', 'Breast_MRI_369', 'Breast_MRI_370', 'Breast_MRI_371', 'Breast_MRI_372', 'Breast_MRI_373', 'Breast_MRI_374', 'Breast_MRI_375', 'Breast_MRI_376', 'Breast_MRI_377', 'Breast_MRI_378', 'Breast_MRI_379', 'Breast_MRI_380', 'Breast_MRI_381', 'Breast_MRI_382', 'Breast_MRI_383', 'Breast_MRI_384', 'Breast_MRI_385', 'Breast_MRI_386', 'Breast_MRI_387', 'Breast_MRI_388', 'Breast_MRI_389', 'Breast_MRI_390', 'Breast_MRI_391', 'Breast_MRI_392', 'Breast_MRI_393', 'Breast_MRI_394', 'Breast_MRI_395', 'Breast_MRI_396', 'Breast_MRI_397', 'Breast_MRI_398', 'Breast_MRI_399', 'Breast_MRI_400', 'Breast_MRI_401', 'Breast_MRI_402', 'Breast_MRI_403', 'Breast_MRI_404', 'Breast_MRI_405', 'Breast_MRI_406', 'Breast_MRI_407', 'Breast_MRI_408', 'Breast_MRI_409', 'Breast_MRI_410', 'Breast_MRI_411', 'Breast_MRI_412', 'Breast_MRI_413', 'Breast_MRI_414', 'Breast_MRI_415', 'Breast_MRI_416', 'Breast_MRI_417', 'Breast_MRI_418', 'Breast_MRI_419', 'Breast_MRI_420', 'Breast_MRI_421', 'Breast_MRI_422', 'Breast_MRI_423', 'Breast_MRI_424', 'Breast_MRI_425', 'Breast_MRI_426', 'Breast_MRI_427', 'Breast_MRI_428', 'Breast_MRI_429', 'Breast_MRI_430', 'Breast_MRI_431', 'Breast_MRI_432', 'Breast_MRI_433', 'Breast_MRI_434', 'Breast_MRI_435', 'Breast_MRI_436', 'Breast_MRI_437', 'Breast_MRI_438', 'Breast_MRI_439', 'Breast_MRI_440', 'Breast_MRI_441', 'Breast_MRI_442', 'Breast_MRI_443', 'Breast_MRI_444', 'Breast_MRI_445', 'Breast_MRI_446', 'Breast_MRI_447', 'Breast_MRI_448', 'Breast_MRI_449', 'Breast_MRI_450', 'Breast_MRI_451', 'Breast_MRI_452', 'Breast_MRI_453', 'Breast_MRI_454', 'Breast_MRI_455', 'Breast_MRI_456', 'Breast_MRI_457', 'Breast_MRI_458', 'Breast_MRI_459', 'Breast_MRI_460', 'Breast_MRI_461', 'Breast_MRI_462', 'Breast_MRI_463', 'Breast_MRI_464', 'Breast_MRI_465', 'Breast_MRI_466', 'Breast_MRI_467', 'Breast_MRI_468', 'Breast_MRI_469', 'Breast_MRI_470', 'Breast_MRI_471', 'Breast_MRI_472', 'Breast_MRI_473', 'Breast_MRI_474', 'Breast_MRI_475', 'Breast_MRI_476', 'Breast_MRI_477', 'Breast_MRI_478', 'Breast_MRI_479', 'Breast_MRI_480', 'Breast_MRI_481', 'Breast_MRI_482', 'Breast_MRI_483', 'Breast_MRI_484', 'Breast_MRI_485', 'Breast_MRI_486', 'Breast_MRI_487', 'Breast_MRI_488', 'Breast_MRI_489', 'Breast_MRI_490', 'Breast_MRI_491', 'Breast_MRI_492', 'Breast_MRI_493', 'Breast_MRI_494', 'Breast_MRI_495', 'Breast_MRI_496', 'Breast_MRI_497', 'Breast_MRI_498', 'Breast_MRI_499', 'Breast_MRI_500', 'Breast_MRI_501', 'Breast_MRI_502', 'Breast_MRI_503', 'Breast_MRI_504', 'Breast_MRI_505', 'Breast_MRI_506', 'Breast_MRI_507', 'Breast_MRI_508', 'Breast_MRI_510', 'Breast_MRI_511', 'Breast_MRI_512', 'Breast_MRI_513', 'Breast_MRI_514', 'Breast_MRI_515', 'Breast_MRI_516', 'Breast_MRI_517', 'Breast_MRI_518', 'Breast_MRI_519', 'Breast_MRI_520', 'Breast_MRI_521', 'Breast_MRI_522', 'Breast_MRI_523', 'Breast_MRI_524', 'Breast_MRI_525', 'Breast_MRI_526', 'Breast_MRI_527', 'Breast_MRI_528', 'Breast_MRI_529', 'Breast_MRI_530', 'Breast_MRI_531', 'Breast_MRI_532', 'Breast_MRI_533', 'Breast_MRI_534', 'Breast_MRI_535', 'Breast_MRI_536', 'Breast_MRI_537', 'Breast_MRI_538', 'Breast_MRI_539', 'Breast_MRI_540', 'Breast_MRI_541', 'Breast_MRI_542', 'Breast_MRI_543', 'Breast_MRI_544', 'Breast_MRI_545', 'Breast_MRI_546', 'Breast_MRI_547', 'Breast_MRI_548', 'Breast_MRI_549', 'Breast_MRI_550', 'Breast_MRI_551', 'Breast_MRI_552', 'Breast_MRI_553', 'Breast_MRI_554', 'Breast_MRI_555', 'Breast_MRI_556', 'Breast_MRI_557', 'Breast_MRI_558', 'Breast_MRI_559', 'Breast_MRI_560', 'Breast_MRI_561', 'Breast_MRI_562', 'Breast_MRI_563', 'Breast_MRI_564', 'Breast_MRI_565', 'Breast_MRI_566', 'Breast_MRI_567', 'Breast_MRI_568', 'Breast_MRI_569', 'Breast_MRI_570', 'Breast_MRI_571', 'Breast_MRI_572', 'Breast_MRI_573', 'Breast_MRI_574', 'Breast_MRI_575', 'Breast_MRI_576', 'Breast_MRI_578', 'Breast_MRI_579', 'Breast_MRI_580', 'Breast_MRI_581', 'Breast_MRI_582', 'Breast_MRI_583', 'Breast_MRI_584', 'Breast_MRI_585', 'Breast_MRI_586', 'Breast_MRI_587', 'Breast_MRI_588', 'Breast_MRI_589', 'Breast_MRI_590', 'Breast_MRI_591', 'Breast_MRI_592', 'Breast_MRI_593', 'Breast_MRI_594', 'Breast_MRI_595', 'Breast_MRI_596', 'Breast_MRI_597', 'Breast_MRI_598', 'Breast_MRI_599', 'Breast_MRI_600', 'Breast_MRI_601', 'Breast_MRI_602', 'Breast_MRI_603', 'Breast_MRI_604', 'Breast_MRI_605', 'Breast_MRI_606', 'Breast_MRI_607', 'Breast_MRI_608', 'Breast_MRI_609', 'Breast_MRI_610', 'Breast_MRI_611', 'Breast_MRI_612', 'Breast_MRI_613', 'Breast_MRI_614', 'Breast_MRI_615', 'Breast_MRI_616', 'Breast_MRI_617', 'Breast_MRI_618', 'Breast_MRI_619', 'Breast_MRI_620', 'Breast_MRI_621', 'Breast_MRI_622', 'Breast_MRI_623', 'Breast_MRI_624', 'Breast_MRI_625', 'Breast_MRI_626', 'Breast_MRI_627', 'Breast_MRI_628', 'Breast_MRI_629', 'Breast_MRI_630', 'Breast_MRI_631', 'Breast_MRI_632', 'Breast_MRI_633', 'Breast_MRI_634', 'Breast_MRI_635', 'Breast_MRI_636', 'Breast_MRI_637', 'Breast_MRI_638', 'Breast_MRI_639', 'Breast_MRI_640', 'Breast_MRI_641', 'Breast_MRI_642', 'Breast_MRI_643', 'Breast_MRI_644', 'Breast_MRI_645', 'Breast_MRI_647', 'Breast_MRI_648', 'Breast_MRI_649', 'Breast_MRI_650', 'Breast_MRI_651', 'Breast_MRI_652', 'Breast_MRI_653', 'Breast_MRI_654', 'Breast_MRI_655', 'Breast_MRI_656', 'Breast_MRI_657', 'Breast_MRI_658', 'Breast_MRI_659', 'Breast_MRI_660', 'Breast_MRI_661', 'Breast_MRI_662', 'Breast_MRI_663', 'Breast_MRI_664', 'Breast_MRI_665', 'Breast_MRI_666', 'Breast_MRI_667', 'Breast_MRI_668', 'Breast_MRI_669', 'Breast_MRI_670', 'Breast_MRI_672', 'Breast_MRI_673', 'Breast_MRI_674', 'Breast_MRI_675', 'Breast_MRI_676', 'Breast_MRI_677', 'Breast_MRI_678', 'Breast_MRI_679', 'Breast_MRI_680', 'Breast_MRI_681', 'Breast_MRI_682', 'Breast_MRI_683', 'Breast_MRI_684', 'Breast_MRI_685', 'Breast_MRI_686', 'Breast_MRI_687', 'Breast_MRI_688', 'Breast_MRI_689', 'Breast_MRI_690', 'Breast_MRI_691', 'Breast_MRI_692', 'Breast_MRI_693', 'Breast_MRI_694', 'Breast_MRI_695', 'Breast_MRI_696', 'Breast_MRI_697', 'Breast_MRI_698', 'Breast_MRI_699', 'Breast_MRI_700', 'Breast_MRI_701', 'Breast_MRI_702', 'Breast_MRI_703', 'Breast_MRI_704', 'Breast_MRI_705', 'Breast_MRI_706', 'Breast_MRI_707', 'Breast_MRI_708', 'Breast_MRI_709', 'Breast_MRI_710', 'Breast_MRI_711', 'Breast_MRI_712', 'Breast_MRI_713', 'Breast_MRI_714', 'Breast_MRI_715', 'Breast_MRI_716', 'Breast_MRI_717', 'Breast_MRI_718', 'Breast_MRI_719', 'Breast_MRI_720', 'Breast_MRI_721', 'Breast_MRI_722', 'Breast_MRI_723', 'Breast_MRI_724', 'Breast_MRI_725', 'Breast_MRI_726', 'Breast_MRI_727', 'Breast_MRI_728', 'Breast_MRI_729', 'Breast_MRI_730', 'Breast_MRI_731', 'Breast_MRI_732', 'Breast_MRI_733', 'Breast_MRI_734', 'Breast_MRI_735', 'Breast_MRI_736', 'Breast_MRI_737', 'Breast_MRI_738', 'Breast_MRI_739', 'Breast_MRI_740', 'Breast_MRI_741', 'Breast_MRI_742', 'Breast_MRI_743', 'Breast_MRI_744', 'Breast_MRI_745', 'Breast_MRI_746', 'Breast_MRI_747', 'Breast_MRI_748', 'Breast_MRI_749', 'Breast_MRI_750', 'Breast_MRI_751', 'Breast_MRI_752', 'Breast_MRI_753', 'Breast_MRI_754', 'Breast_MRI_755', 'Breast_MRI_756', 'Breast_MRI_757', 'Breast_MRI_758', 'Breast_MRI_759', 'Breast_MRI_760', 'Breast_MRI_761', 'Breast_MRI_762', 'Breast_MRI_763', 'Breast_MRI_764', 'Breast_MRI_765', 'Breast_MRI_766', 'Breast_MRI_767', 'Breast_MRI_768', 'Breast_MRI_769', 'Breast_MRI_770', 'Breast_MRI_771', 'Breast_MRI_772', 'Breast_MRI_773', 'Breast_MRI_774', 'Breast_MRI_775', 'Breast_MRI_776', 'Breast_MRI_777', 'Breast_MRI_778', 'Breast_MRI_779', 'Breast_MRI_780', 'Breast_MRI_781', 'Breast_MRI_782', 'Breast_MRI_783', 'Breast_MRI_784', 'Breast_MRI_785', 'Breast_MRI_786', 'Breast_MRI_787', 'Breast_MRI_788', 'Breast_MRI_789', 'Breast_MRI_790', 'Breast_MRI_791', 'Breast_MRI_792', 'Breast_MRI_793', 'Breast_MRI_794', 'Breast_MRI_795', 'Breast_MRI_796', 'Breast_MRI_797', 'Breast_MRI_798', 'Breast_MRI_799', 'Breast_MRI_800', 'Breast_MRI_801', 'Breast_MRI_802', 'Breast_MRI_803', 'Breast_MRI_804', 'Breast_MRI_805', 'Breast_MRI_806', 'Breast_MRI_807', 'Breast_MRI_808', 'Breast_MRI_809', 'Breast_MRI_810', 'Breast_MRI_811', 'Breast_MRI_812', 'Breast_MRI_813', 'Breast_MRI_814', 'Breast_MRI_815', 'Breast_MRI_816', 'Breast_MRI_817', 'Breast_MRI_818', 'Breast_MRI_819', 'Breast_MRI_820', 'Breast_MRI_821', 'Breast_MRI_822', 'Breast_MRI_823', 'Breast_MRI_824', 'Breast_MRI_825', 'Breast_MRI_826', 'Breast_MRI_827', 'Breast_MRI_828', 'Breast_MRI_829', 'Breast_MRI_830', 'Breast_MRI_831', 'Breast_MRI_832', 'Breast_MRI_833', 'Breast_MRI_834', 'Breast_MRI_835', 'Breast_MRI_836', 'Breast_MRI_837', 'Breast_MRI_838', 'Breast_MRI_839', 'Breast_MRI_840', 'Breast_MRI_841', 'Breast_MRI_842', 'Breast_MRI_843', 'Breast_MRI_844', 'Breast_MRI_845', 'Breast_MRI_846', 'Breast_MRI_847', 'Breast_MRI_848', 'Breast_MRI_849', 'Breast_MRI_850', 'Breast_MRI_851', 'Breast_MRI_852', 'Breast_MRI_853', 'Breast_MRI_854', 'Breast_MRI_855', 'Breast_MRI_856', 'Breast_MRI_857', 'Breast_MRI_858', 'Breast_MRI_859', 'Breast_MRI_860', 'Breast_MRI_861', 'Breast_MRI_862', 'Breast_MRI_863', 'Breast_MRI_864', 'Breast_MRI_865', 'Breast_MRI_866', 'Breast_MRI_867', 'Breast_MRI_868', 'Breast_MRI_869', 'Breast_MRI_870', 'Breast_MRI_871', 'Breast_MRI_872', 'Breast_MRI_873', 'Breast_MRI_874', 'Breast_MRI_875', 'Breast_MRI_876', 'Breast_MRI_877', 'Breast_MRI_878', 'Breast_MRI_879', 'Breast_MRI_880', 'Breast_MRI_881', 'Breast_MRI_882', 'Breast_MRI_883', 'Breast_MRI_884', 'Breast_MRI_885', 'Breast_MRI_886', 'Breast_MRI_887', 'Breast_MRI_888', 'Breast_MRI_889', 'Breast_MRI_890', 'Breast_MRI_891', 'Breast_MRI_892', 'Breast_MRI_893', 'Breast_MRI_894', 'Breast_MRI_895', 'Breast_MRI_896', 'Breast_MRI_897', 'Breast_MRI_898', 'Breast_MRI_899', 'Breast_MRI_900', 'Breast_MRI_901', 'Breast_MRI_902', 'Breast_MRI_903', 'Breast_MRI_904', 'Breast_MRI_905', 'Breast_MRI_906', 'Breast_MRI_907', 'Breast_MRI_908', 'Breast_MRI_909', 'Breast_MRI_910', 'Breast_MRI_911', 'Breast_MRI_912', 'Breast_MRI_913', 'Breast_MRI_914', 'Breast_MRI_915', 'Breast_MRI_916', 'Breast_MRI_917', 'Breast_MRI_918', 'Breast_MRI_919', 'Breast_MRI_920', 'Breast_MRI_921', 'Breast_MRI_922']
row 0: Patient ID                                  Breast_MRI_001
Date of Birth (Days)                              0.705925
Image Position of Patient (Y)                     0.180863
Image Position of Patient (Z)                     0.746579
Image Position of Patient (X)                     0.068046
Days to MRI (From the Date of Diagnosis)          0.255952
TR (Repetition Time)                              0.150454
TE (Echo Time)                                    0.073041
Oncotype score                                    0.241581
Race and Ethnicity_1                                     0
Slice Thickness _18                                      0
Tumor Grade(M)\n(Mitotic)_1.0                            1
Tumor Grade(M)\n(Mitotic)_3.0                            0
Bilateral Information_0                                  1
Tumor Grade(T)\n(Tubule)_3.0                             1
FOV Computed (Field of View) in cm _11                   0
Tumor Progression_1.0                                    0
Tumor Progression_2.0                                    1
Tumor Progression_3.0                                    0
Tumor Progression_4.0                                    0
Name: 0, dtype: object
Epoch 1/50
/home/ramin.kahidi/software/miniconda3/envs/pytorch/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/ramin.kahidi/software/miniconda3/envs/pytorch/lib/python3.7/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
  warn(f"Failed to load image Python extension: {e}")
Training labels: tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [1., 0., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.057464599609375, 0.32958984375, -0.34814453125, 0.2122802734375]
[0.057891845703125, 0.53955078125, -0.60986328125, 0.415283203125]
[0.76611328125, -0.04852294921875, 0.297119140625, -0.20166015625]
[0.13330078125, 0.1834716796875, 0.072265625, -0.072265625]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.057464599609375, 0.32958984375, -0.34814453125, 0.2122802734375]
[0.057891845703125, 0.53955078125, -0.60986328125, 0.415283203125]
[0.76611328125, -0.04852294921875, 0.297119140625, -0.20166015625]
[0.13330078125, 0.1834716796875, 0.072265625, -0.072265625]
This is the real loss :  tensor(0.2493, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:   Epoch  Batch  ...      Loss                                         True Value
0     0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...

[1 rows x 5 columns]
Training labels: tensor([[0., 1., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[-0.059722900390625, 0.6669921875, -0.76318359375, 0.521484375]
[0.64990234375, -0.1229248046875, 0.25048828125, -0.280029296875]
[0.152587890625, 0.1693115234375, -0.05694580078125, 0.072509765625]
[0.1077880859375, 0.261474609375, 0.0443115234375, 0.0556640625]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[-0.059722900390625, 0.6669921875, -0.76318359375, 0.521484375]
[0.64990234375, -0.1229248046875, 0.25048828125, -0.280029296875]
[0.152587890625, 0.1693115234375, -0.05694580078125, 0.072509765625]
[0.1077880859375, 0.261474609375, 0.0443115234375, 0.0556640625]
This is the real loss :  tensor(0.1795, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:   Epoch  Batch  ...      Loss                                         True Value
0     0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1     0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...

[2 rows x 5 columns]
Training labels: tensor([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [1., 0., 0., 0.],
        [0., 1., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.54736328125, 0.256591796875, 0.11138916015625, 0.192138671875]
[0.486083984375, 0.194091796875, 0.06024169921875, 0.1932373046875]
[0.7900390625, -0.2371826171875, 0.057159423828125, 0.55419921875]
[-0.195556640625, 1.4453125, -0.288818359375, -0.1280517578125]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.54736328125, 0.256591796875, 0.11138916015625, 0.192138671875]
[0.486083984375, 0.194091796875, 0.06024169921875, 0.1932373046875]
[0.7900390625, -0.2371826171875, 0.057159423828125, 0.55419921875]
[-0.195556640625, 1.4453125, -0.288818359375, -0.1280517578125]
This is the real loss :  tensor(0.1777, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:   Epoch  Batch  ...      Loss                                         True Value
0     0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1     0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2     0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...

[3 rows x 5 columns]
Training labels: tensor([[1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.196533203125, 0.154541015625, -0.099365234375, 0.1646728515625]
[-0.0126190185546875, 1.0654296875, 0.0222625732421875, -0.09796142578125]
[0.2578125, 0.341796875, -0.0188751220703125, 0.0048065185546875]
[1.40625, -0.0306854248046875, 0.08868408203125, 0.5126953125]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.196533203125, 0.154541015625, -0.099365234375, 0.1646728515625]
[-0.0126190185546875, 1.0654296875, 0.0222625732421875, -0.09796142578125]
[0.2578125, 0.341796875, -0.0188751220703125, 0.0048065185546875]
[1.40625, -0.0306854248046875, 0.08868408203125, 0.5126953125]
This is the real loss :  tensor(0.4031, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:   Epoch  Batch  ...      Loss                                         True Value
0     0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1     0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2     0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3     0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...

[4 rows x 5 columns]
Training labels: tensor([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.493896484375, 0.3515625, -0.1910400390625, 0.04931640625]
[0.341796875, 0.2587890625, -0.10211181640625, -0.1114501953125]
[0.269775390625, 1.0224609375, -0.212890625, 0.3095703125]
[0.46240234375, 0.1607666015625, 0.2587890625, -0.0180206298828125]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.493896484375, 0.3515625, -0.1910400390625, 0.04931640625]
[0.341796875, 0.2587890625, -0.10211181640625, -0.1114501953125]
[0.269775390625, 1.0224609375, -0.212890625, 0.3095703125]
[0.46240234375, 0.1607666015625, 0.2587890625, -0.0180206298828125]
This is the real loss :  tensor(0.2635, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:   Epoch  Batch  ...      Loss                                         True Value
0     0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1     0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2     0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3     0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4     0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...

[5 rows x 5 columns]
Training labels: tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [1., 0., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.236083984375, 0.426025390625, -0.1961669921875, 0.30517578125]
[0.346435546875, 0.293701171875, -0.16748046875, 0.272216796875]
[0.60888671875, 0.26904296875, 0.392333984375, -0.260009765625]
[0.357666015625, 0.70166015625, 0.005374908447265625, 0.1905517578125]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.236083984375, 0.426025390625, -0.1961669921875, 0.30517578125]
[0.346435546875, 0.293701171875, -0.16748046875, 0.272216796875]
[0.60888671875, 0.26904296875, 0.392333984375, -0.260009765625]
[0.357666015625, 0.70166015625, 0.005374908447265625, 0.1905517578125]
This is the real loss :  tensor(0.2303, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:   Epoch  Batch  ...      Loss                                         True Value
0     0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1     0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2     0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3     0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4     0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5     0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...

[6 rows x 5 columns]
Training labels: tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.4560546875, 0.654296875, -0.265869140625, 0.1864013671875]
[0.47802734375, 0.384765625, 0.12841796875, 0.3251953125]
[0.0260009765625, 0.354736328125, -0.00800323486328125, -0.0914306640625]
[0.48974609375, 0.294677734375, 0.1307373046875, -0.1099853515625]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.4560546875, 0.654296875, -0.265869140625, 0.1864013671875]
[0.47802734375, 0.384765625, 0.12841796875, 0.3251953125]
[0.0260009765625, 0.354736328125, -0.00800323486328125, -0.0914306640625]
[0.48974609375, 0.294677734375, 0.1307373046875, -0.1099853515625]
This is the real loss :  tensor(0.2225, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:   Epoch  Batch  ...      Loss                                         True Value
0     0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1     0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2     0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3     0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4     0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5     0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6     0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...

[7 rows x 5 columns]
Training labels: tensor([[1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.26025390625, 0.255126953125, 0.122802734375, -0.1177978515625]
[0.2032470703125, 0.18115234375, 0.09063720703125, -0.041534423828125]
[0.689453125, 0.443603515625, -0.032958984375, 0.1983642578125]
[0.1695556640625, 0.24609375, -0.32421875, -0.059112548828125]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.26025390625, 0.255126953125, 0.122802734375, -0.1177978515625]
[0.2032470703125, 0.18115234375, 0.09063720703125, -0.041534423828125]
[0.689453125, 0.443603515625, -0.032958984375, 0.1983642578125]
[0.1695556640625, 0.24609375, -0.32421875, -0.059112548828125]
This is the real loss :  tensor(0.1781, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:   Epoch  Batch  ...      Loss                                         True Value
0     0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1     0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2     0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3     0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4     0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5     0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6     0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7     0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...

[8 rows x 5 columns]
Training labels: tensor([[0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.57958984375, 0.2042236328125, 0.201171875, -0.03338623046875]
[0.5947265625, 0.366943359375, -0.0654296875, 0.21630859375]
[0.0098419189453125, 0.49609375, 0.057861328125, -0.505859375]
[0.29736328125, 0.54541015625, -0.15234375, 0.280517578125]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.57958984375, 0.2042236328125, 0.201171875, -0.03338623046875]
[0.5947265625, 0.366943359375, -0.0654296875, 0.21630859375]
[0.0098419189453125, 0.49609375, 0.057861328125, -0.505859375]
[0.29736328125, 0.54541015625, -0.15234375, 0.280517578125]
This is the real loss :  tensor(0.2626, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:   Epoch  Batch  ...      Loss                                         True Value
0     0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1     0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2     0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3     0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4     0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5     0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6     0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7     0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8     0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...

[9 rows x 5 columns]
Training labels: tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.1824951171875, 0.607421875, 0.2216796875, 0.08599853515625]
[0.46435546875, 0.3623046875, -0.04754638671875, 0.064453125]
[0.28515625, 0.51953125, -0.03369140625, 0.146728515625]
[0.44580078125, 0.29150390625, 0.0210113525390625, 0.0029659271240234375]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.1824951171875, 0.607421875, 0.2216796875, 0.08599853515625]
[0.46435546875, 0.3623046875, -0.04754638671875, 0.064453125]
[0.28515625, 0.51953125, -0.03369140625, 0.146728515625]
[0.44580078125, 0.29150390625, 0.0210113525390625, 0.0029659271240234375]
This is the real loss :  tensor(0.1531, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:   Epoch  Batch  ...      Loss                                         True Value
0     0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1     0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2     0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3     0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4     0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5     0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6     0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7     0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8     0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9     0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...

[10 rows x 5 columns]
Training labels: tensor([[0., 1., 0., 0.],
        [1., 0., 0., 0.],
        [0., 0., 0., 1.],
        [1., 0., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.299072265625, 0.0289306640625, 0.11529541015625, -0.038116455078125]
[0.252197265625, 0.212158203125, 0.12445068359375, -0.1915283203125]
[0.4345703125, 0.435546875, -0.1473388671875, 0.12054443359375]
[0.26416015625, 0.370849609375, -0.1444091796875, 0.14501953125]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.299072265625, 0.0289306640625, 0.11529541015625, -0.038116455078125]
[0.252197265625, 0.212158203125, 0.12445068359375, -0.1915283203125]
[0.4345703125, 0.435546875, -0.1473388671875, 0.12054443359375]
[0.26416015625, 0.370849609375, -0.1444091796875, 0.14501953125]
This is the real loss :  tensor(0.2249, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...

[11 rows x 5 columns]
Training labels: tensor([[0., 1., 0., 0.],
        [0., 0., 0., 1.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.60302734375, 0.478271484375, 0.0014848709106445312, -0.03582763671875]
[0.32861328125, 0.28369140625, -0.013275146484375, 0.16162109375]
[0.3271484375, -0.10919189453125, 0.2147216796875, 0.006839752197265625]
[0.19677734375, 0.69580078125, -0.057830810546875, -0.00801849365234375]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.60302734375, 0.478271484375, 0.0014848709106445312, -0.03582763671875]
[0.32861328125, 0.28369140625, -0.013275146484375, 0.16162109375]
[0.3271484375, -0.10919189453125, 0.2147216796875, 0.006839752197265625]
[0.19677734375, 0.69580078125, -0.057830810546875, -0.00801849365234375]
This is the real loss :  tensor(0.1983, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...

[12 rows x 5 columns]
Training labels: tensor([[1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [1., 0., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.385498046875, 0.39697265625, -0.2376708984375, 0.262451171875]
[0.041015625, 0.1019287109375, 0.15771484375, -0.328369140625]
[0.38232421875, 0.498779296875, 0.0305328369140625, 0.1766357421875]
[0.407470703125, 0.432373046875, -0.1544189453125, 0.0716552734375]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.385498046875, 0.39697265625, -0.2376708984375, 0.262451171875]
[0.041015625, 0.1019287109375, 0.15771484375, -0.328369140625]
[0.38232421875, 0.498779296875, 0.0305328369140625, 0.1766357421875]
[0.407470703125, 0.432373046875, -0.1544189453125, 0.0716552734375]
This is the real loss :  tensor(0.1700, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...

[13 rows x 5 columns]
Training labels: tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [1., 0., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.421630859375, 0.45947265625, -0.044830322265625, 0.09344482421875]
[0.52001953125, 0.26171875, 0.1466064453125, 0.08892822265625]
[0.2042236328125, 0.332763671875, 0.0186920166015625, -0.0792236328125]
[0.4169921875, 0.356689453125, 0.0211639404296875, 0.05743408203125]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.421630859375, 0.45947265625, -0.044830322265625, 0.09344482421875]
[0.52001953125, 0.26171875, 0.1466064453125, 0.08892822265625]
[0.2042236328125, 0.332763671875, 0.0186920166015625, -0.0792236328125]
[0.4169921875, 0.356689453125, 0.0211639404296875, 0.05743408203125]
This is the real loss :  tensor(0.1431, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...

[14 rows x 5 columns]
Training labels: tensor([[0., 0., 0., 1.],
        [0., 1., 0., 0.],
        [1., 0., 0., 0.],
        [0., 1., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.307373046875, 0.50537109375, 0.0147247314453125, 0.0030574798583984375]
[0.50634765625, 0.345458984375, 0.277099609375, -0.016021728515625]
[0.5283203125, 0.483642578125, 0.1895751953125, -0.073486328125]
[0.385009765625, 0.2139892578125, -0.1446533203125, 0.3388671875]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.307373046875, 0.50537109375, 0.0147247314453125, 0.0030574798583984375]
[0.50634765625, 0.345458984375, 0.277099609375, -0.016021728515625]
[0.5283203125, 0.483642578125, 0.1895751953125, -0.073486328125]
[0.385009765625, 0.2139892578125, -0.1446533203125, 0.3388671875]
This is the real loss :  tensor(0.2191, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...

[15 rows x 5 columns]
Training labels: tensor([[0., 0., 1., 0.],
        [1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.317626953125, 0.252197265625, -0.10284423828125, 0.2171630859375]
[0.5966796875, 0.383544921875, 0.2392578125, -0.09173583984375]
[0.2529296875, 0.489013671875, -0.042877197265625, 0.04730224609375]
[0.5029296875, 0.439697265625, 0.044403076171875, 0.0017757415771484375]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.317626953125, 0.252197265625, -0.10284423828125, 0.2171630859375]
[0.5966796875, 0.383544921875, 0.2392578125, -0.09173583984375]
[0.2529296875, 0.489013671875, -0.042877197265625, 0.04730224609375]
[0.5029296875, 0.439697265625, 0.044403076171875, 0.0017757415771484375]
This is the real loss :  tensor(0.1688, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...

[16 rows x 5 columns]
Training labels: tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.2861328125, 0.1436767578125, 0.047210693359375, 0.3544921875]
[0.55419921875, 0.435546875, 0.1080322265625, -0.031097412109375]
[0.47705078125, 0.466552734375, 0.12841796875, -0.05023193359375]
[0.3408203125, 0.4287109375, 0.019989013671875, -0.01568603515625]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.2861328125, 0.1436767578125, 0.047210693359375, 0.3544921875]
[0.55419921875, 0.435546875, 0.1080322265625, -0.031097412109375]
[0.47705078125, 0.466552734375, 0.12841796875, -0.05023193359375]
[0.3408203125, 0.4287109375, 0.019989013671875, -0.01568603515625]
This is the real loss :  tensor(0.1597, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...

[17 rows x 5 columns]
Training labels: tensor([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.418212890625, 0.41064453125, 0.09906005859375, -0.0538330078125]
[0.393798828125, 0.3818359375, 0.1385498046875, -0.09527587890625]
[0.16015625, 0.2274169921875, 0.0036468505859375, 0.275634765625]
[0.54931640625, 0.529296875, -0.0673828125, 0.0908203125]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.418212890625, 0.41064453125, 0.09906005859375, -0.0538330078125]
[0.393798828125, 0.3818359375, 0.1385498046875, -0.09527587890625]
[0.16015625, 0.2274169921875, 0.0036468505859375, 0.275634765625]
[0.54931640625, 0.529296875, -0.0673828125, 0.0908203125]
This is the real loss :  tensor(0.1764, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...

[18 rows x 5 columns]
Training labels: tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [1., 0., 0., 0.],
        [0., 1., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.46533203125, 0.433837890625, 0.0889892578125, -0.051666259765625]
[0.401123046875, 0.322509765625, 0.1492919921875, -0.1402587890625]
[0.241455078125, 0.368408203125, 0.06298828125, 0.40771484375]
[0.445556640625, 0.626953125, -0.06329345703125, 0.106201171875]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.46533203125, 0.433837890625, 0.0889892578125, -0.051666259765625]
[0.401123046875, 0.322509765625, 0.1492919921875, -0.1402587890625]
[0.241455078125, 0.368408203125, 0.06298828125, 0.40771484375]
[0.445556640625, 0.626953125, -0.06329345703125, 0.106201171875]
This is the real loss :  tensor(0.1488, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...

[19 rows x 5 columns]
Training labels: tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [1., 0., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.49267578125, 0.50732421875, 0.0985107421875, -0.04669189453125]
[0.383544921875, 0.4345703125, 0.08770751953125, -0.053558349609375]
[0.468017578125, 0.7705078125, 0.046783447265625, -0.0421142578125]
[0.134521484375, 0.2415771484375, 0.12347412109375, 0.294189453125]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.49267578125, 0.50732421875, 0.0985107421875, -0.04669189453125]
[0.383544921875, 0.4345703125, 0.08770751953125, -0.053558349609375]
[0.468017578125, 0.7705078125, 0.046783447265625, -0.0421142578125]
[0.134521484375, 0.2415771484375, 0.12347412109375, 0.294189453125]
This is the real loss :  tensor(0.1350, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...

[20 rows x 5 columns]
Training labels: tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.51416015625, 0.56640625, 0.063720703125, 0.0206146240234375]
[0.267578125, 0.2252197265625, 0.2176513671875, 0.1746826171875]
[0.346923828125, 0.6416015625, 0.055633544921875, -0.032745361328125]
[0.34423828125, 0.5224609375, 0.04833984375, 0.004608154296875]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.51416015625, 0.56640625, 0.063720703125, 0.0206146240234375]
[0.267578125, 0.2252197265625, 0.2176513671875, 0.1746826171875]
[0.346923828125, 0.6416015625, 0.055633544921875, -0.032745361328125]
[0.34423828125, 0.5224609375, 0.04833984375, 0.004608154296875]
This is the real loss :  tensor(0.1721, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...

[21 rows x 5 columns]
Training labels: tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [1., 0., 0., 0.],
        [0., 1., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.27392578125, 0.246826171875, 0.227294921875, 0.07025146484375]
[0.327880859375, 0.67626953125, 0.007488250732421875, 0.151123046875]
[0.51123046875, 0.54052734375, 0.084716796875, -0.0941162109375]
[0.434814453125, 0.5693359375, 0.1177978515625, -0.042327880859375]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.27392578125, 0.246826171875, 0.227294921875, 0.07025146484375]
[0.327880859375, 0.67626953125, 0.007488250732421875, 0.151123046875]
[0.51123046875, 0.54052734375, 0.084716796875, -0.0941162109375]
[0.434814453125, 0.5693359375, 0.1177978515625, -0.042327880859375]
This is the real loss :  tensor(0.1170, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...

[22 rows x 5 columns]
Training labels: tensor([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.298828125, 0.5, 0.07025146484375, 0.060546875]
[0.216796875, 0.427734375, 0.1619873046875, 0.07086181640625]
[0.3232421875, 0.5595703125, 0.071044921875, 0.0509033203125]
[0.3623046875, 0.60546875, 0.0023975372314453125, 0.02679443359375]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.298828125, 0.5, 0.07025146484375, 0.060546875]
[0.216796875, 0.427734375, 0.1619873046875, 0.07086181640625]
[0.3232421875, 0.5595703125, 0.071044921875, 0.0509033203125]
[0.3623046875, 0.60546875, 0.0023975372314453125, 0.02679443359375]
This is the real loss :  tensor(0.1785, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...

[23 rows x 5 columns]
Training labels: tensor([[0., 1., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.4560546875, 0.44091796875, 0.1409912109375, -0.053741455078125]
[0.307373046875, 0.67041015625, 0.06561279296875, 0.1907958984375]
[0.21435546875, 0.56884765625, 0.0997314453125, 0.037628173828125]
[0.488525390625, 0.52783203125, 0.027252197265625, 0.020721435546875]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.4560546875, 0.44091796875, 0.1409912109375, -0.053741455078125]
[0.307373046875, 0.67041015625, 0.06561279296875, 0.1907958984375]
[0.21435546875, 0.56884765625, 0.0997314453125, 0.037628173828125]
[0.488525390625, 0.52783203125, 0.027252197265625, 0.020721435546875]
This is the real loss :  tensor(0.1879, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
23     0    NaN  ...  0.187924  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...

[24 rows x 5 columns]
Training labels: tensor([[0., 1., 0., 0.],
        [1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.4091796875, 0.50830078125, 0.235107421875, 0.189453125]
[0.44921875, 0.53173828125, 0.10736083984375, -0.01371002197265625]
[0.386962890625, 0.52685546875, 0.075439453125, -0.01605224609375]
[0.22216796875, 0.71875, -0.08795166015625, 0.03326416015625]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.4091796875, 0.50830078125, 0.235107421875, 0.189453125]
[0.44921875, 0.53173828125, 0.10736083984375, -0.01371002197265625]
[0.386962890625, 0.52685546875, 0.075439453125, -0.01605224609375]
[0.22216796875, 0.71875, -0.08795166015625, 0.03326416015625]
This is the real loss :  tensor(0.1009, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
23     0    NaN  ...  0.187924  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
24     0    NaN  ...  0.100940  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...

[25 rows x 5 columns]
Training labels: tensor([[0., 0., 1., 0.],
        [1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [1., 0., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.446044921875, 0.59130859375, 0.0307159423828125, 0.01401519775390625]
[0.40673828125, 0.70654296875, -0.040252685546875, 0.088623046875]
[0.33837890625, 0.5595703125, 0.1431884765625, -0.013671875]
[0.265625, 0.494140625, 0.252197265625, 0.08489990234375]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.446044921875, 0.59130859375, 0.0307159423828125, 0.01401519775390625]
[0.40673828125, 0.70654296875, -0.040252685546875, 0.088623046875]
[0.33837890625, 0.5595703125, 0.1431884765625, -0.013671875]
[0.265625, 0.494140625, 0.252197265625, 0.08489990234375]
This is the real loss :  tensor(0.2208, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
23     0    NaN  ...  0.187924  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
24     0    NaN  ...  0.100940  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
25     0    NaN  ...  0.220775  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...

[26 rows x 5 columns]
Training labels: tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [1., 0., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.3349609375, 0.57861328125, 0.037872314453125, 0.0048370361328125]
[0.360107421875, 0.6240234375, 0.050811767578125, 0.05621337890625]
[0.28271484375, 0.43798828125, 0.202392578125, -0.03460693359375]
[0.388671875, 0.5283203125, 0.1002197265625, -0.025482177734375]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.3349609375, 0.57861328125, 0.037872314453125, 0.0048370361328125]
[0.360107421875, 0.6240234375, 0.050811767578125, 0.05621337890625]
[0.28271484375, 0.43798828125, 0.202392578125, -0.03460693359375]
[0.388671875, 0.5283203125, 0.1002197265625, -0.025482177734375]
This is the real loss :  tensor(0.1043, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
23     0    NaN  ...  0.187924  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
24     0    NaN  ...  0.100940  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
25     0    NaN  ...  0.220775  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
26     0    NaN  ...  0.104343  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...

[27 rows x 5 columns]
Training labels: tensor([[1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [0., 1., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.357421875, 0.403076171875, 0.2568359375, -0.05206298828125]
[0.30126953125, 0.64501953125, 0.007427215576171875, 0.1087646484375]
[0.281005859375, 0.6240234375, 0.046722412109375, 0.033172607421875]
[0.4365234375, 0.52197265625, 0.08697509765625, -0.03802490234375]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.357421875, 0.403076171875, 0.2568359375, -0.05206298828125]
[0.30126953125, 0.64501953125, 0.007427215576171875, 0.1087646484375]
[0.281005859375, 0.6240234375, 0.046722412109375, 0.033172607421875]
[0.4365234375, 0.52197265625, 0.08697509765625, -0.03802490234375]
This is the real loss :  tensor(0.1811, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
23     0    NaN  ...  0.187924  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
24     0    NaN  ...  0.100940  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
25     0    NaN  ...  0.220775  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
26     0    NaN  ...  0.104343  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
27     0    NaN  ...  0.181120  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...

[28 rows x 5 columns]
Training labels: tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.406494140625, 0.51513671875, 0.07159423828125, -0.032958984375]
[0.31298828125, 0.6806640625, 0.023284912109375, 0.0965576171875]
[0.418701171875, 0.42138671875, 0.111328125, -0.041717529296875]
[0.28125, 0.52734375, 0.2083740234375, -0.0672607421875]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.406494140625, 0.51513671875, 0.07159423828125, -0.032958984375]
[0.31298828125, 0.6806640625, 0.023284912109375, 0.0965576171875]
[0.418701171875, 0.42138671875, 0.111328125, -0.041717529296875]
[0.28125, 0.52734375, 0.2083740234375, -0.0672607421875]
This is the real loss :  tensor(0.1854, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
23     0    NaN  ...  0.187924  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
24     0    NaN  ...  0.100940  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
25     0    NaN  ...  0.220775  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
26     0    NaN  ...  0.104343  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
27     0    NaN  ...  0.181120  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
28     0    NaN  ...  0.185398  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...

[29 rows x 5 columns]
Training labels: tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [1., 0., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.36865234375, 0.361572265625, 0.187744140625, -0.109375]
[0.371337890625, 0.6123046875, -0.045623779296875, 0.1395263671875]
[0.290283203125, 0.5126953125, 0.08984375, 0.01348876953125]
[0.334228515625, 0.57763671875, 0.168212890625, -0.01922607421875]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.36865234375, 0.361572265625, 0.187744140625, -0.109375]
[0.371337890625, 0.6123046875, -0.045623779296875, 0.1395263671875]
[0.290283203125, 0.5126953125, 0.08984375, 0.01348876953125]
[0.334228515625, 0.57763671875, 0.168212890625, -0.01922607421875]
This is the real loss :  tensor(0.1273, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
23     0    NaN  ...  0.187924  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
24     0    NaN  ...  0.100940  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
25     0    NaN  ...  0.220775  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
26     0    NaN  ...  0.104343  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
27     0    NaN  ...  0.181120  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
28     0    NaN  ...  0.185398  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
29     0    NaN  ...  0.127251  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...

[30 rows x 5 columns]
Training labels: tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [1., 0., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.390380859375, 0.5849609375, 0.0118408203125, 0.11798095703125]
[0.334716796875, 0.468505859375, 0.1719970703125, 0.0633544921875]
[0.396484375, 0.41162109375, 0.1390380859375, -0.08245849609375]
[0.2259521484375, 0.76171875, 0.0491943359375, -0.05609130859375]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.390380859375, 0.5849609375, 0.0118408203125, 0.11798095703125]
[0.334716796875, 0.468505859375, 0.1719970703125, 0.0633544921875]
[0.396484375, 0.41162109375, 0.1390380859375, -0.08245849609375]
[0.2259521484375, 0.76171875, 0.0491943359375, -0.05609130859375]
This is the real loss :  tensor(0.1794, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
23     0    NaN  ...  0.187924  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
24     0    NaN  ...  0.100940  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
25     0    NaN  ...  0.220775  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
26     0    NaN  ...  0.104343  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
27     0    NaN  ...  0.181120  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
28     0    NaN  ...  0.185398  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
29     0    NaN  ...  0.127251  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
30     0    NaN  ...  0.179403  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...

[31 rows x 5 columns]
Training labels: tensor([[0., 0., 1., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.322998046875, 0.478515625, 0.08740234375, 0.032073974609375]
[0.3603515625, 0.496337890625, 0.04705810546875, 0.0185699462890625]
[0.288330078125, 0.5107421875, 0.14453125, -0.0104827880859375]
[0.423828125, 0.5068359375, 0.11572265625, -0.0269012451171875]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.322998046875, 0.478515625, 0.08740234375, 0.032073974609375]
[0.3603515625, 0.496337890625, 0.04705810546875, 0.0185699462890625]
[0.288330078125, 0.5107421875, 0.14453125, -0.0104827880859375]
[0.423828125, 0.5068359375, 0.11572265625, -0.0269012451171875]
This is the real loss :  tensor(0.2010, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
23     0    NaN  ...  0.187924  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
24     0    NaN  ...  0.100940  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
25     0    NaN  ...  0.220775  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
26     0    NaN  ...  0.104343  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
27     0    NaN  ...  0.181120  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
28     0    NaN  ...  0.185398  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
29     0    NaN  ...  0.127251  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
30     0    NaN  ...  0.179403  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
31     0    NaN  ...  0.201033  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...

[32 rows x 5 columns]
Training labels: tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [1., 0., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.226318359375, 0.36572265625, 0.249755859375, -0.0841064453125]
[0.3125, 0.462646484375, 0.08526611328125, 0.06292724609375]
[0.32958984375, 0.5302734375, 0.11865234375, -0.007411956787109375]
[0.371337890625, 0.427978515625, 0.072998046875, 0.00038814544677734375]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.226318359375, 0.36572265625, 0.249755859375, -0.0841064453125]
[0.3125, 0.462646484375, 0.08526611328125, 0.06292724609375]
[0.32958984375, 0.5302734375, 0.11865234375, -0.007411956787109375]
[0.371337890625, 0.427978515625, 0.072998046875, 0.00038814544677734375]
This is the real loss :  tensor(0.1329, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
23     0    NaN  ...  0.187924  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
24     0    NaN  ...  0.100940  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
25     0    NaN  ...  0.220775  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
26     0    NaN  ...  0.104343  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
27     0    NaN  ...  0.181120  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
28     0    NaN  ...  0.185398  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
29     0    NaN  ...  0.127251  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
30     0    NaN  ...  0.179403  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
31     0    NaN  ...  0.201033  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
32     0    NaN  ...  0.132909  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...

[33 rows x 5 columns]
Training labels: tensor([[1., 0., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [1., 0., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.41943359375, 0.5107421875, 0.035308837890625, -0.005313873291015625]
[0.276611328125, 0.2484130859375, 0.377685546875, -0.04766845703125]
[0.365234375, 0.494384765625, 0.10662841796875, -0.0340576171875]
[0.30615234375, 0.43994140625, 0.06597900390625, 0.058837890625]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.41943359375, 0.5107421875, 0.035308837890625, -0.005313873291015625]
[0.276611328125, 0.2484130859375, 0.377685546875, -0.04766845703125]
[0.365234375, 0.494384765625, 0.10662841796875, -0.0340576171875]
[0.30615234375, 0.43994140625, 0.06597900390625, 0.058837890625]
This is the real loss :  tensor(0.1382, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
23     0    NaN  ...  0.187924  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
24     0    NaN  ...  0.100940  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
25     0    NaN  ...  0.220775  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
26     0    NaN  ...  0.104343  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
27     0    NaN  ...  0.181120  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
28     0    NaN  ...  0.185398  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
29     0    NaN  ...  0.127251  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
30     0    NaN  ...  0.179403  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
31     0    NaN  ...  0.201033  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
32     0    NaN  ...  0.132909  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
33     0    NaN  ...  0.138208  [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...

[34 rows x 5 columns]
Training labels: tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.340576171875, 0.3759765625, 0.07135009765625, 0.035552978515625]
[0.354736328125, 0.6083984375, 0.09466552734375, 0.01904296875]
[0.3388671875, 0.419921875, 0.0384521484375, -0.019744873046875]
[0.2138671875, 0.3984375, 0.250732421875, -0.07574462890625]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.340576171875, 0.3759765625, 0.07135009765625, 0.035552978515625]
[0.354736328125, 0.6083984375, 0.09466552734375, 0.01904296875]
[0.3388671875, 0.419921875, 0.0384521484375, -0.019744873046875]
[0.2138671875, 0.3984375, 0.250732421875, -0.07574462890625]
This is the real loss :  tensor(0.1125, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
23     0    NaN  ...  0.187924  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
24     0    NaN  ...  0.100940  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
25     0    NaN  ...  0.220775  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
26     0    NaN  ...  0.104343  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
27     0    NaN  ...  0.181120  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
28     0    NaN  ...  0.185398  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
29     0    NaN  ...  0.127251  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
30     0    NaN  ...  0.179403  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
31     0    NaN  ...  0.201033  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
32     0    NaN  ...  0.132909  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
33     0    NaN  ...  0.138208  [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
34     0    NaN  ...  0.112530  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...

[35 rows x 5 columns]
Training labels: tensor([[1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [0., 0., 1., 0.],
        [1., 0., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.44873046875, 0.61181640625, 0.052520751953125, -0.043792724609375]
[0.25341796875, 0.37109375, 0.1689453125, -0.00803375244140625]
[0.37158203125, 0.501953125, 0.074462890625, 0.0120849609375]
[0.38671875, 0.2529296875, 0.2880859375, 0.05194091796875]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.44873046875, 0.61181640625, 0.052520751953125, -0.043792724609375]
[0.25341796875, 0.37109375, 0.1689453125, -0.00803375244140625]
[0.37158203125, 0.501953125, 0.074462890625, 0.0120849609375]
[0.38671875, 0.2529296875, 0.2880859375, 0.05194091796875]
This is the real loss :  tensor(0.1987, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
23     0    NaN  ...  0.187924  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
24     0    NaN  ...  0.100940  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
25     0    NaN  ...  0.220775  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
26     0    NaN  ...  0.104343  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
27     0    NaN  ...  0.181120  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
28     0    NaN  ...  0.185398  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
29     0    NaN  ...  0.127251  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
30     0    NaN  ...  0.179403  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
31     0    NaN  ...  0.201033  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
32     0    NaN  ...  0.132909  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
33     0    NaN  ...  0.138208  [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
34     0    NaN  ...  0.112530  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
35     0    NaN  ...  0.198698  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...

[36 rows x 5 columns]
Training labels: tensor([[1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.269287109375, 0.303466796875, 0.287841796875, -0.07220458984375]
[0.350830078125, 0.258544921875, 0.197265625, 0.01177978515625]
[0.345947265625, 0.5009765625, 0.034332275390625, -0.01540374755859375]
[0.3212890625, 0.406982421875, 0.00905609130859375, -0.007457733154296875]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.269287109375, 0.303466796875, 0.287841796875, -0.07220458984375]
[0.350830078125, 0.258544921875, 0.197265625, 0.01177978515625]
[0.345947265625, 0.5009765625, 0.034332275390625, -0.01540374755859375]
[0.3212890625, 0.406982421875, 0.00905609130859375, -0.007457733154296875]
This is the real loss :  tensor(0.1292, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
23     0    NaN  ...  0.187924  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
24     0    NaN  ...  0.100940  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
25     0    NaN  ...  0.220775  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
26     0    NaN  ...  0.104343  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
27     0    NaN  ...  0.181120  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
28     0    NaN  ...  0.185398  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
29     0    NaN  ...  0.127251  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
30     0    NaN  ...  0.179403  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
31     0    NaN  ...  0.201033  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
32     0    NaN  ...  0.132909  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
33     0    NaN  ...  0.138208  [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
34     0    NaN  ...  0.112530  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
35     0    NaN  ...  0.198698  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
36     0    NaN  ...  0.129161  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...

[37 rows x 5 columns]
Training labels: tensor([[0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [1., 0., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.3583984375, 0.372314453125, 0.185302734375, -0.08612060546875]
[0.2425537109375, 0.51025390625, 0.1292724609375, 0.042449951171875]
[0.3935546875, 0.3515625, 0.2064208984375, -0.003814697265625]
[0.392333984375, 0.451171875, 0.06024169921875, 0.017120361328125]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.3583984375, 0.372314453125, 0.185302734375, -0.08612060546875]
[0.2425537109375, 0.51025390625, 0.1292724609375, 0.042449951171875]
[0.3935546875, 0.3515625, 0.2064208984375, -0.003814697265625]
[0.392333984375, 0.451171875, 0.06024169921875, 0.017120361328125]
This is the real loss :  tensor(0.1531, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
23     0    NaN  ...  0.187924  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
24     0    NaN  ...  0.100940  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
25     0    NaN  ...  0.220775  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
26     0    NaN  ...  0.104343  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
27     0    NaN  ...  0.181120  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
28     0    NaN  ...  0.185398  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
29     0    NaN  ...  0.127251  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
30     0    NaN  ...  0.179403  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
31     0    NaN  ...  0.201033  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
32     0    NaN  ...  0.132909  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
33     0    NaN  ...  0.138208  [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
34     0    NaN  ...  0.112530  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
35     0    NaN  ...  0.198698  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
36     0    NaN  ...  0.129161  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
37     0    NaN  ...  0.153133  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...

[38 rows x 5 columns]
Training labels: tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [1., 0., 0., 0.],
        [0., 1., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.384521484375, 0.362548828125, 0.05316162109375, -0.0067901611328125]
[0.428955078125, 0.197509765625, 0.36279296875, -0.035186767578125]
[0.283935546875, 0.434326171875, 0.1357421875, -0.0116424560546875]
[0.269775390625, 0.65576171875, 0.0496826171875, 0.1314697265625]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.384521484375, 0.362548828125, 0.05316162109375, -0.0067901611328125]
[0.428955078125, 0.197509765625, 0.36279296875, -0.035186767578125]
[0.283935546875, 0.434326171875, 0.1357421875, -0.0116424560546875]
[0.269775390625, 0.65576171875, 0.0496826171875, 0.1314697265625]
This is the real loss :  tensor(0.1503, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
23     0    NaN  ...  0.187924  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
24     0    NaN  ...  0.100940  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
25     0    NaN  ...  0.220775  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
26     0    NaN  ...  0.104343  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
27     0    NaN  ...  0.181120  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
28     0    NaN  ...  0.185398  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
29     0    NaN  ...  0.127251  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
30     0    NaN  ...  0.179403  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
31     0    NaN  ...  0.201033  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
32     0    NaN  ...  0.132909  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
33     0    NaN  ...  0.138208  [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
34     0    NaN  ...  0.112530  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
35     0    NaN  ...  0.198698  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
36     0    NaN  ...  0.129161  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
37     0    NaN  ...  0.153133  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
38     0    NaN  ...  0.150310  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...

[39 rows x 5 columns]
Training labels: tensor([[0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [1., 0., 0., 0.],
        [0., 1., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.38232421875, 0.60400390625, 0.0689697265625, -0.0109100341796875]
[0.364990234375, 0.357421875, 0.141357421875, 0.0211181640625]
[0.30419921875, 0.30859375, 0.243896484375, -0.054718017578125]
[0.427001953125, 0.35107421875, 0.096923828125, 0.044708251953125]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.38232421875, 0.60400390625, 0.0689697265625, -0.0109100341796875]
[0.364990234375, 0.357421875, 0.141357421875, 0.0211181640625]
[0.30419921875, 0.30859375, 0.243896484375, -0.054718017578125]
[0.427001953125, 0.35107421875, 0.096923828125, 0.044708251953125]
This is the real loss :  tensor(0.2001, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
23     0    NaN  ...  0.187924  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
24     0    NaN  ...  0.100940  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
25     0    NaN  ...  0.220775  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
26     0    NaN  ...  0.104343  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
27     0    NaN  ...  0.181120  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
28     0    NaN  ...  0.185398  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
29     0    NaN  ...  0.127251  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
30     0    NaN  ...  0.179403  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
31     0    NaN  ...  0.201033  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
32     0    NaN  ...  0.132909  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
33     0    NaN  ...  0.138208  [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
34     0    NaN  ...  0.112530  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
35     0    NaN  ...  0.198698  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
36     0    NaN  ...  0.129161  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
37     0    NaN  ...  0.153133  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
38     0    NaN  ...  0.150310  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
39     0    NaN  ...  0.200072  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...

[40 rows x 5 columns]
Training labels: tensor([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.48291015625, 0.297607421875, 0.24560546875, 0.053436279296875]
[0.36474609375, 0.42724609375, 0.0941162109375, -0.0213775634765625]
[0.329345703125, 0.47216796875, 0.05523681640625, 0.02679443359375]
[0.314208984375, 0.28564453125, 0.2276611328125, -0.0450439453125]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.48291015625, 0.297607421875, 0.24560546875, 0.053436279296875]
[0.36474609375, 0.42724609375, 0.0941162109375, -0.0213775634765625]
[0.329345703125, 0.47216796875, 0.05523681640625, 0.02679443359375]
[0.314208984375, 0.28564453125, 0.2276611328125, -0.0450439453125]
This is the real loss :  tensor(0.2005, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
23     0    NaN  ...  0.187924  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
24     0    NaN  ...  0.100940  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
25     0    NaN  ...  0.220775  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
26     0    NaN  ...  0.104343  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
27     0    NaN  ...  0.181120  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
28     0    NaN  ...  0.185398  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
29     0    NaN  ...  0.127251  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
30     0    NaN  ...  0.179403  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
31     0    NaN  ...  0.201033  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
32     0    NaN  ...  0.132909  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
33     0    NaN  ...  0.138208  [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
34     0    NaN  ...  0.112530  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
35     0    NaN  ...  0.198698  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
36     0    NaN  ...  0.129161  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
37     0    NaN  ...  0.153133  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
38     0    NaN  ...  0.150310  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
39     0    NaN  ...  0.200072  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
40     0    NaN  ...  0.200540  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...

[41 rows x 5 columns]
Training labels: tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.342041015625, 0.52197265625, 0.06585693359375, 0.0260009765625]
[0.3740234375, 0.40234375, 0.2337646484375, 0.046844482421875]
[0.415771484375, 0.350830078125, 0.06439208984375, 0.061981201171875]
[0.362060546875, 0.311767578125, 0.160400390625, -0.07208251953125]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.342041015625, 0.52197265625, 0.06585693359375, 0.0260009765625]
[0.3740234375, 0.40234375, 0.2337646484375, 0.046844482421875]
[0.415771484375, 0.350830078125, 0.06439208984375, 0.061981201171875]
[0.362060546875, 0.311767578125, 0.160400390625, -0.07208251953125]
This is the real loss :  tensor(0.1564, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
23     0    NaN  ...  0.187924  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
24     0    NaN  ...  0.100940  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
25     0    NaN  ...  0.220775  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
26     0    NaN  ...  0.104343  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
27     0    NaN  ...  0.181120  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
28     0    NaN  ...  0.185398  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
29     0    NaN  ...  0.127251  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
30     0    NaN  ...  0.179403  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
31     0    NaN  ...  0.201033  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
32     0    NaN  ...  0.132909  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
33     0    NaN  ...  0.138208  [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
34     0    NaN  ...  0.112530  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
35     0    NaN  ...  0.198698  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
36     0    NaN  ...  0.129161  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
37     0    NaN  ...  0.153133  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
38     0    NaN  ...  0.150310  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
39     0    NaN  ...  0.200072  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
40     0    NaN  ...  0.200540  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
41     0    NaN  ...  0.156391  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...

[42 rows x 5 columns]
Training labels: tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [1., 0., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.380859375, 0.42529296875, 0.1260986328125, 0.01224517822265625]
[0.39697265625, 0.435302734375, 0.1839599609375, 0.00759124755859375]
[0.5029296875, 0.5244140625, 0.03839111328125, 0.1234130859375]
[0.341064453125, 0.434814453125, 0.2113037109375, -0.07659912109375]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.380859375, 0.42529296875, 0.1260986328125, 0.01224517822265625]
[0.39697265625, 0.435302734375, 0.1839599609375, 0.00759124755859375]
[0.5029296875, 0.5244140625, 0.03839111328125, 0.1234130859375]
[0.341064453125, 0.434814453125, 0.2113037109375, -0.07659912109375]
This is the real loss :  tensor(0.1413, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
23     0    NaN  ...  0.187924  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
24     0    NaN  ...  0.100940  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
25     0    NaN  ...  0.220775  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
26     0    NaN  ...  0.104343  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
27     0    NaN  ...  0.181120  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
28     0    NaN  ...  0.185398  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
29     0    NaN  ...  0.127251  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
30     0    NaN  ...  0.179403  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
31     0    NaN  ...  0.201033  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
32     0    NaN  ...  0.132909  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
33     0    NaN  ...  0.138208  [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
34     0    NaN  ...  0.112530  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
35     0    NaN  ...  0.198698  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
36     0    NaN  ...  0.129161  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
37     0    NaN  ...  0.153133  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
38     0    NaN  ...  0.150310  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
39     0    NaN  ...  0.200072  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
40     0    NaN  ...  0.200540  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
41     0    NaN  ...  0.156391  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
42     0    NaN  ...  0.141264  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...

[43 rows x 5 columns]
Training labels: tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.484619140625, 0.55859375, 0.01322174072265625, 0.04974365234375]
[0.44140625, 0.413818359375, 0.278076171875, 0.0233306884765625]
[0.3642578125, 0.444580078125, 0.1737060546875, 0.01053619384765625]
[0.403076171875, 0.42333984375, 0.129638671875, 0.016021728515625]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.484619140625, 0.55859375, 0.01322174072265625, 0.04974365234375]
[0.44140625, 0.413818359375, 0.278076171875, 0.0233306884765625]
[0.3642578125, 0.444580078125, 0.1737060546875, 0.01053619384765625]
[0.403076171875, 0.42333984375, 0.129638671875, 0.016021728515625]
This is the real loss :  tensor(0.1270, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
23     0    NaN  ...  0.187924  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
24     0    NaN  ...  0.100940  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
25     0    NaN  ...  0.220775  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
26     0    NaN  ...  0.104343  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
27     0    NaN  ...  0.181120  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
28     0    NaN  ...  0.185398  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
29     0    NaN  ...  0.127251  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
30     0    NaN  ...  0.179403  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
31     0    NaN  ...  0.201033  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
32     0    NaN  ...  0.132909  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
33     0    NaN  ...  0.138208  [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
34     0    NaN  ...  0.112530  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
35     0    NaN  ...  0.198698  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
36     0    NaN  ...  0.129161  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
37     0    NaN  ...  0.153133  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
38     0    NaN  ...  0.150310  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
39     0    NaN  ...  0.200072  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
40     0    NaN  ...  0.200540  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
41     0    NaN  ...  0.156391  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
42     0    NaN  ...  0.141264  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
43     0    NaN  ...  0.127012  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...

[44 rows x 5 columns]
Training labels: tensor([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.417236328125, 0.45263671875, 0.11187744140625, -0.0189971923828125]
[0.304443359375, 0.6318359375, 0.06549072265625, 0.06304931640625]
[0.472900390625, 0.45263671875, 0.1475830078125, 0.0386962890625]
[0.38818359375, 0.384765625, 0.235107421875, 0.01169586181640625]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.417236328125, 0.45263671875, 0.11187744140625, -0.0189971923828125]
[0.304443359375, 0.6318359375, 0.06549072265625, 0.06304931640625]
[0.472900390625, 0.45263671875, 0.1475830078125, 0.0386962890625]
[0.38818359375, 0.384765625, 0.235107421875, 0.01169586181640625]
This is the real loss :  tensor(0.1174, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
23     0    NaN  ...  0.187924  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
24     0    NaN  ...  0.100940  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
25     0    NaN  ...  0.220775  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
26     0    NaN  ...  0.104343  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
27     0    NaN  ...  0.181120  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
28     0    NaN  ...  0.185398  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
29     0    NaN  ...  0.127251  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
30     0    NaN  ...  0.179403  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
31     0    NaN  ...  0.201033  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
32     0    NaN  ...  0.132909  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
33     0    NaN  ...  0.138208  [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
34     0    NaN  ...  0.112530  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
35     0    NaN  ...  0.198698  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
36     0    NaN  ...  0.129161  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
37     0    NaN  ...  0.153133  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
38     0    NaN  ...  0.150310  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
39     0    NaN  ...  0.200072  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
40     0    NaN  ...  0.200540  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
41     0    NaN  ...  0.156391  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
42     0    NaN  ...  0.141264  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
43     0    NaN  ...  0.127012  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
44     0    NaN  ...  0.117352  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...

[45 rows x 5 columns]
Training labels: tensor([[0., 1., 0., 0.],
        [1., 0., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.421630859375, 0.45361328125, 0.230712890625, 0.020843505859375]
[0.420654296875, 0.46435546875, 0.12017822265625, -0.0179595947265625]
[0.5302734375, 0.560546875, 0.1165771484375, 0.108642578125]
[0.403564453125, 0.537109375, 0.12255859375, -0.01465606689453125]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.421630859375, 0.45361328125, 0.230712890625, 0.020843505859375]
[0.420654296875, 0.46435546875, 0.12017822265625, -0.0179595947265625]
[0.5302734375, 0.560546875, 0.1165771484375, 0.108642578125]
[0.403564453125, 0.537109375, 0.12255859375, -0.01465606689453125]
This is the real loss :  tensor(0.1798, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
23     0    NaN  ...  0.187924  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
24     0    NaN  ...  0.100940  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
25     0    NaN  ...  0.220775  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
26     0    NaN  ...  0.104343  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
27     0    NaN  ...  0.181120  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
28     0    NaN  ...  0.185398  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
29     0    NaN  ...  0.127251  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
30     0    NaN  ...  0.179403  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
31     0    NaN  ...  0.201033  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
32     0    NaN  ...  0.132909  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
33     0    NaN  ...  0.138208  [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
34     0    NaN  ...  0.112530  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
35     0    NaN  ...  0.198698  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
36     0    NaN  ...  0.129161  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
37     0    NaN  ...  0.153133  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
38     0    NaN  ...  0.150310  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
39     0    NaN  ...  0.200072  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
40     0    NaN  ...  0.200540  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
41     0    NaN  ...  0.156391  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
42     0    NaN  ...  0.141264  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
43     0    NaN  ...  0.127012  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
44     0    NaN  ...  0.117352  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
45     0    NaN  ...  0.179751  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...

[46 rows x 5 columns]
Training labels: tensor([[1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.446533203125, 0.476806640625, 0.1278076171875, -0.005603790283203125]
[0.410888671875, 0.5478515625, 0.100830078125, 0.0677490234375]
[0.380859375, 0.634765625, 0.015289306640625, 0.048828125]
[0.5458984375, 0.513671875, 0.2369384765625, 0.07672119140625]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.446533203125, 0.476806640625, 0.1278076171875, -0.005603790283203125]
[0.410888671875, 0.5478515625, 0.100830078125, 0.0677490234375]
[0.380859375, 0.634765625, 0.015289306640625, 0.048828125]
[0.5458984375, 0.513671875, 0.2369384765625, 0.07672119140625]
This is the real loss :  tensor(0.1306, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
23     0    NaN  ...  0.187924  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
24     0    NaN  ...  0.100940  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
25     0    NaN  ...  0.220775  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
26     0    NaN  ...  0.104343  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
27     0    NaN  ...  0.181120  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
28     0    NaN  ...  0.185398  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
29     0    NaN  ...  0.127251  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
30     0    NaN  ...  0.179403  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
31     0    NaN  ...  0.201033  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
32     0    NaN  ...  0.132909  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
33     0    NaN  ...  0.138208  [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
34     0    NaN  ...  0.112530  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
35     0    NaN  ...  0.198698  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
36     0    NaN  ...  0.129161  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
37     0    NaN  ...  0.153133  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
38     0    NaN  ...  0.150310  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
39     0    NaN  ...  0.200072  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
40     0    NaN  ...  0.200540  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
41     0    NaN  ...  0.156391  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
42     0    NaN  ...  0.141264  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
43     0    NaN  ...  0.127012  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
44     0    NaN  ...  0.117352  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
45     0    NaN  ...  0.179751  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
46     0    NaN  ...  0.130600  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...

[47 rows x 5 columns]
Training labels: tensor([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.357177734375, 0.53271484375, 0.2205810546875, 0.0985107421875]
[0.472900390625, 0.59814453125, 0.00749969482421875, -0.00811004638671875]
[0.546875, 0.6455078125, 0.10333251953125, 0.07568359375]
[0.418212890625, 0.459716796875, 0.19287109375, -0.0017242431640625]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.357177734375, 0.53271484375, 0.2205810546875, 0.0985107421875]
[0.472900390625, 0.59814453125, 0.00749969482421875, -0.00811004638671875]
[0.546875, 0.6455078125, 0.10333251953125, 0.07568359375]
[0.418212890625, 0.459716796875, 0.19287109375, -0.0017242431640625]
This is the real loss :  tensor(0.1998, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
23     0    NaN  ...  0.187924  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
24     0    NaN  ...  0.100940  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
25     0    NaN  ...  0.220775  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
26     0    NaN  ...  0.104343  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
27     0    NaN  ...  0.181120  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
28     0    NaN  ...  0.185398  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
29     0    NaN  ...  0.127251  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
30     0    NaN  ...  0.179403  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
31     0    NaN  ...  0.201033  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
32     0    NaN  ...  0.132909  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
33     0    NaN  ...  0.138208  [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
34     0    NaN  ...  0.112530  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
35     0    NaN  ...  0.198698  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
36     0    NaN  ...  0.129161  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
37     0    NaN  ...  0.153133  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
38     0    NaN  ...  0.150310  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
39     0    NaN  ...  0.200072  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
40     0    NaN  ...  0.200540  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
41     0    NaN  ...  0.156391  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
42     0    NaN  ...  0.141264  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
43     0    NaN  ...  0.127012  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
44     0    NaN  ...  0.117352  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
45     0    NaN  ...  0.179751  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
46     0    NaN  ...  0.130600  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
47     0    NaN  ...  0.199766  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...

[48 rows x 5 columns]
Training labels: tensor([[0., 1., 0., 0.],
        [1., 0., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.425537109375, 0.498291015625, 0.2001953125, 0.0153961181640625]
[0.35888671875, 0.50244140625, 0.1829833984375, 0.051788330078125]
[0.52001953125, 0.59912109375, 0.0799560546875, 0.01020050048828125]
[0.4443359375, 0.59228515625, 0.035919189453125, 0.029266357421875]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.425537109375, 0.498291015625, 0.2001953125, 0.0153961181640625]
[0.35888671875, 0.50244140625, 0.1829833984375, 0.051788330078125]
[0.52001953125, 0.59912109375, 0.0799560546875, 0.01020050048828125]
[0.4443359375, 0.59228515625, 0.035919189453125, 0.029266357421875]
This is the real loss :  tensor(0.1884, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
23     0    NaN  ...  0.187924  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
24     0    NaN  ...  0.100940  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
25     0    NaN  ...  0.220775  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
26     0    NaN  ...  0.104343  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
27     0    NaN  ...  0.181120  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
28     0    NaN  ...  0.185398  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
29     0    NaN  ...  0.127251  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
30     0    NaN  ...  0.179403  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
31     0    NaN  ...  0.201033  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
32     0    NaN  ...  0.132909  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
33     0    NaN  ...  0.138208  [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
34     0    NaN  ...  0.112530  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
35     0    NaN  ...  0.198698  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
36     0    NaN  ...  0.129161  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
37     0    NaN  ...  0.153133  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
38     0    NaN  ...  0.150310  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
39     0    NaN  ...  0.200072  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
40     0    NaN  ...  0.200540  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
41     0    NaN  ...  0.156391  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
42     0    NaN  ...  0.141264  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
43     0    NaN  ...  0.127012  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
44     0    NaN  ...  0.117352  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
45     0    NaN  ...  0.179751  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
46     0    NaN  ...  0.130600  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
47     0    NaN  ...  0.199766  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
48     0    NaN  ...  0.188407  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...

[49 rows x 5 columns]
Training labels: tensor([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [1., 0., 0., 0.],
        [0., 0., 1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.5576171875, 0.59716796875, 0.0298309326171875, -0.013519287109375]
[0.370361328125, 0.498291015625, 0.2406005859375, 0.05426025390625]
[0.445068359375, 0.421142578125, 0.265380859375, -0.01448822021484375]
[0.41943359375, 0.63916015625, 0.00505828857421875, 0.03466796875]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.5576171875, 0.59716796875, 0.0298309326171875, -0.013519287109375]
[0.370361328125, 0.498291015625, 0.2406005859375, 0.05426025390625]
[0.445068359375, 0.421142578125, 0.265380859375, -0.01448822021484375]
[0.41943359375, 0.63916015625, 0.00505828857421875, 0.03466796875]
This is the real loss :  tensor(0.2232, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
23     0    NaN  ...  0.187924  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
24     0    NaN  ...  0.100940  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
25     0    NaN  ...  0.220775  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
26     0    NaN  ...  0.104343  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
27     0    NaN  ...  0.181120  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
28     0    NaN  ...  0.185398  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
29     0    NaN  ...  0.127251  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
30     0    NaN  ...  0.179403  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
31     0    NaN  ...  0.201033  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
32     0    NaN  ...  0.132909  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
33     0    NaN  ...  0.138208  [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
34     0    NaN  ...  0.112530  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
35     0    NaN  ...  0.198698  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
36     0    NaN  ...  0.129161  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
37     0    NaN  ...  0.153133  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
38     0    NaN  ...  0.150310  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
39     0    NaN  ...  0.200072  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
40     0    NaN  ...  0.200540  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
41     0    NaN  ...  0.156391  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
42     0    NaN  ...  0.141264  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
43     0    NaN  ...  0.127012  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
44     0    NaN  ...  0.117352  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
45     0    NaN  ...  0.179751  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
46     0    NaN  ...  0.130600  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
47     0    NaN  ...  0.199766  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
48     0    NaN  ...  0.188407  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
49     0    NaN  ...  0.223180  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...

[50 rows x 5 columns]
Training labels: tensor([[1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.447265625, 0.447021484375, 0.1734619140625, 0.0103912353515625]
[0.380615234375, 0.58203125, 0.1376953125, 0.022491455078125]
[0.395263671875, 0.5546875, 0.045684814453125, 0.019927978515625]
[0.4677734375, 0.460693359375, 0.11627197265625, -0.01172637939453125]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.447265625, 0.447021484375, 0.1734619140625, 0.0103912353515625]
[0.380615234375, 0.58203125, 0.1376953125, 0.022491455078125]
[0.395263671875, 0.5546875, 0.045684814453125, 0.019927978515625]
[0.4677734375, 0.460693359375, 0.11627197265625, -0.01172637939453125]
This is the real loss :  tensor(0.1539, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
23     0    NaN  ...  0.187924  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
24     0    NaN  ...  0.100940  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
25     0    NaN  ...  0.220775  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
26     0    NaN  ...  0.104343  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
27     0    NaN  ...  0.181120  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
28     0    NaN  ...  0.185398  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
29     0    NaN  ...  0.127251  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
30     0    NaN  ...  0.179403  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
31     0    NaN  ...  0.201033  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
32     0    NaN  ...  0.132909  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
33     0    NaN  ...  0.138208  [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
34     0    NaN  ...  0.112530  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
35     0    NaN  ...  0.198698  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
36     0    NaN  ...  0.129161  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
37     0    NaN  ...  0.153133  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
38     0    NaN  ...  0.150310  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
39     0    NaN  ...  0.200072  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
40     0    NaN  ...  0.200540  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
41     0    NaN  ...  0.156391  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
42     0    NaN  ...  0.141264  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
43     0    NaN  ...  0.127012  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
44     0    NaN  ...  0.117352  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
45     0    NaN  ...  0.179751  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
46     0    NaN  ...  0.130600  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
47     0    NaN  ...  0.199766  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
48     0    NaN  ...  0.188407  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
49     0    NaN  ...  0.223180  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
50     0    NaN  ...  0.153902  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...

[51 rows x 5 columns]
Training labels: tensor([[1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.478271484375, 0.478515625, 0.0179901123046875, 0.01343536376953125]
[0.55322265625, 0.424560546875, 0.1527099609375, -0.07733154296875]
[0.392333984375, 0.51220703125, 0.245849609375, 0.166748046875]
[0.351318359375, 0.6455078125, 0.0005197525024414062, 0.049041748046875]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.478271484375, 0.478515625, 0.0179901123046875, 0.01343536376953125]
[0.55322265625, 0.424560546875, 0.1527099609375, -0.07733154296875]
[0.392333984375, 0.51220703125, 0.245849609375, 0.166748046875]
[0.351318359375, 0.6455078125, 0.0005197525024414062, 0.049041748046875]
This is the real loss :  tensor(0.1544, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
23     0    NaN  ...  0.187924  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
24     0    NaN  ...  0.100940  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
25     0    NaN  ...  0.220775  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
26     0    NaN  ...  0.104343  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
27     0    NaN  ...  0.181120  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
28     0    NaN  ...  0.185398  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
29     0    NaN  ...  0.127251  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
30     0    NaN  ...  0.179403  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
31     0    NaN  ...  0.201033  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
32     0    NaN  ...  0.132909  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
33     0    NaN  ...  0.138208  [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
34     0    NaN  ...  0.112530  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
35     0    NaN  ...  0.198698  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
36     0    NaN  ...  0.129161  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
37     0    NaN  ...  0.153133  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
38     0    NaN  ...  0.150310  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
39     0    NaN  ...  0.200072  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
40     0    NaN  ...  0.200540  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
41     0    NaN  ...  0.156391  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
42     0    NaN  ...  0.141264  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
43     0    NaN  ...  0.127012  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
44     0    NaN  ...  0.117352  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
45     0    NaN  ...  0.179751  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
46     0    NaN  ...  0.130600  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
47     0    NaN  ...  0.199766  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
48     0    NaN  ...  0.188407  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
49     0    NaN  ...  0.223180  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
50     0    NaN  ...  0.153902  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
51     0    NaN  ...  0.154411  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...

[52 rows x 5 columns]
Training labels: tensor([[0., 0., 1., 0.],
        [1., 0., 0., 0.],
        [0., 0., 1., 0.],
        [1., 0., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.41162109375, 0.525390625, 0.07574462890625, 0.05438232421875]
[0.459228515625, 0.430908203125, 0.139404296875, 0.067626953125]
[0.34130859375, 0.54345703125, 0.09564208984375, -0.051177978515625]
[0.5087890625, 0.382080078125, 0.198486328125, -0.059173583984375]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.41162109375, 0.525390625, 0.07574462890625, 0.05438232421875]
[0.459228515625, 0.430908203125, 0.139404296875, 0.067626953125]
[0.34130859375, 0.54345703125, 0.09564208984375, -0.051177978515625]
[0.5087890625, 0.382080078125, 0.198486328125, -0.059173583984375]
This is the real loss :  tensor(0.2167, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
23     0    NaN  ...  0.187924  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
24     0    NaN  ...  0.100940  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
25     0    NaN  ...  0.220775  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
26     0    NaN  ...  0.104343  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
27     0    NaN  ...  0.181120  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
28     0    NaN  ...  0.185398  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
29     0    NaN  ...  0.127251  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
30     0    NaN  ...  0.179403  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
31     0    NaN  ...  0.201033  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
32     0    NaN  ...  0.132909  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
33     0    NaN  ...  0.138208  [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
34     0    NaN  ...  0.112530  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
35     0    NaN  ...  0.198698  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
36     0    NaN  ...  0.129161  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
37     0    NaN  ...  0.153133  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
38     0    NaN  ...  0.150310  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
39     0    NaN  ...  0.200072  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
40     0    NaN  ...  0.200540  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
41     0    NaN  ...  0.156391  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
42     0    NaN  ...  0.141264  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
43     0    NaN  ...  0.127012  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
44     0    NaN  ...  0.117352  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
45     0    NaN  ...  0.179751  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
46     0    NaN  ...  0.130600  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
47     0    NaN  ...  0.199766  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
48     0    NaN  ...  0.188407  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
49     0    NaN  ...  0.223180  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
50     0    NaN  ...  0.153902  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
51     0    NaN  ...  0.154411  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
52     0    NaN  ...  0.216705  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...

[53 rows x 5 columns]
Training labels: tensor([[1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.472900390625, 0.44091796875, 0.1485595703125, 0.05157470703125]
[0.451904296875, 0.4912109375, 0.038970947265625, -0.0152130126953125]
[0.48681640625, 0.39013671875, 0.1583251953125, -0.1253662109375]
[0.41552734375, 0.446533203125, 0.1973876953125, 0.059234619140625]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.472900390625, 0.44091796875, 0.1485595703125, 0.05157470703125]
[0.451904296875, 0.4912109375, 0.038970947265625, -0.0152130126953125]
[0.48681640625, 0.39013671875, 0.1583251953125, -0.1253662109375]
[0.41552734375, 0.446533203125, 0.1973876953125, 0.059234619140625]
This is the real loss :  tensor(0.1672, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
23     0    NaN  ...  0.187924  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
24     0    NaN  ...  0.100940  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
25     0    NaN  ...  0.220775  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
26     0    NaN  ...  0.104343  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
27     0    NaN  ...  0.181120  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
28     0    NaN  ...  0.185398  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
29     0    NaN  ...  0.127251  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
30     0    NaN  ...  0.179403  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
31     0    NaN  ...  0.201033  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
32     0    NaN  ...  0.132909  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
33     0    NaN  ...  0.138208  [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
34     0    NaN  ...  0.112530  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
35     0    NaN  ...  0.198698  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
36     0    NaN  ...  0.129161  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
37     0    NaN  ...  0.153133  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
38     0    NaN  ...  0.150310  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
39     0    NaN  ...  0.200072  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
40     0    NaN  ...  0.200540  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
41     0    NaN  ...  0.156391  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
42     0    NaN  ...  0.141264  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
43     0    NaN  ...  0.127012  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
44     0    NaN  ...  0.117352  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
45     0    NaN  ...  0.179751  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
46     0    NaN  ...  0.130600  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
47     0    NaN  ...  0.199766  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
48     0    NaN  ...  0.188407  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
49     0    NaN  ...  0.223180  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
50     0    NaN  ...  0.153902  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
51     0    NaN  ...  0.154411  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
52     0    NaN  ...  0.216705  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
53     0    NaN  ...  0.167201  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...

[54 rows x 5 columns]
Training labels: tensor([[1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.372802734375, 0.471435546875, 0.09063720703125, -0.0063323974609375]
[0.496337890625, 0.3291015625, 0.1837158203125, -0.0853271484375]
[0.383056640625, 0.44677734375, 0.0673828125, 0.08026123046875]
[0.5322265625, 0.275390625, 0.270751953125, -0.117431640625]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.372802734375, 0.471435546875, 0.09063720703125, -0.0063323974609375]
[0.496337890625, 0.3291015625, 0.1837158203125, -0.0853271484375]
[0.383056640625, 0.44677734375, 0.0673828125, 0.08026123046875]
[0.5322265625, 0.275390625, 0.270751953125, -0.117431640625]
This is the real loss :  tensor(0.1250, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
23     0    NaN  ...  0.187924  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
24     0    NaN  ...  0.100940  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
25     0    NaN  ...  0.220775  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
26     0    NaN  ...  0.104343  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
27     0    NaN  ...  0.181120  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
28     0    NaN  ...  0.185398  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
29     0    NaN  ...  0.127251  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
30     0    NaN  ...  0.179403  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
31     0    NaN  ...  0.201033  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
32     0    NaN  ...  0.132909  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
33     0    NaN  ...  0.138208  [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
34     0    NaN  ...  0.112530  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
35     0    NaN  ...  0.198698  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
36     0    NaN  ...  0.129161  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
37     0    NaN  ...  0.153133  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
38     0    NaN  ...  0.150310  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
39     0    NaN  ...  0.200072  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
40     0    NaN  ...  0.200540  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
41     0    NaN  ...  0.156391  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
42     0    NaN  ...  0.141264  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
43     0    NaN  ...  0.127012  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
44     0    NaN  ...  0.117352  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
45     0    NaN  ...  0.179751  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
46     0    NaN  ...  0.130600  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
47     0    NaN  ...  0.199766  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
48     0    NaN  ...  0.188407  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
49     0    NaN  ...  0.223180  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
50     0    NaN  ...  0.153902  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
51     0    NaN  ...  0.154411  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
52     0    NaN  ...  0.216705  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
53     0    NaN  ...  0.167201  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
54     0    NaN  ...  0.124991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...

[55 rows x 5 columns]
Training labels: tensor([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 4])
[0.48583984375, 0.435546875, 0.03411865234375, 0.031646728515625]
[0.47705078125, 0.2705078125, 0.2257080078125, -0.1341552734375]
[0.43505859375, 0.2841796875, 0.215576171875, -0.016021728515625]
[0.353515625, 0.47119140625, 0.08477783203125, 0.01003265380859375]
Logits shape: torch.Size([4, 4])
Labels shape: torch.Size([4, 4])
[0.48583984375, 0.435546875, 0.03411865234375, 0.031646728515625]
[0.47705078125, 0.2705078125, 0.2257080078125, -0.1341552734375]
[0.43505859375, 0.2841796875, 0.215576171875, -0.016021728515625]
[0.353515625, 0.47119140625, 0.08477783203125, 0.01003265380859375]
This is the real loss :  tensor(0.1591, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                         True Value
0      0    NaN  ...  0.249333  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
1      0    NaN  ...  0.179528  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
2      0    NaN  ...  0.177695  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
3      0    NaN  ...  0.403061  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
4      0    NaN  ...  0.263510  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
5      0    NaN  ...  0.230346  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
6      0    NaN  ...  0.222499  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
7      0    NaN  ...  0.178121  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
8      0    NaN  ...  0.262577  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
9      0    NaN  ...  0.153102  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
10     0    NaN  ...  0.224883  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
11     0    NaN  ...  0.198261  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...
12     0    NaN  ...  0.169991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
13     0    NaN  ...  0.143123  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
14     0    NaN  ...  0.219086  [[0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [...
15     0    NaN  ...  0.168835  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
16     0    NaN  ...  0.159736  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
17     0    NaN  ...  0.176351  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
18     0    NaN  ...  0.148801  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
19     0    NaN  ...  0.134975  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
20     0    NaN  ...  0.172143  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
21     0    NaN  ...  0.116963  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
22     0    NaN  ...  0.178496  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
23     0    NaN  ...  0.187924  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
24     0    NaN  ...  0.100940  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
25     0    NaN  ...  0.220775  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
26     0    NaN  ...  0.104343  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
27     0    NaN  ...  0.181120  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
28     0    NaN  ...  0.185398  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
29     0    NaN  ...  0.127251  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
30     0    NaN  ...  0.179403  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
31     0    NaN  ...  0.201033  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
32     0    NaN  ...  0.132909  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
33     0    NaN  ...  0.138208  [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
34     0    NaN  ...  0.112530  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
35     0    NaN  ...  0.198698  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
36     0    NaN  ...  0.129161  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
37     0    NaN  ...  0.153133  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
38     0    NaN  ...  0.150310  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
39     0    NaN  ...  0.200072  [[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
40     0    NaN  ...  0.200540  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
41     0    NaN  ...  0.156391  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
42     0    NaN  ...  0.141264  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
43     0    NaN  ...  0.127012  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
44     0    NaN  ...  0.117352  [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...
45     0    NaN  ...  0.179751  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
46     0    NaN  ...  0.130600  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
47     0    NaN  ...  0.199766  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
48     0    NaN  ...  0.188407  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
49     0    NaN  ...  0.223180  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [...
50     0    NaN  ...  0.153902  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
51     0    NaN  ...  0.154411  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
52     0    NaN  ...  0.216705  [[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
53     0    NaN  ...  0.167201  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
54     0    NaN  ...  0.124991  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...
55     0    NaN  ...  0.159112  [[0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [...

[56 rows x 5 columns]slurmstepd: error: *** JOB 29135039 ON fg1 CANCELLED AT 2024-04-11T23:11:48 ***
