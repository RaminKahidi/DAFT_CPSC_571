True
Training completed.
MRI Directory: ./ProcessedDataset/MRIs/
Tabular Directory: ./RawDataset/ER_target.csv
tabularFileName: ER_target
MONAI version: 1.1.0
Numpy version: 1.19.2
Pytorch version: 1.13.1+cu117
MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False
MONAI rev id: a2ec3752f54bfc3b40e7952234fbeb5452ed63e3
MONAI __file__: /home/ramin.kahidi/software/miniconda3/envs/pytorch/lib/python3.7/site-packages/monai/__init__.py

Optional dependencies:
Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.
Nibabel version: NOT INSTALLED or UNKNOWN VERSION.
scikit-image version: NOT INSTALLED or UNKNOWN VERSION.
Pillow version: 9.4.0
Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.
gdown version: NOT INSTALLED or UNKNOWN VERSION.
TorchVision version: 0.13.1a0
tqdm version: 4.66.2
lmdb version: NOT INSTALLED or UNKNOWN VERSION.
psutil version: NOT INSTALLED or UNKNOWN VERSION.
pandas version: 1.3.5
einops version: NOT INSTALLED or UNKNOWN VERSION.
transformers version: NOT INSTALLED or UNKNOWN VERSION.
mlflow version: NOT INSTALLED or UNKNOWN VERSION.
pynrrd version: NOT INSTALLED or UNKNOWN VERSION.

For details about installing the optional dependencies, please visit:
    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies

DAFT
workers: 20
dcm_patient_filenames:  ['Breast_MRI_001.pt', 'Breast_MRI_002.pt', 'Breast_MRI_003.pt', 'Breast_MRI_004.pt', 'Breast_MRI_005.pt', 'Breast_MRI_006.pt', 'Breast_MRI_007.pt', 'Breast_MRI_008.pt', 'Breast_MRI_009.pt', 'Breast_MRI_010.pt', 'Breast_MRI_011.pt', 'Breast_MRI_012.pt', 'Breast_MRI_013.pt', 'Breast_MRI_014.pt', 'Breast_MRI_015.pt', 'Breast_MRI_016.pt', 'Breast_MRI_017.pt', 'Breast_MRI_018.pt', 'Breast_MRI_019.pt', 'Breast_MRI_020.pt', 'Breast_MRI_021.pt', 'Breast_MRI_022.pt', 'Breast_MRI_023.pt', 'Breast_MRI_024.pt', 'Breast_MRI_025.pt', 'Breast_MRI_026.pt', 'Breast_MRI_027.pt', 'Breast_MRI_028.pt', 'Breast_MRI_029.pt', 'Breast_MRI_030.pt', 'Breast_MRI_031.pt', 'Breast_MRI_032.pt', 'Breast_MRI_033.pt', 'Breast_MRI_034.pt', 'Breast_MRI_035.pt', 'Breast_MRI_036.pt', 'Breast_MRI_037.pt', 'Breast_MRI_038.pt', 'Breast_MRI_039.pt', 'Breast_MRI_040.pt', 'Breast_MRI_041.pt', 'Breast_MRI_042.pt', 'Breast_MRI_043.pt', 'Breast_MRI_044.pt', 'Breast_MRI_045.pt', 'Breast_MRI_046.pt', 'Breast_MRI_047.pt', 'Breast_MRI_048.pt', 'Breast_MRI_049.pt', 'Breast_MRI_050.pt', 'Breast_MRI_051.pt', 'Breast_MRI_052.pt', 'Breast_MRI_053.pt', 'Breast_MRI_054.pt', 'Breast_MRI_055.pt', 'Breast_MRI_056.pt', 'Breast_MRI_057.pt', 'Breast_MRI_058.pt', 'Breast_MRI_059.pt', 'Breast_MRI_060.pt', 'Breast_MRI_061.pt', 'Breast_MRI_062.pt', 'Breast_MRI_063.pt', 'Breast_MRI_064.pt', 'Breast_MRI_065.pt', 'Breast_MRI_066.pt', 'Breast_MRI_067.pt', 'Breast_MRI_068.pt', 'Breast_MRI_069.pt', 'Breast_MRI_070.pt', 'Breast_MRI_071.pt', 'Breast_MRI_072.pt', 'Breast_MRI_073.pt', 'Breast_MRI_074.pt', 'Breast_MRI_075.pt', 'Breast_MRI_076.pt', 'Breast_MRI_077.pt', 'Breast_MRI_078.pt', 'Breast_MRI_079.pt', 'Breast_MRI_080.pt', 'Breast_MRI_081.pt', 'Breast_MRI_082.pt', 'Breast_MRI_083.pt', 'Breast_MRI_084.pt', 'Breast_MRI_085.pt', 'Breast_MRI_086.pt', 'Breast_MRI_087.pt', 'Breast_MRI_088.pt', 'Breast_MRI_089.pt', 'Breast_MRI_090.pt', 'Breast_MRI_091.pt', 'Breast_MRI_092.pt', 'Breast_MRI_093.pt', 'Breast_MRI_094.pt', 'Breast_MRI_095.pt', 'Breast_MRI_096.pt', 'Breast_MRI_097.pt', 'Breast_MRI_098.pt', 'Breast_MRI_099.pt', 'Breast_MRI_100.pt', 'Breast_MRI_101.pt', 'Breast_MRI_102.pt', 'Breast_MRI_103.pt', 'Breast_MRI_104.pt', 'Breast_MRI_105.pt', 'Breast_MRI_106.pt', 'Breast_MRI_107.pt', 'Breast_MRI_108.pt', 'Breast_MRI_109.pt', 'Breast_MRI_110.pt', 'Breast_MRI_111.pt', 'Breast_MRI_112.pt', 'Breast_MRI_113.pt', 'Breast_MRI_114.pt', 'Breast_MRI_115.pt', 'Breast_MRI_116.pt', 'Breast_MRI_117.pt', 'Breast_MRI_118.pt', 'Breast_MRI_119.pt', 'Breast_MRI_121.pt', 'Breast_MRI_122.pt', 'Breast_MRI_123.pt', 'Breast_MRI_124.pt', 'Breast_MRI_125.pt', 'Breast_MRI_126.pt', 'Breast_MRI_127.pt', 'Breast_MRI_128.pt', 'Breast_MRI_129.pt', 'Breast_MRI_131.pt', 'Breast_MRI_132.pt', 'Breast_MRI_133.pt', 'Breast_MRI_134.pt', 'Breast_MRI_135.pt', 'Breast_MRI_136.pt', 'Breast_MRI_137.pt', 'Breast_MRI_138.pt', 'Breast_MRI_139.pt', 'Breast_MRI_140.pt', 'Breast_MRI_141.pt', 'Breast_MRI_142.pt', 'Breast_MRI_143.pt', 'Breast_MRI_144.pt', 'Breast_MRI_145.pt', 'Breast_MRI_146.pt', 'Breast_MRI_147.pt', 'Breast_MRI_148.pt', 'Breast_MRI_149.pt', 'Breast_MRI_150.pt', 'Breast_MRI_151.pt', 'Breast_MRI_152.pt', 'Breast_MRI_153.pt', 'Breast_MRI_154.pt', 'Breast_MRI_155.pt', 'Breast_MRI_156.pt', 'Breast_MRI_157.pt', 'Breast_MRI_158.pt', 'Breast_MRI_159.pt', 'Breast_MRI_160.pt', 'Breast_MRI_161.pt', 'Breast_MRI_162.pt', 'Breast_MRI_163.pt', 'Breast_MRI_164.pt', 'Breast_MRI_165.pt', 'Breast_MRI_166.pt', 'Breast_MRI_168.pt', 'Breast_MRI_169.pt', 'Breast_MRI_170.pt', 'Breast_MRI_171.pt', 'Breast_MRI_172.pt', 'Breast_MRI_173.pt', 'Breast_MRI_174.pt', 'Breast_MRI_175.pt', 'Breast_MRI_176.pt', 'Breast_MRI_177.pt', 'Breast_MRI_178.pt', 'Breast_MRI_179.pt', 'Breast_MRI_180.pt', 'Breast_MRI_181.pt', 'Breast_MRI_182.pt', 'Breast_MRI_183.pt', 'Breast_MRI_184.pt', 'Breast_MRI_185.pt', 'Breast_MRI_186.pt', 'Breast_MRI_187.pt', 'Breast_MRI_188.pt', 'Breast_MRI_189.pt', 'Breast_MRI_190.pt', 'Breast_MRI_191.pt', 'Breast_MRI_192.pt', 'Breast_MRI_193.pt', 'Breast_MRI_194.pt', 'Breast_MRI_195.pt', 'Breast_MRI_196.pt', 'Breast_MRI_197.pt', 'Breast_MRI_198.pt', 'Breast_MRI_199.pt', 'Breast_MRI_200.pt', 'Breast_MRI_201.pt', 'Breast_MRI_202.pt', 'Breast_MRI_203.pt', 'Breast_MRI_204.pt', 'Breast_MRI_205.pt', 'Breast_MRI_206.pt', 'Breast_MRI_207.pt', 'Breast_MRI_208.pt', 'Breast_MRI_209.pt', 'Breast_MRI_210.pt', 'Breast_MRI_211.pt', 'Breast_MRI_212.pt', 'Breast_MRI_213.pt', 'Breast_MRI_214.pt', 'Breast_MRI_215.pt', 'Breast_MRI_216.pt', 'Breast_MRI_217.pt', 'Breast_MRI_218.pt', 'Breast_MRI_219.pt', 'Breast_MRI_220.pt', 'Breast_MRI_221.pt', 'Breast_MRI_222.pt', 'Breast_MRI_223.pt', 'Breast_MRI_224.pt', 'Breast_MRI_225.pt', 'Breast_MRI_226.pt', 'Breast_MRI_227.pt', 'Breast_MRI_228.pt', 'Breast_MRI_229.pt', 'Breast_MRI_230.pt', 'Breast_MRI_231.pt', 'Breast_MRI_232.pt', 'Breast_MRI_233.pt', 'Breast_MRI_234.pt', 'Breast_MRI_235.pt', 'Breast_MRI_236.pt', 'Breast_MRI_237.pt', 'Breast_MRI_238.pt', 'Breast_MRI_239.pt', 'Breast_MRI_240.pt', 'Breast_MRI_241.pt', 'Breast_MRI_242.pt', 'Breast_MRI_243.pt', 'Breast_MRI_244.pt', 'Breast_MRI_245.pt', 'Breast_MRI_246.pt', 'Breast_MRI_247.pt', 'Breast_MRI_248.pt', 'Breast_MRI_249.pt', 'Breast_MRI_251.pt', 'Breast_MRI_252.pt', 'Breast_MRI_253.pt', 'Breast_MRI_254.pt', 'Breast_MRI_255.pt', 'Breast_MRI_256.pt', 'Breast_MRI_257.pt', 'Breast_MRI_258.pt', 'Breast_MRI_259.pt', 'Breast_MRI_260.pt', 'Breast_MRI_261.pt', 'Breast_MRI_262.pt', 'Breast_MRI_263.pt', 'Breast_MRI_264.pt', 'Breast_MRI_265.pt', 'Breast_MRI_266.pt', 'Breast_MRI_267.pt', 'Breast_MRI_268.pt', 'Breast_MRI_269.pt', 'Breast_MRI_270.pt', 'Breast_MRI_271.pt', 'Breast_MRI_272.pt', 'Breast_MRI_273.pt', 'Breast_MRI_274.pt', 'Breast_MRI_275.pt', 'Breast_MRI_276.pt', 'Breast_MRI_277.pt', 'Breast_MRI_278.pt', 'Breast_MRI_279.pt', 'Breast_MRI_280.pt', 'Breast_MRI_281.pt', 'Breast_MRI_282.pt', 'Breast_MRI_283.pt', 'Breast_MRI_284.pt', 'Breast_MRI_285.pt', 'Breast_MRI_286.pt', 'Breast_MRI_287.pt', 'Breast_MRI_288.pt', 'Breast_MRI_289.pt', 'Breast_MRI_290.pt', 'Breast_MRI_291.pt', 'Breast_MRI_292.pt', 'Breast_MRI_293.pt', 'Breast_MRI_294.pt', 'Breast_MRI_295.pt', 'Breast_MRI_296.pt', 'Breast_MRI_297.pt', 'Breast_MRI_298.pt', 'Breast_MRI_299.pt', 'Breast_MRI_301.pt', 'Breast_MRI_302.pt', 'Breast_MRI_303.pt', 'Breast_MRI_304.pt', 'Breast_MRI_305.pt', 'Breast_MRI_306.pt', 'Breast_MRI_307.pt', 'Breast_MRI_308.pt', 'Breast_MRI_309.pt', 'Breast_MRI_310.pt', 'Breast_MRI_311.pt', 'Breast_MRI_312.pt', 'Breast_MRI_313.pt', 'Breast_MRI_314.pt', 'Breast_MRI_315.pt', 'Breast_MRI_316.pt', 'Breast_MRI_317.pt', 'Breast_MRI_318.pt', 'Breast_MRI_319.pt', 'Breast_MRI_320.pt', 'Breast_MRI_321.pt', 'Breast_MRI_322.pt', 'Breast_MRI_323.pt', 'Breast_MRI_324.pt', 'Breast_MRI_325.pt', 'Breast_MRI_326.pt', 'Breast_MRI_327.pt', 'Breast_MRI_328.pt', 'Breast_MRI_329.pt', 'Breast_MRI_330.pt', 'Breast_MRI_331.pt', 'Breast_MRI_332.pt', 'Breast_MRI_333.pt', 'Breast_MRI_334.pt', 'Breast_MRI_335.pt', 'Breast_MRI_336.pt', 'Breast_MRI_337.pt', 'Breast_MRI_338.pt', 'Breast_MRI_339.pt', 'Breast_MRI_340.pt', 'Breast_MRI_341.pt', 'Breast_MRI_342.pt', 'Breast_MRI_343.pt', 'Breast_MRI_344.pt', 'Breast_MRI_345.pt', 'Breast_MRI_346.pt', 'Breast_MRI_347.pt', 'Breast_MRI_348.pt', 'Breast_MRI_349.pt', 'Breast_MRI_350.pt', 'Breast_MRI_351.pt', 'Breast_MRI_352.pt', 'Breast_MRI_353.pt', 'Breast_MRI_354.pt', 'Breast_MRI_355.pt', 'Breast_MRI_356.pt', 'Breast_MRI_357.pt', 'Breast_MRI_358.pt', 'Breast_MRI_359.pt', 'Breast_MRI_360.pt', 'Breast_MRI_361.pt', 'Breast_MRI_362.pt', 'Breast_MRI_363.pt', 'Breast_MRI_364.pt', 'Breast_MRI_365.pt', 'Breast_MRI_366.pt', 'Breast_MRI_367.pt', 'Breast_MRI_368.pt', 'Breast_MRI_369.pt', 'Breast_MRI_370.pt', 'Breast_MRI_371.pt', 'Breast_MRI_372.pt', 'Breast_MRI_373.pt', 'Breast_MRI_374.pt', 'Breast_MRI_375.pt', 'Breast_MRI_376.pt', 'Breast_MRI_377.pt', 'Breast_MRI_378.pt', 'Breast_MRI_379.pt', 'Breast_MRI_380.pt', 'Breast_MRI_381.pt', 'Breast_MRI_382.pt', 'Breast_MRI_383.pt', 'Breast_MRI_384.pt', 'Breast_MRI_385.pt', 'Breast_MRI_386.pt', 'Breast_MRI_387.pt', 'Breast_MRI_388.pt', 'Breast_MRI_389.pt', 'Breast_MRI_390.pt', 'Breast_MRI_391.pt', 'Breast_MRI_392.pt', 'Breast_MRI_393.pt', 'Breast_MRI_394.pt', 'Breast_MRI_395.pt', 'Breast_MRI_396.pt', 'Breast_MRI_397.pt', 'Breast_MRI_398.pt', 'Breast_MRI_399.pt', 'Breast_MRI_400.pt', 'Breast_MRI_401.pt', 'Breast_MRI_402.pt', 'Breast_MRI_403.pt', 'Breast_MRI_404.pt', 'Breast_MRI_405.pt', 'Breast_MRI_406.pt', 'Breast_MRI_407.pt', 'Breast_MRI_408.pt', 'Breast_MRI_409.pt', 'Breast_MRI_410.pt', 'Breast_MRI_411.pt', 'Breast_MRI_412.pt', 'Breast_MRI_413.pt', 'Breast_MRI_414.pt', 'Breast_MRI_415.pt', 'Breast_MRI_416.pt', 'Breast_MRI_417.pt', 'Breast_MRI_418.pt', 'Breast_MRI_419.pt', 'Breast_MRI_420.pt', 'Breast_MRI_421.pt', 'Breast_MRI_422.pt', 'Breast_MRI_423.pt', 'Breast_MRI_424.pt', 'Breast_MRI_425.pt', 'Breast_MRI_426.pt', 'Breast_MRI_427.pt', 'Breast_MRI_428.pt', 'Breast_MRI_429.pt', 'Breast_MRI_430.pt', 'Breast_MRI_431.pt', 'Breast_MRI_432.pt', 'Breast_MRI_433.pt', 'Breast_MRI_434.pt', 'Breast_MRI_435.pt', 'Breast_MRI_436.pt', 'Breast_MRI_437.pt', 'Breast_MRI_438.pt', 'Breast_MRI_439.pt', 'Breast_MRI_440.pt', 'Breast_MRI_441.pt', 'Breast_MRI_442.pt', 'Breast_MRI_443.pt', 'Breast_MRI_444.pt', 'Breast_MRI_445.pt', 'Breast_MRI_446.pt', 'Breast_MRI_447.pt', 'Breast_MRI_448.pt', 'Breast_MRI_449.pt', 'Breast_MRI_450.pt', 'Breast_MRI_451.pt', 'Breast_MRI_452.pt', 'Breast_MRI_453.pt', 'Breast_MRI_454.pt', 'Breast_MRI_455.pt', 'Breast_MRI_456.pt', 'Breast_MRI_457.pt', 'Breast_MRI_458.pt', 'Breast_MRI_459.pt', 'Breast_MRI_460.pt', 'Breast_MRI_461.pt', 'Breast_MRI_462.pt', 'Breast_MRI_463.pt', 'Breast_MRI_464.pt', 'Breast_MRI_465.pt', 'Breast_MRI_466.pt', 'Breast_MRI_467.pt', 'Breast_MRI_468.pt', 'Breast_MRI_469.pt', 'Breast_MRI_470.pt', 'Breast_MRI_471.pt', 'Breast_MRI_472.pt', 'Breast_MRI_473.pt', 'Breast_MRI_474.pt', 'Breast_MRI_475.pt', 'Breast_MRI_476.pt', 'Breast_MRI_477.pt', 'Breast_MRI_478.pt', 'Breast_MRI_479.pt', 'Breast_MRI_480.pt', 'Breast_MRI_481.pt', 'Breast_MRI_482.pt', 'Breast_MRI_483.pt', 'Breast_MRI_484.pt', 'Breast_MRI_485.pt', 'Breast_MRI_486.pt', 'Breast_MRI_487.pt', 'Breast_MRI_488.pt', 'Breast_MRI_489.pt', 'Breast_MRI_490.pt', 'Breast_MRI_491.pt', 'Breast_MRI_492.pt', 'Breast_MRI_493.pt', 'Breast_MRI_494.pt', 'Breast_MRI_495.pt', 'Breast_MRI_496.pt', 'Breast_MRI_497.pt', 'Breast_MRI_498.pt', 'Breast_MRI_499.pt', 'Breast_MRI_500.pt', 'Breast_MRI_501.pt', 'Breast_MRI_502.pt', 'Breast_MRI_503.pt', 'Breast_MRI_504.pt', 'Breast_MRI_505.pt', 'Breast_MRI_506.pt', 'Breast_MRI_507.pt', 'Breast_MRI_508.pt', 'Breast_MRI_509.pt', 'Breast_MRI_510.pt', 'Breast_MRI_511.pt', 'Breast_MRI_512.pt', 'Breast_MRI_513.pt', 'Breast_MRI_514.pt', 'Breast_MRI_515.pt', 'Breast_MRI_516.pt', 'Breast_MRI_517.pt', 'Breast_MRI_518.pt', 'Breast_MRI_519.pt', 'Breast_MRI_520.pt', 'Breast_MRI_521.pt', 'Breast_MRI_522.pt', 'Breast_MRI_523.pt', 'Breast_MRI_524.pt', 'Breast_MRI_525.pt', 'Breast_MRI_526.pt', 'Breast_MRI_527.pt', 'Breast_MRI_528.pt', 'Breast_MRI_529.pt', 'Breast_MRI_530.pt', 'Breast_MRI_531.pt', 'Breast_MRI_532.pt', 'Breast_MRI_533.pt', 'Breast_MRI_534.pt', 'Breast_MRI_535.pt', 'Breast_MRI_536.pt', 'Breast_MRI_537.pt', 'Breast_MRI_538.pt', 'Breast_MRI_539.pt', 'Breast_MRI_540.pt', 'Breast_MRI_541.pt', 'Breast_MRI_542.pt', 'Breast_MRI_543.pt', 'Breast_MRI_544.pt', 'Breast_MRI_545.pt', 'Breast_MRI_546.pt', 'Breast_MRI_547.pt', 'Breast_MRI_548.pt', 'Breast_MRI_549.pt', 'Breast_MRI_550.pt', 'Breast_MRI_551.pt', 'Breast_MRI_552.pt', 'Breast_MRI_553.pt', 'Breast_MRI_554.pt', 'Breast_MRI_555.pt', 'Breast_MRI_556.pt', 'Breast_MRI_557.pt', 'Breast_MRI_558.pt', 'Breast_MRI_559.pt', 'Breast_MRI_560.pt', 'Breast_MRI_561.pt', 'Breast_MRI_562.pt', 'Breast_MRI_563.pt', 'Breast_MRI_564.pt', 'Breast_MRI_565.pt', 'Breast_MRI_566.pt', 'Breast_MRI_567.pt', 'Breast_MRI_568.pt', 'Breast_MRI_569.pt', 'Breast_MRI_570.pt', 'Breast_MRI_571.pt', 'Breast_MRI_572.pt', 'Breast_MRI_573.pt', 'Breast_MRI_574.pt', 'Breast_MRI_575.pt', 'Breast_MRI_576.pt', 'Breast_MRI_578.pt', 'Breast_MRI_579.pt', 'Breast_MRI_580.pt', 'Breast_MRI_581.pt', 'Breast_MRI_582.pt', 'Breast_MRI_583.pt', 'Breast_MRI_584.pt', 'Breast_MRI_585.pt', 'Breast_MRI_586.pt', 'Breast_MRI_587.pt', 'Breast_MRI_588.pt', 'Breast_MRI_589.pt', 'Breast_MRI_590.pt', 'Breast_MRI_591.pt', 'Breast_MRI_592.pt', 'Breast_MRI_593.pt', 'Breast_MRI_594.pt', 'Breast_MRI_595.pt', 'Breast_MRI_596.pt', 'Breast_MRI_597.pt', 'Breast_MRI_598.pt', 'Breast_MRI_599.pt', 'Breast_MRI_600.pt', 'Breast_MRI_601.pt', 'Breast_MRI_602.pt', 'Breast_MRI_603.pt', 'Breast_MRI_604.pt', 'Breast_MRI_605.pt', 'Breast_MRI_606.pt', 'Breast_MRI_607.pt', 'Breast_MRI_608.pt', 'Breast_MRI_609.pt', 'Breast_MRI_610.pt', 'Breast_MRI_611.pt', 'Breast_MRI_612.pt', 'Breast_MRI_613.pt', 'Breast_MRI_614.pt', 'Breast_MRI_615.pt', 'Breast_MRI_616.pt', 'Breast_MRI_617.pt', 'Breast_MRI_618.pt', 'Breast_MRI_619.pt', 'Breast_MRI_620.pt', 'Breast_MRI_621.pt', 'Breast_MRI_622.pt', 'Breast_MRI_623.pt', 'Breast_MRI_624.pt', 'Breast_MRI_625.pt', 'Breast_MRI_626.pt', 'Breast_MRI_627.pt', 'Breast_MRI_628.pt', 'Breast_MRI_629.pt', 'Breast_MRI_630.pt', 'Breast_MRI_631.pt', 'Breast_MRI_632.pt', 'Breast_MRI_633.pt', 'Breast_MRI_634.pt', 'Breast_MRI_635.pt', 'Breast_MRI_636.pt', 'Breast_MRI_637.pt', 'Breast_MRI_638.pt', 'Breast_MRI_639.pt', 'Breast_MRI_640.pt', 'Breast_MRI_641.pt', 'Breast_MRI_642.pt', 'Breast_MRI_643.pt', 'Breast_MRI_644.pt', 'Breast_MRI_645.pt', 'Breast_MRI_647.pt', 'Breast_MRI_648.pt', 'Breast_MRI_649.pt', 'Breast_MRI_650.pt', 'Breast_MRI_651.pt', 'Breast_MRI_652.pt', 'Breast_MRI_653.pt', 'Breast_MRI_654.pt', 'Breast_MRI_655.pt', 'Breast_MRI_656.pt', 'Breast_MRI_657.pt', 'Breast_MRI_658.pt', 'Breast_MRI_659.pt', 'Breast_MRI_660.pt', 'Breast_MRI_661.pt', 'Breast_MRI_662.pt', 'Breast_MRI_663.pt', 'Breast_MRI_664.pt', 'Breast_MRI_665.pt', 'Breast_MRI_666.pt', 'Breast_MRI_667.pt', 'Breast_MRI_668.pt', 'Breast_MRI_669.pt', 'Breast_MRI_670.pt', 'Breast_MRI_672.pt', 'Breast_MRI_673.pt', 'Breast_MRI_674.pt', 'Breast_MRI_675.pt', 'Breast_MRI_676.pt', 'Breast_MRI_677.pt', 'Breast_MRI_678.pt', 'Breast_MRI_679.pt', 'Breast_MRI_680.pt', 'Breast_MRI_681.pt', 'Breast_MRI_682.pt', 'Breast_MRI_683.pt', 'Breast_MRI_684.pt', 'Breast_MRI_685.pt', 'Breast_MRI_686.pt', 'Breast_MRI_687.pt', 'Breast_MRI_688.pt', 'Breast_MRI_689.pt', 'Breast_MRI_690.pt', 'Breast_MRI_691.pt', 'Breast_MRI_692.pt', 'Breast_MRI_693.pt', 'Breast_MRI_694.pt', 'Breast_MRI_695.pt', 'Breast_MRI_696.pt', 'Breast_MRI_697.pt', 'Breast_MRI_698.pt', 'Breast_MRI_699.pt', 'Breast_MRI_700.pt', 'Breast_MRI_701.pt', 'Breast_MRI_702.pt', 'Breast_MRI_703.pt', 'Breast_MRI_704.pt', 'Breast_MRI_705.pt', 'Breast_MRI_706.pt', 'Breast_MRI_707.pt', 'Breast_MRI_708.pt', 'Breast_MRI_709.pt', 'Breast_MRI_710.pt', 'Breast_MRI_711.pt', 'Breast_MRI_712.pt', 'Breast_MRI_713.pt', 'Breast_MRI_714.pt', 'Breast_MRI_715.pt', 'Breast_MRI_716.pt', 'Breast_MRI_717.pt', 'Breast_MRI_718.pt', 'Breast_MRI_719.pt', 'Breast_MRI_720.pt', 'Breast_MRI_721.pt', 'Breast_MRI_722.pt', 'Breast_MRI_723.pt', 'Breast_MRI_724.pt', 'Breast_MRI_725.pt', 'Breast_MRI_726.pt', 'Breast_MRI_727.pt', 'Breast_MRI_728.pt', 'Breast_MRI_729.pt', 'Breast_MRI_730.pt', 'Breast_MRI_731.pt', 'Breast_MRI_732.pt', 'Breast_MRI_733.pt', 'Breast_MRI_734.pt', 'Breast_MRI_735.pt', 'Breast_MRI_736.pt', 'Breast_MRI_737.pt', 'Breast_MRI_738.pt', 'Breast_MRI_739.pt', 'Breast_MRI_740.pt', 'Breast_MRI_741.pt', 'Breast_MRI_742.pt', 'Breast_MRI_743.pt', 'Breast_MRI_744.pt', 'Breast_MRI_745.pt', 'Breast_MRI_746.pt', 'Breast_MRI_747.pt', 'Breast_MRI_748.pt', 'Breast_MRI_749.pt', 'Breast_MRI_750.pt', 'Breast_MRI_751.pt', 'Breast_MRI_752.pt', 'Breast_MRI_753.pt', 'Breast_MRI_754.pt', 'Breast_MRI_755.pt', 'Breast_MRI_756.pt', 'Breast_MRI_757.pt', 'Breast_MRI_758.pt', 'Breast_MRI_759.pt', 'Breast_MRI_760.pt', 'Breast_MRI_761.pt', 'Breast_MRI_762.pt', 'Breast_MRI_763.pt', 'Breast_MRI_764.pt', 'Breast_MRI_765.pt', 'Breast_MRI_766.pt', 'Breast_MRI_767.pt', 'Breast_MRI_768.pt', 'Breast_MRI_769.pt', 'Breast_MRI_770.pt', 'Breast_MRI_771.pt', 'Breast_MRI_772.pt', 'Breast_MRI_773.pt', 'Breast_MRI_774.pt', 'Breast_MRI_775.pt', 'Breast_MRI_776.pt', 'Breast_MRI_777.pt', 'Breast_MRI_778.pt', 'Breast_MRI_779.pt', 'Breast_MRI_780.pt', 'Breast_MRI_781.pt', 'Breast_MRI_782.pt', 'Breast_MRI_783.pt', 'Breast_MRI_784.pt', 'Breast_MRI_785.pt', 'Breast_MRI_786.pt', 'Breast_MRI_787.pt', 'Breast_MRI_788.pt', 'Breast_MRI_789.pt', 'Breast_MRI_790.pt', 'Breast_MRI_791.pt', 'Breast_MRI_792.pt', 'Breast_MRI_793.pt', 'Breast_MRI_794.pt', 'Breast_MRI_795.pt', 'Breast_MRI_796.pt', 'Breast_MRI_797.pt', 'Breast_MRI_798.pt', 'Breast_MRI_799.pt', 'Breast_MRI_800.pt', 'Breast_MRI_801.pt', 'Breast_MRI_802.pt', 'Breast_MRI_803.pt', 'Breast_MRI_804.pt', 'Breast_MRI_805.pt', 'Breast_MRI_806.pt', 'Breast_MRI_807.pt', 'Breast_MRI_808.pt', 'Breast_MRI_809.pt', 'Breast_MRI_810.pt', 'Breast_MRI_811.pt', 'Breast_MRI_812.pt', 'Breast_MRI_813.pt', 'Breast_MRI_814.pt', 'Breast_MRI_815.pt', 'Breast_MRI_816.pt', 'Breast_MRI_817.pt', 'Breast_MRI_818.pt', 'Breast_MRI_819.pt', 'Breast_MRI_820.pt', 'Breast_MRI_821.pt', 'Breast_MRI_822.pt', 'Breast_MRI_823.pt', 'Breast_MRI_824.pt', 'Breast_MRI_825.pt', 'Breast_MRI_826.pt', 'Breast_MRI_827.pt', 'Breast_MRI_828.pt', 'Breast_MRI_829.pt', 'Breast_MRI_830.pt', 'Breast_MRI_831.pt', 'Breast_MRI_832.pt', 'Breast_MRI_833.pt', 'Breast_MRI_834.pt', 'Breast_MRI_835.pt', 'Breast_MRI_836.pt', 'Breast_MRI_837.pt', 'Breast_MRI_838.pt', 'Breast_MRI_839.pt', 'Breast_MRI_840.pt', 'Breast_MRI_841.pt', 'Breast_MRI_842.pt', 'Breast_MRI_843.pt', 'Breast_MRI_844.pt', 'Breast_MRI_845.pt', 'Breast_MRI_846.pt', 'Breast_MRI_847.pt', 'Breast_MRI_848.pt', 'Breast_MRI_849.pt', 'Breast_MRI_850.pt', 'Breast_MRI_851.pt', 'Breast_MRI_852.pt', 'Breast_MRI_853.pt', 'Breast_MRI_854.pt', 'Breast_MRI_855.pt', 'Breast_MRI_856.pt', 'Breast_MRI_857.pt', 'Breast_MRI_858.pt', 'Breast_MRI_859.pt', 'Breast_MRI_860.pt', 'Breast_MRI_861.pt', 'Breast_MRI_862.pt', 'Breast_MRI_863.pt', 'Breast_MRI_864.pt', 'Breast_MRI_865.pt', 'Breast_MRI_866.pt', 'Breast_MRI_867.pt', 'Breast_MRI_868.pt', 'Breast_MRI_869.pt', 'Breast_MRI_870.pt', 'Breast_MRI_871.pt', 'Breast_MRI_872.pt', 'Breast_MRI_873.pt', 'Breast_MRI_874.pt', 'Breast_MRI_875.pt', 'Breast_MRI_876.pt', 'Breast_MRI_877.pt', 'Breast_MRI_878.pt', 'Breast_MRI_879.pt', 'Breast_MRI_880.pt', 'Breast_MRI_881.pt', 'Breast_MRI_882.pt', 'Breast_MRI_883.pt', 'Breast_MRI_884.pt', 'Breast_MRI_885.pt', 'Breast_MRI_886.pt', 'Breast_MRI_887.pt', 'Breast_MRI_888.pt', 'Breast_MRI_889.pt', 'Breast_MRI_890.pt', 'Breast_MRI_891.pt', 'Breast_MRI_892.pt', 'Breast_MRI_893.pt', 'Breast_MRI_894.pt', 'Breast_MRI_895.pt', 'Breast_MRI_896.pt', 'Breast_MRI_897.pt', 'Breast_MRI_898.pt', 'Breast_MRI_899.pt', 'Breast_MRI_900.pt', 'Breast_MRI_901.pt', 'Breast_MRI_902.pt', 'Breast_MRI_903.pt', 'Breast_MRI_904.pt', 'Breast_MRI_905.pt', 'Breast_MRI_906.pt', 'Breast_MRI_907.pt', 'Breast_MRI_908.pt', 'Breast_MRI_909.pt', 'Breast_MRI_910.pt', 'Breast_MRI_911.pt', 'Breast_MRI_912.pt', 'Breast_MRI_913.pt', 'Breast_MRI_914.pt', 'Breast_MRI_915.pt', 'Breast_MRI_916.pt', 'Breast_MRI_917.pt', 'Breast_MRI_918.pt', 'Breast_MRI_919.pt', 'Breast_MRI_920.pt', 'Breast_MRI_921.pt', 'Breast_MRI_922.pt']
row 0: Patient ID                                  Breast_MRI_001
Image Position of Patient (X)                     0.068046
Date of Birth (Days)                              0.705925
Image Position of Patient (Z)                     0.746579
Image Position of Patient (Y)                     0.180863
Days to MRI (From the Date of Diagnosis)          0.255952
TR (Repetition Time)                              0.150454
Tumor Grade(N)\n(Nuclear)_3.0                            0
TE (Echo Time)                                    0.073041
Oncotype score                                    0.241581
Tumor Grade(M)\n(Mitotic)_1.0                            1
Tumor Progression_0                                      1
Tumor Progression_1                                      0
Name: 0, dtype: object
Epoch 1/50
Training labels: tensor([[1., 0.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.2232666015625, -0.269287109375]
[0.264892578125, -0.11944580078125]
[0.2890625, -0.401611328125]
[0.3154296875, -0.1317138671875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.2232666015625, -0.269287109375]
[0.264892578125, -0.11944580078125]
[0.2890625, -0.401611328125]
[0.3154296875, -0.1317138671875]
This is the real loss :  tensor(0.5824, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:   Epoch  Batch  ...      Loss                                        True Value
0     0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[1 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.4765625, -0.07904052734375]
[0.367919921875, -0.119384765625]
[0.429443359375, -0.02886962890625]
[0.40234375, -0.05828857421875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.4765625, -0.07904052734375]
[0.367919921875, -0.119384765625]
[0.429443359375, -0.02886962890625]
[0.40234375, -0.05828857421875]
This is the real loss :  tensor(0.5485, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:   Epoch  Batch  ...      Loss                                        True Value
0     0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1     0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[2 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.22119140625, 0.11004638671875]
[0.1826171875, 0.1278076171875]
[0.7265625, -0.0236053466796875]
[0.548828125, -0.0196075439453125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.22119140625, 0.11004638671875]
[0.1826171875, 0.1278076171875]
[0.7265625, -0.0236053466796875]
[0.548828125, -0.0196075439453125]
This is the real loss :  tensor(0.4268, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:   Epoch  Batch  ...      Loss                                        True Value
0     0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1     0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2     0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[3 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.08111572265625, 0.08514404296875]
[0.140380859375, 0.26416015625]
[0.63720703125, 0.07562255859375]
[0.6396484375, 0.2110595703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.08111572265625, 0.08514404296875]
[0.140380859375, 0.26416015625]
[0.63720703125, 0.07562255859375]
[0.6396484375, 0.2110595703125]
This is the real loss :  tensor(0.4621, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:   Epoch  Batch  ...      Loss                                        True Value
0     0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1     0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2     0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3     0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[4 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.2127685546875, 0.14794921875]
[0.51025390625, 0.216796875]
[0.45263671875, 0.2861328125]
[0.213134765625, 0.2239990234375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.2127685546875, 0.14794921875]
[0.51025390625, 0.216796875]
[0.45263671875, 0.2861328125]
[0.213134765625, 0.2239990234375]
This is the real loss :  tensor(0.2609, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:   Epoch  Batch  ...      Loss                                        True Value
0     0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1     0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2     0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3     0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4     0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[5 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.200927734375, 0.162353515625]
[0.1456298828125, 0.156494140625]
[0.33154296875, 0.357421875]
[0.58154296875, 0.2763671875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.200927734375, 0.162353515625]
[0.1456298828125, 0.156494140625]
[0.33154296875, 0.357421875]
[0.58154296875, 0.2763671875]
This is the real loss :  tensor(0.3574, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:   Epoch  Batch  ...      Loss                                        True Value
0     0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1     0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2     0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3     0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4     0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5     0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[6 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.26025390625, 0.338134765625]
[0.2332763671875, 0.364990234375]
[0.260009765625, 0.18359375]
[0.50634765625, 0.264892578125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.26025390625, 0.338134765625]
[0.2332763671875, 0.364990234375]
[0.260009765625, 0.18359375]
[0.50634765625, 0.264892578125]
This is the real loss :  tensor(0.2927, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:   Epoch  Batch  ...      Loss                                        True Value
0     0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1     0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2     0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3     0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4     0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5     0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6     0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[7 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.119140625, 0.373291015625]
[0.16357421875, 0.250732421875]
[0.4560546875, 0.36083984375]
[0.37451171875, 0.235107421875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.119140625, 0.373291015625]
[0.16357421875, 0.250732421875]
[0.4560546875, 0.36083984375]
[0.37451171875, 0.235107421875]
This is the real loss :  tensor(0.2335, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:   Epoch  Batch  ...      Loss                                        True Value
0     0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1     0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2     0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3     0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4     0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5     0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6     0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7     0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[8 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.215576171875, 0.40087890625]
[0.52587890625, 0.26123046875]
[0.212158203125, 0.2734375]
[0.219482421875, 0.3916015625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.215576171875, 0.40087890625]
[0.52587890625, 0.26123046875]
[0.212158203125, 0.2734375]
[0.219482421875, 0.3916015625]
This is the real loss :  tensor(0.2265, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:   Epoch  Batch  ...      Loss                                        True Value
0     0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1     0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2     0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3     0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4     0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5     0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6     0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7     0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8     0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[9 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.361328125, 0.1300048828125]
[0.1334228515625, 0.2685546875]
[0.56884765625, 0.276611328125]
[0.030731201171875, 0.61572265625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.361328125, 0.1300048828125]
[0.1334228515625, 0.2685546875]
[0.56884765625, 0.276611328125]
[0.030731201171875, 0.61572265625]
This is the real loss :  tensor(0.2466, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:   Epoch  Batch  ...      Loss                                        True Value
0     0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1     0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2     0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3     0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4     0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5     0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6     0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7     0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8     0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9     0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[10 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.0472412109375, 0.47998046875]
[0.0782470703125, 0.47412109375]
[0.9091796875, 0.208740234375]
[0.254638671875, 0.32470703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.0472412109375, 0.47998046875]
[0.0782470703125, 0.47412109375]
[0.9091796875, 0.208740234375]
[0.254638671875, 0.32470703125]
This is the real loss :  tensor(0.1410, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[11 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.2489013671875, 0.395263671875]
[0.468994140625, 0.3046875]
[0.43896484375, 0.384765625]
[0.1607666015625, 0.6015625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.2489013671875, 0.395263671875]
[0.468994140625, 0.3046875]
[0.43896484375, 0.384765625]
[0.1607666015625, 0.6015625]
This is the real loss :  tensor(0.2223, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[12 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.50341796875, 0.25927734375]
[0.4658203125, 0.1982421875]
[0.01392364501953125, 0.5986328125]
[0.084228515625, 0.5419921875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.50341796875, 0.25927734375]
[0.4658203125, 0.1982421875]
[0.01392364501953125, 0.5986328125]
[0.084228515625, 0.5419921875]
This is the real loss :  tensor(0.2550, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[13 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.218505859375, 0.642578125]
[0.1446533203125, 0.60400390625]
[0.54296875, 0.37255859375]
[0.323486328125, 0.279541015625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.218505859375, 0.642578125]
[0.1446533203125, 0.60400390625]
[0.54296875, 0.37255859375]
[0.323486328125, 0.279541015625]
This is the real loss :  tensor(0.2082, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[14 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.1719970703125, 0.501953125]
[0.060699462890625, 0.82275390625]
[0.474609375, 0.2237548828125]
[0.384765625, 0.38916015625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1719970703125, 0.501953125]
[0.060699462890625, 0.82275390625]
[0.474609375, 0.2237548828125]
[0.384765625, 0.38916015625]
This is the real loss :  tensor(0.2077, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[15 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[-0.021728515625, 0.8115234375]
[0.560546875, 0.46630859375]
[0.166748046875, 0.429443359375]
[0.280029296875, 0.347900390625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.021728515625, 0.8115234375]
[0.560546875, 0.46630859375]
[0.166748046875, 0.429443359375]
[0.280029296875, 0.347900390625]
This is the real loss :  tensor(0.2522, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[16 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.149658203125, 0.556640625]
[0.205810546875, 0.5302734375]
[0.48974609375, 0.52392578125]
[0.07843017578125, 0.54345703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.149658203125, 0.556640625]
[0.205810546875, 0.5302734375]
[0.48974609375, 0.52392578125]
[0.07843017578125, 0.54345703125]
This is the real loss :  tensor(0.1454, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[17 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.1209716796875, 0.6591796875]
[0.386962890625, 0.480712890625]
[0.0955810546875, 0.6396484375]
[0.2529296875, 0.439453125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1209716796875, 0.6591796875]
[0.386962890625, 0.480712890625]
[0.0955810546875, 0.6396484375]
[0.2529296875, 0.439453125]
This is the real loss :  tensor(0.1334, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[18 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.07635498046875, 0.63232421875]
[0.2259521484375, 0.55029296875]
[0.27587890625, 0.60693359375]
[0.27099609375, 0.69384765625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.07635498046875, 0.63232421875]
[0.2259521484375, 0.55029296875]
[0.27587890625, 0.60693359375]
[0.27099609375, 0.69384765625]
This is the real loss :  tensor(0.0990, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[19 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.329833984375, 0.59423828125]
[0.055389404296875, 0.7255859375]
[0.2088623046875, 0.60009765625]
[0.2039794921875, 0.63427734375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.329833984375, 0.59423828125]
[0.055389404296875, 0.7255859375]
[0.2088623046875, 0.60009765625]
[0.2039794921875, 0.63427734375]
This is the real loss :  tensor(0.1574, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[20 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.1602783203125, 0.576171875]
[0.172119140625, 0.59423828125]
[0.01751708984375, 0.73681640625]
[0.365478515625, 0.66455078125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1602783203125, 0.576171875]
[0.172119140625, 0.59423828125]
[0.01751708984375, 0.73681640625]
[0.365478515625, 0.66455078125]
This is the real loss :  tensor(0.0894, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[21 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.1546630859375, 0.6640625]
[0.1883544921875, 0.5]
[0.09173583984375, 0.75048828125]
[0.1920166015625, 0.7373046875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1546630859375, 0.6640625]
[0.1883544921875, 0.5]
[0.09173583984375, 0.75048828125]
[0.1920166015625, 0.7373046875]
This is the real loss :  tensor(0.1528, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[22 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.11834716796875, 0.7900390625]
[0.11981201171875, 0.70068359375]
[0.033172607421875, 0.75048828125]
[0.32763671875, 0.5732421875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.11834716796875, 0.7900390625]
[0.11981201171875, 0.70068359375]
[0.033172607421875, 0.75048828125]
[0.32763671875, 0.5732421875]
This is the real loss :  tensor(0.0644, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[23 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.084716796875, 0.68994140625]
[0.03240966796875, 0.787109375]
[0.1976318359375, 0.69384765625]
[0.158935546875, 0.8408203125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.084716796875, 0.68994140625]
[0.03240966796875, 0.787109375]
[0.1976318359375, 0.69384765625]
[0.158935546875, 0.8408203125]
This is the real loss :  tensor(0.3544, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[24 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.0950927734375, 0.73291015625]
[0.275634765625, 0.69580078125]
[0.05438232421875, 0.77099609375]
[0.0259857177734375, 0.794921875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.0950927734375, 0.73291015625]
[0.275634765625, 0.69580078125]
[0.05438232421875, 0.77099609375]
[0.0259857177734375, 0.794921875]
This is the real loss :  tensor(0.1484, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[25 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[-0.085205078125, 0.7021484375]
[0.23974609375, 0.90234375]
[0.1397705078125, 0.64599609375]
[0.177001953125, 0.73583984375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.085205078125, 0.7021484375]
[0.23974609375, 0.90234375]
[0.1397705078125, 0.64599609375]
[0.177001953125, 0.73583984375]
This is the real loss :  tensor(0.2168, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[26 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.09661865234375, 0.9326171875]
[0.01427459716796875, 0.83837890625]
[0.265869140625, 0.62158203125]
[0.10693359375, 0.72216796875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.09661865234375, 0.9326171875]
[0.01427459716796875, 0.83837890625]
[0.265869140625, 0.62158203125]
[0.10693359375, 0.72216796875]
This is the real loss :  tensor(0.4916, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[27 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.150634765625, 0.7568359375]
[0.0665283203125, 0.84716796875]
[0.318359375, 0.59033203125]
[0.0307769775390625, 0.810546875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.150634765625, 0.7568359375]
[0.0665283203125, 0.84716796875]
[0.318359375, 0.59033203125]
[0.0307769775390625, 0.810546875]
This is the real loss :  tensor(0.1199, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.119946  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[28 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[-0.05194091796875, 1.009765625]
[0.10577392578125, 0.69189453125]
[0.425537109375, 0.755859375]
[0.172607421875, 0.68896484375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.05194091796875, 1.009765625]
[0.10577392578125, 0.69189453125]
[0.425537109375, 0.755859375]
[0.172607421875, 0.68896484375]
This is the real loss :  tensor(0.2886, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.119946  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
28     0    NaN  ...  0.288627  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[29 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.11444091796875, 0.83056640625]
[0.0223388671875, 0.671875]
[0.042938232421875, 0.84130859375]
[0.5458984375, 0.63330078125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.11444091796875, 0.83056640625]
[0.0223388671875, 0.671875]
[0.042938232421875, 0.84130859375]
[0.5458984375, 0.63330078125]
This is the real loss :  tensor(0.4394, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.119946  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
28     0    NaN  ...  0.288627  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.439450  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[30 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.11627197265625, 0.8203125]
[0.225830078125, 0.71630859375]
[0.424072265625, 0.52001953125]
[0.07232666015625, 0.8720703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.11627197265625, 0.8203125]
[0.225830078125, 0.71630859375]
[0.424072265625, 0.52001953125]
[0.07232666015625, 0.8720703125]
This is the real loss :  tensor(0.0761, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.119946  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
28     0    NaN  ...  0.288627  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.439450  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
30     0    NaN  ...  0.076138  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[31 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.44873046875, 0.53173828125]
[0.25634765625, 0.56494140625]
[0.034820556640625, 0.87109375]
[0.1400146484375, 0.76611328125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.44873046875, 0.53173828125]
[0.25634765625, 0.56494140625]
[0.034820556640625, 0.87109375]
[0.1400146484375, 0.76611328125]
This is the real loss :  tensor(0.1167, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.119946  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
28     0    NaN  ...  0.288627  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.439450  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
30     0    NaN  ...  0.076138  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
31     0    NaN  ...  0.116721  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[32 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.230712890625, 0.5166015625]
[0.040069580078125, 0.84423828125]
[0.51416015625, 0.64404296875]
[0.2105712890625, 0.8349609375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.230712890625, 0.5166015625]
[0.040069580078125, 0.84423828125]
[0.51416015625, 0.64404296875]
[0.2105712890625, 0.8349609375]
This is the real loss :  tensor(0.2530, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.119946  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
28     0    NaN  ...  0.288627  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.439450  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
30     0    NaN  ...  0.076138  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
31     0    NaN  ...  0.116721  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
32     0    NaN  ...  0.253024  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[33 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.48291015625, 0.54248046875]
[0.339111328125, 0.68994140625]
[0.0191192626953125, 0.9033203125]
[0.1390380859375, 0.53564453125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.48291015625, 0.54248046875]
[0.339111328125, 0.68994140625]
[0.0191192626953125, 0.9033203125]
[0.1390380859375, 0.53564453125]
This is the real loss :  tensor(0.2263, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.119946  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
28     0    NaN  ...  0.288627  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.439450  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
30     0    NaN  ...  0.076138  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
31     0    NaN  ...  0.116721  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
32     0    NaN  ...  0.253024  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
33     0    NaN  ...  0.226335  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[34 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.1502685546875, 0.66748046875]
[0.318603515625, 0.80029296875]
[0.267333984375, 0.61669921875]
[0.438720703125, 0.658203125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1502685546875, 0.66748046875]
[0.318603515625, 0.80029296875]
[0.267333984375, 0.61669921875]
[0.438720703125, 0.658203125]
This is the real loss :  tensor(0.2296, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.119946  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
28     0    NaN  ...  0.288627  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.439450  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
30     0    NaN  ...  0.076138  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
31     0    NaN  ...  0.116721  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
32     0    NaN  ...  0.253024  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
33     0    NaN  ...  0.226335  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
34     0    NaN  ...  0.229582  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[35 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.08306884765625, 0.73291015625]
[0.1943359375, 0.77978515625]
[0.38037109375, 0.57861328125]
[0.453125, 0.44287109375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.08306884765625, 0.73291015625]
[0.1943359375, 0.77978515625]
[0.38037109375, 0.57861328125]
[0.453125, 0.44287109375]
This is the real loss :  tensor(0.2878, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.119946  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
28     0    NaN  ...  0.288627  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.439450  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
30     0    NaN  ...  0.076138  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
31     0    NaN  ...  0.116721  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
32     0    NaN  ...  0.253024  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
33     0    NaN  ...  0.226335  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
34     0    NaN  ...  0.229582  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
35     0    NaN  ...  0.287768  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[36 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.52490234375, 0.51611328125]
[0.122314453125, 0.87841796875]
[0.3232421875, 0.537109375]
[0.2242431640625, 0.71923828125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.52490234375, 0.51611328125]
[0.122314453125, 0.87841796875]
[0.3232421875, 0.537109375]
[0.2242431640625, 0.71923828125]
This is the real loss :  tensor(0.1747, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.119946  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
28     0    NaN  ...  0.288627  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.439450  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
30     0    NaN  ...  0.076138  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
31     0    NaN  ...  0.116721  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
32     0    NaN  ...  0.253024  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
33     0    NaN  ...  0.226335  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
34     0    NaN  ...  0.229582  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
35     0    NaN  ...  0.287768  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
36     0    NaN  ...  0.174679  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[37 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.129638671875, 0.7158203125]
[0.51513671875, 0.380859375]
[0.183349609375, 0.72998046875]
[0.422607421875, 0.64306640625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.129638671875, 0.7158203125]
[0.51513671875, 0.380859375]
[0.183349609375, 0.72998046875]
[0.422607421875, 0.64306640625]
This is the real loss :  tensor(0.1113, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.119946  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
28     0    NaN  ...  0.288627  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.439450  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
30     0    NaN  ...  0.076138  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
31     0    NaN  ...  0.116721  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
32     0    NaN  ...  0.253024  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
33     0    NaN  ...  0.226335  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
34     0    NaN  ...  0.229582  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
35     0    NaN  ...  0.287768  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
36     0    NaN  ...  0.174679  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
37     0    NaN  ...  0.111280  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[38 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.2152099609375, 0.65869140625]
[0.708984375, 0.5126953125]
[0.1785888671875, 0.59033203125]
[0.10589599609375, 0.68505859375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.2152099609375, 0.65869140625]
[0.708984375, 0.5126953125]
[0.1785888671875, 0.59033203125]
[0.10589599609375, 0.68505859375]
This is the real loss :  tensor(0.3503, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.119946  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
28     0    NaN  ...  0.288627  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.439450  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
30     0    NaN  ...  0.076138  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
31     0    NaN  ...  0.116721  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
32     0    NaN  ...  0.253024  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
33     0    NaN  ...  0.226335  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
34     0    NaN  ...  0.229582  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
35     0    NaN  ...  0.287768  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
36     0    NaN  ...  0.174679  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
37     0    NaN  ...  0.111280  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
38     0    NaN  ...  0.350286  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[39 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.2464599609375, 0.83154296875]
[0.53271484375, 0.396728515625]
[0.460693359375, 0.59765625]
[0.1756591796875, 0.85546875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.2464599609375, 0.83154296875]
[0.53271484375, 0.396728515625]
[0.460693359375, 0.59765625]
[0.1756591796875, 0.85546875]
This is the real loss :  tensor(0.1453, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.119946  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
28     0    NaN  ...  0.288627  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.439450  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
30     0    NaN  ...  0.076138  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
31     0    NaN  ...  0.116721  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
32     0    NaN  ...  0.253024  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
33     0    NaN  ...  0.226335  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
34     0    NaN  ...  0.229582  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
35     0    NaN  ...  0.287768  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
36     0    NaN  ...  0.174679  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
37     0    NaN  ...  0.111280  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
38     0    NaN  ...  0.350286  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.145338  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[40 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.435791015625, 0.342041015625]
[0.481201171875, 0.66455078125]
[0.10186767578125, 0.9296875]
[0.37841796875, 0.63232421875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.435791015625, 0.342041015625]
[0.481201171875, 0.66455078125]
[0.10186767578125, 0.9296875]
[0.37841796875, 0.63232421875]
This is the real loss :  tensor(0.1576, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.119946  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
28     0    NaN  ...  0.288627  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.439450  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
30     0    NaN  ...  0.076138  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
31     0    NaN  ...  0.116721  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
32     0    NaN  ...  0.253024  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
33     0    NaN  ...  0.226335  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
34     0    NaN  ...  0.229582  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
35     0    NaN  ...  0.287768  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
36     0    NaN  ...  0.174679  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
37     0    NaN  ...  0.111280  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
38     0    NaN  ...  0.350286  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.145338  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
40     0    NaN  ...  0.157576  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[41 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.28955078125, 0.736328125]
[0.29052734375, 0.6328125]
[0.51171875, 0.638671875]
[0.368408203125, 0.60009765625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.28955078125, 0.736328125]
[0.29052734375, 0.6328125]
[0.51171875, 0.638671875]
[0.368408203125, 0.60009765625]
This is the real loss :  tensor(0.1643, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.119946  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
28     0    NaN  ...  0.288627  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.439450  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
30     0    NaN  ...  0.076138  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
31     0    NaN  ...  0.116721  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
32     0    NaN  ...  0.253024  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
33     0    NaN  ...  0.226335  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
34     0    NaN  ...  0.229582  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
35     0    NaN  ...  0.287768  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
36     0    NaN  ...  0.174679  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
37     0    NaN  ...  0.111280  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
38     0    NaN  ...  0.350286  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.145338  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
40     0    NaN  ...  0.157576  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
41     0    NaN  ...  0.164320  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[42 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.3193359375, 0.66552734375]
[0.56982421875, 0.422607421875]
[0.1907958984375, 0.87255859375]
[0.2257080078125, 0.5390625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.3193359375, 0.66552734375]
[0.56982421875, 0.422607421875]
[0.1907958984375, 0.87255859375]
[0.2257080078125, 0.5390625]
This is the real loss :  tensor(0.4470, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.119946  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
28     0    NaN  ...  0.288627  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.439450  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
30     0    NaN  ...  0.076138  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
31     0    NaN  ...  0.116721  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
32     0    NaN  ...  0.253024  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
33     0    NaN  ...  0.226335  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
34     0    NaN  ...  0.229582  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
35     0    NaN  ...  0.287768  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
36     0    NaN  ...  0.174679  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
37     0    NaN  ...  0.111280  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
38     0    NaN  ...  0.350286  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.145338  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
40     0    NaN  ...  0.157576  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
41     0    NaN  ...  0.164320  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
42     0    NaN  ...  0.447021  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[43 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.263671875, 0.6708984375]
[0.2208251953125, 0.7783203125]
[0.35107421875, 0.58544921875]
[0.479248046875, 0.473876953125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.263671875, 0.6708984375]
[0.2208251953125, 0.7783203125]
[0.35107421875, 0.58544921875]
[0.479248046875, 0.473876953125]
This is the real loss :  tensor(0.1347, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.119946  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
28     0    NaN  ...  0.288627  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.439450  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
30     0    NaN  ...  0.076138  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
31     0    NaN  ...  0.116721  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
32     0    NaN  ...  0.253024  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
33     0    NaN  ...  0.226335  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
34     0    NaN  ...  0.229582  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
35     0    NaN  ...  0.287768  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
36     0    NaN  ...  0.174679  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
37     0    NaN  ...  0.111280  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
38     0    NaN  ...  0.350286  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.145338  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
40     0    NaN  ...  0.157576  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
41     0    NaN  ...  0.164320  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
42     0    NaN  ...  0.447021  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
43     0    NaN  ...  0.134666  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[44 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.2646484375, 0.5888671875]
[0.345703125, 0.57421875]
[0.166259765625, 0.97802734375]
[0.68408203125, 0.4912109375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.2646484375, 0.5888671875]
[0.345703125, 0.57421875]
[0.166259765625, 0.97802734375]
[0.68408203125, 0.4912109375]
This is the real loss :  tensor(0.2190, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.119946  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
28     0    NaN  ...  0.288627  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.439450  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
30     0    NaN  ...  0.076138  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
31     0    NaN  ...  0.116721  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
32     0    NaN  ...  0.253024  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
33     0    NaN  ...  0.226335  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
34     0    NaN  ...  0.229582  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
35     0    NaN  ...  0.287768  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
36     0    NaN  ...  0.174679  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
37     0    NaN  ...  0.111280  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
38     0    NaN  ...  0.350286  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.145338  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
40     0    NaN  ...  0.157576  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
41     0    NaN  ...  0.164320  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
42     0    NaN  ...  0.447021  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
43     0    NaN  ...  0.134666  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
44     0    NaN  ...  0.218983  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[45 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.3408203125, 0.734375]
[0.3896484375, 0.389404296875]
[0.5546875, 0.68994140625]
[0.16259765625, 0.86181640625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.3408203125, 0.734375]
[0.3896484375, 0.389404296875]
[0.5546875, 0.68994140625]
[0.16259765625, 0.86181640625]
This is the real loss :  tensor(0.1451, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.119946  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
28     0    NaN  ...  0.288627  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.439450  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
30     0    NaN  ...  0.076138  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
31     0    NaN  ...  0.116721  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
32     0    NaN  ...  0.253024  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
33     0    NaN  ...  0.226335  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
34     0    NaN  ...  0.229582  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
35     0    NaN  ...  0.287768  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
36     0    NaN  ...  0.174679  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
37     0    NaN  ...  0.111280  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
38     0    NaN  ...  0.350286  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.145338  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
40     0    NaN  ...  0.157576  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
41     0    NaN  ...  0.164320  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
42     0    NaN  ...  0.447021  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
43     0    NaN  ...  0.134666  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
44     0    NaN  ...  0.218983  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
45     0    NaN  ...  0.145089  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[46 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.2060546875, 0.70556640625]
[0.49853515625, 0.677734375]
[0.50244140625, 0.45703125]
[0.204345703125, 0.79638671875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.2060546875, 0.70556640625]
[0.49853515625, 0.677734375]
[0.50244140625, 0.45703125]
[0.204345703125, 0.79638671875]
This is the real loss :  tensor(0.1838, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.119946  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
28     0    NaN  ...  0.288627  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.439450  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
30     0    NaN  ...  0.076138  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
31     0    NaN  ...  0.116721  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
32     0    NaN  ...  0.253024  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
33     0    NaN  ...  0.226335  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
34     0    NaN  ...  0.229582  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
35     0    NaN  ...  0.287768  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
36     0    NaN  ...  0.174679  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
37     0    NaN  ...  0.111280  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
38     0    NaN  ...  0.350286  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.145338  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
40     0    NaN  ...  0.157576  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
41     0    NaN  ...  0.164320  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
42     0    NaN  ...  0.447021  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
43     0    NaN  ...  0.134666  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
44     0    NaN  ...  0.218983  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
45     0    NaN  ...  0.145089  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
46     0    NaN  ...  0.183802  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[47 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.1904296875, 0.93212890625]
[0.5322265625, 0.5146484375]
[0.205810546875, 0.61767578125]
[0.56103515625, 0.76171875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1904296875, 0.93212890625]
[0.5322265625, 0.5146484375]
[0.205810546875, 0.61767578125]
[0.56103515625, 0.76171875]
This is the real loss :  tensor(0.1356, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.119946  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
28     0    NaN  ...  0.288627  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.439450  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
30     0    NaN  ...  0.076138  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
31     0    NaN  ...  0.116721  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
32     0    NaN  ...  0.253024  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
33     0    NaN  ...  0.226335  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
34     0    NaN  ...  0.229582  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
35     0    NaN  ...  0.287768  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
36     0    NaN  ...  0.174679  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
37     0    NaN  ...  0.111280  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
38     0    NaN  ...  0.350286  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.145338  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
40     0    NaN  ...  0.157576  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
41     0    NaN  ...  0.164320  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
42     0    NaN  ...  0.447021  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
43     0    NaN  ...  0.134666  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
44     0    NaN  ...  0.218983  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
45     0    NaN  ...  0.145089  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
46     0    NaN  ...  0.183802  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
47     0    NaN  ...  0.135577  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[48 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.1741943359375, 0.93310546875]
[0.2293701171875, 0.6025390625]
[0.31787109375, 0.61572265625]
[0.7197265625, 0.64990234375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1741943359375, 0.93310546875]
[0.2293701171875, 0.6025390625]
[0.31787109375, 0.61572265625]
[0.7197265625, 0.64990234375]
This is the real loss :  tensor(0.2177, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.119946  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
28     0    NaN  ...  0.288627  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.439450  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
30     0    NaN  ...  0.076138  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
31     0    NaN  ...  0.116721  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
32     0    NaN  ...  0.253024  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
33     0    NaN  ...  0.226335  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
34     0    NaN  ...  0.229582  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
35     0    NaN  ...  0.287768  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
36     0    NaN  ...  0.174679  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
37     0    NaN  ...  0.111280  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
38     0    NaN  ...  0.350286  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.145338  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
40     0    NaN  ...  0.157576  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
41     0    NaN  ...  0.164320  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
42     0    NaN  ...  0.447021  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
43     0    NaN  ...  0.134666  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
44     0    NaN  ...  0.218983  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
45     0    NaN  ...  0.145089  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
46     0    NaN  ...  0.183802  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
47     0    NaN  ...  0.135577  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
48     0    NaN  ...  0.217672  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[49 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.38037109375, 0.7822265625]
[0.412109375, 0.7431640625]
[0.442626953125, 0.498779296875]
[0.303466796875, 0.8271484375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.38037109375, 0.7822265625]
[0.412109375, 0.7431640625]
[0.442626953125, 0.498779296875]
[0.303466796875, 0.8271484375]
This is the real loss :  tensor(0.1387, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.119946  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
28     0    NaN  ...  0.288627  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.439450  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
30     0    NaN  ...  0.076138  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
31     0    NaN  ...  0.116721  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
32     0    NaN  ...  0.253024  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
33     0    NaN  ...  0.226335  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
34     0    NaN  ...  0.229582  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
35     0    NaN  ...  0.287768  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
36     0    NaN  ...  0.174679  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
37     0    NaN  ...  0.111280  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
38     0    NaN  ...  0.350286  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.145338  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
40     0    NaN  ...  0.157576  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
41     0    NaN  ...  0.164320  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
42     0    NaN  ...  0.447021  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
43     0    NaN  ...  0.134666  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
44     0    NaN  ...  0.218983  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
45     0    NaN  ...  0.145089  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
46     0    NaN  ...  0.183802  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
47     0    NaN  ...  0.135577  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
48     0    NaN  ...  0.217672  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
49     0    NaN  ...  0.138665  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[50 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.63134765625, 0.55712890625]
[0.397216796875, 0.468505859375]
[0.1600341796875, 0.75390625]
[0.1846923828125, 0.814453125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.63134765625, 0.55712890625]
[0.397216796875, 0.468505859375]
[0.1600341796875, 0.75390625]
[0.1846923828125, 0.814453125]
This is the real loss :  tensor(0.1665, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.119946  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
28     0    NaN  ...  0.288627  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.439450  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
30     0    NaN  ...  0.076138  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
31     0    NaN  ...  0.116721  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
32     0    NaN  ...  0.253024  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
33     0    NaN  ...  0.226335  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
34     0    NaN  ...  0.229582  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
35     0    NaN  ...  0.287768  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
36     0    NaN  ...  0.174679  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
37     0    NaN  ...  0.111280  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
38     0    NaN  ...  0.350286  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.145338  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
40     0    NaN  ...  0.157576  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
41     0    NaN  ...  0.164320  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
42     0    NaN  ...  0.447021  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
43     0    NaN  ...  0.134666  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
44     0    NaN  ...  0.218983  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
45     0    NaN  ...  0.145089  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
46     0    NaN  ...  0.183802  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
47     0    NaN  ...  0.135577  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
48     0    NaN  ...  0.217672  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
49     0    NaN  ...  0.138665  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
50     0    NaN  ...  0.166536  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[51 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.4326171875, 0.8037109375]
[0.326416015625, 0.8759765625]
[0.37109375, 0.80224609375]
[0.413818359375, 0.4375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.4326171875, 0.8037109375]
[0.326416015625, 0.8759765625]
[0.37109375, 0.80224609375]
[0.413818359375, 0.4375]
This is the real loss :  tensor(0.1265, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.119946  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
28     0    NaN  ...  0.288627  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.439450  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
30     0    NaN  ...  0.076138  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
31     0    NaN  ...  0.116721  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
32     0    NaN  ...  0.253024  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
33     0    NaN  ...  0.226335  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
34     0    NaN  ...  0.229582  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
35     0    NaN  ...  0.287768  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
36     0    NaN  ...  0.174679  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
37     0    NaN  ...  0.111280  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
38     0    NaN  ...  0.350286  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.145338  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
40     0    NaN  ...  0.157576  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
41     0    NaN  ...  0.164320  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
42     0    NaN  ...  0.447021  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
43     0    NaN  ...  0.134666  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
44     0    NaN  ...  0.218983  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
45     0    NaN  ...  0.145089  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
46     0    NaN  ...  0.183802  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
47     0    NaN  ...  0.135577  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
48     0    NaN  ...  0.217672  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
49     0    NaN  ...  0.138665  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
50     0    NaN  ...  0.166536  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
51     0    NaN  ...  0.126511  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[52 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.4013671875, 0.58154296875]
[0.1807861328125, 0.9814453125]
[0.32275390625, 0.81982421875]
[0.55615234375, 0.5234375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.4013671875, 0.58154296875]
[0.1807861328125, 0.9814453125]
[0.32275390625, 0.81982421875]
[0.55615234375, 0.5234375]
This is the real loss :  tensor(0.1753, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.119946  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
28     0    NaN  ...  0.288627  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.439450  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
30     0    NaN  ...  0.076138  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
31     0    NaN  ...  0.116721  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
32     0    NaN  ...  0.253024  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
33     0    NaN  ...  0.226335  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
34     0    NaN  ...  0.229582  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
35     0    NaN  ...  0.287768  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
36     0    NaN  ...  0.174679  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
37     0    NaN  ...  0.111280  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
38     0    NaN  ...  0.350286  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.145338  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
40     0    NaN  ...  0.157576  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
41     0    NaN  ...  0.164320  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
42     0    NaN  ...  0.447021  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
43     0    NaN  ...  0.134666  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
44     0    NaN  ...  0.218983  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
45     0    NaN  ...  0.145089  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
46     0    NaN  ...  0.183802  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
47     0    NaN  ...  0.135577  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
48     0    NaN  ...  0.217672  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
49     0    NaN  ...  0.138665  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
50     0    NaN  ...  0.166536  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
51     0    NaN  ...  0.126511  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
52     0    NaN  ...  0.175329  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[53 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.334228515625, 0.767578125]
[0.459716796875, 0.611328125]
[0.447021484375, 0.54248046875]
[0.1435546875, 0.88720703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.334228515625, 0.767578125]
[0.459716796875, 0.611328125]
[0.447021484375, 0.54248046875]
[0.1435546875, 0.88720703125]
This is the real loss :  tensor(0.1213, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.119946  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
28     0    NaN  ...  0.288627  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.439450  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
30     0    NaN  ...  0.076138  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
31     0    NaN  ...  0.116721  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
32     0    NaN  ...  0.253024  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
33     0    NaN  ...  0.226335  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
34     0    NaN  ...  0.229582  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
35     0    NaN  ...  0.287768  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
36     0    NaN  ...  0.174679  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
37     0    NaN  ...  0.111280  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
38     0    NaN  ...  0.350286  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.145338  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
40     0    NaN  ...  0.157576  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
41     0    NaN  ...  0.164320  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
42     0    NaN  ...  0.447021  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
43     0    NaN  ...  0.134666  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
44     0    NaN  ...  0.218983  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
45     0    NaN  ...  0.145089  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
46     0    NaN  ...  0.183802  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
47     0    NaN  ...  0.135577  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
48     0    NaN  ...  0.217672  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
49     0    NaN  ...  0.138665  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
50     0    NaN  ...  0.166536  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
51     0    NaN  ...  0.126511  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
52     0    NaN  ...  0.175329  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
53     0    NaN  ...  0.121327  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[54 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.45556640625, 0.67822265625]
[0.2310791015625, 0.92578125]
[0.49609375, 0.453369140625]
[0.2548828125, 0.93505859375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.45556640625, 0.67822265625]
[0.2310791015625, 0.92578125]
[0.49609375, 0.453369140625]
[0.2548828125, 0.93505859375]
This is the real loss :  tensor(0.1787, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.119946  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
28     0    NaN  ...  0.288627  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.439450  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
30     0    NaN  ...  0.076138  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
31     0    NaN  ...  0.116721  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
32     0    NaN  ...  0.253024  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
33     0    NaN  ...  0.226335  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
34     0    NaN  ...  0.229582  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
35     0    NaN  ...  0.287768  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
36     0    NaN  ...  0.174679  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
37     0    NaN  ...  0.111280  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
38     0    NaN  ...  0.350286  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.145338  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
40     0    NaN  ...  0.157576  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
41     0    NaN  ...  0.164320  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
42     0    NaN  ...  0.447021  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
43     0    NaN  ...  0.134666  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
44     0    NaN  ...  0.218983  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
45     0    NaN  ...  0.145089  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
46     0    NaN  ...  0.183802  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
47     0    NaN  ...  0.135577  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
48     0    NaN  ...  0.217672  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
49     0    NaN  ...  0.138665  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
50     0    NaN  ...  0.166536  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
51     0    NaN  ...  0.126511  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
52     0    NaN  ...  0.175329  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
53     0    NaN  ...  0.121327  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
54     0    NaN  ...  0.178675  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[55 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.37353515625, 0.67626953125]
[0.34326171875, 0.8076171875]
[0.20849609375, 0.955078125]
[0.4580078125, 0.62841796875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.37353515625, 0.67626953125]
[0.34326171875, 0.8076171875]
[0.20849609375, 0.955078125]
[0.4580078125, 0.62841796875]
This is the real loss :  tensor(0.2908, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.119946  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
28     0    NaN  ...  0.288627  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.439450  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
30     0    NaN  ...  0.076138  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
31     0    NaN  ...  0.116721  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
32     0    NaN  ...  0.253024  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
33     0    NaN  ...  0.226335  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
34     0    NaN  ...  0.229582  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
35     0    NaN  ...  0.287768  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
36     0    NaN  ...  0.174679  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
37     0    NaN  ...  0.111280  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
38     0    NaN  ...  0.350286  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.145338  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
40     0    NaN  ...  0.157576  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
41     0    NaN  ...  0.164320  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
42     0    NaN  ...  0.447021  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
43     0    NaN  ...  0.134666  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
44     0    NaN  ...  0.218983  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
45     0    NaN  ...  0.145089  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
46     0    NaN  ...  0.183802  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
47     0    NaN  ...  0.135577  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
48     0    NaN  ...  0.217672  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
49     0    NaN  ...  0.138665  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
50     0    NaN  ...  0.166536  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
51     0    NaN  ...  0.126511  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
52     0    NaN  ...  0.175329  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
53     0    NaN  ...  0.121327  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
54     0    NaN  ...  0.178675  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
55     0    NaN  ...  0.290835  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[56 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.51953125, 0.6953125]
[0.1849365234375, 0.78173828125]
[0.34033203125, 0.65478515625]
[0.27197265625, 0.85986328125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.51953125, 0.6953125]
[0.1849365234375, 0.78173828125]
[0.34033203125, 0.65478515625]
[0.27197265625, 0.85986328125]
This is the real loss :  tensor(0.2898, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.119946  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
28     0    NaN  ...  0.288627  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.439450  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
30     0    NaN  ...  0.076138  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
31     0    NaN  ...  0.116721  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
32     0    NaN  ...  0.253024  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
33     0    NaN  ...  0.226335  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
34     0    NaN  ...  0.229582  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
35     0    NaN  ...  0.287768  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
36     0    NaN  ...  0.174679  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
37     0    NaN  ...  0.111280  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
38     0    NaN  ...  0.350286  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.145338  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
40     0    NaN  ...  0.157576  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
41     0    NaN  ...  0.164320  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
42     0    NaN  ...  0.447021  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
43     0    NaN  ...  0.134666  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
44     0    NaN  ...  0.218983  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
45     0    NaN  ...  0.145089  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
46     0    NaN  ...  0.183802  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
47     0    NaN  ...  0.135577  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
48     0    NaN  ...  0.217672  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
49     0    NaN  ...  0.138665  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
50     0    NaN  ...  0.166536  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
51     0    NaN  ...  0.126511  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
52     0    NaN  ...  0.175329  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
53     0    NaN  ...  0.121327  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
54     0    NaN  ...  0.178675  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
55     0    NaN  ...  0.290835  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
56     0    NaN  ...  0.289795  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[57 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.1539306640625, 0.94580078125]
[0.376953125, 0.650390625]
[0.189453125, 0.8544921875]
[0.51025390625, 0.493408203125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1539306640625, 0.94580078125]
[0.376953125, 0.650390625]
[0.189453125, 0.8544921875]
[0.51025390625, 0.493408203125]
This is the real loss :  tensor(0.1081, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.119946  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
28     0    NaN  ...  0.288627  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.439450  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
30     0    NaN  ...  0.076138  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
31     0    NaN  ...  0.116721  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
32     0    NaN  ...  0.253024  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
33     0    NaN  ...  0.226335  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
34     0    NaN  ...  0.229582  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
35     0    NaN  ...  0.287768  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
36     0    NaN  ...  0.174679  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
37     0    NaN  ...  0.111280  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
38     0    NaN  ...  0.350286  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.145338  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
40     0    NaN  ...  0.157576  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
41     0    NaN  ...  0.164320  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
42     0    NaN  ...  0.447021  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
43     0    NaN  ...  0.134666  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
44     0    NaN  ...  0.218983  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
45     0    NaN  ...  0.145089  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
46     0    NaN  ...  0.183802  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
47     0    NaN  ...  0.135577  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
48     0    NaN  ...  0.217672  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
49     0    NaN  ...  0.138665  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
50     0    NaN  ...  0.166536  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
51     0    NaN  ...  0.126511  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
52     0    NaN  ...  0.175329  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
53     0    NaN  ...  0.121327  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
54     0    NaN  ...  0.178675  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
55     0    NaN  ...  0.290835  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
56     0    NaN  ...  0.289795  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
57     0    NaN  ...  0.108126  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[58 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.262939453125, 0.80615234375]
[0.12384033203125, 0.98486328125]
[0.378662109375, 0.615234375]
[0.402099609375, 0.623046875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.262939453125, 0.80615234375]
[0.12384033203125, 0.98486328125]
[0.378662109375, 0.615234375]
[0.402099609375, 0.623046875]
This is the real loss :  tensor(0.1449, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.119946  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
28     0    NaN  ...  0.288627  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.439450  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
30     0    NaN  ...  0.076138  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
31     0    NaN  ...  0.116721  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
32     0    NaN  ...  0.253024  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
33     0    NaN  ...  0.226335  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
34     0    NaN  ...  0.229582  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
35     0    NaN  ...  0.287768  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
36     0    NaN  ...  0.174679  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
37     0    NaN  ...  0.111280  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
38     0    NaN  ...  0.350286  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.145338  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
40     0    NaN  ...  0.157576  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
41     0    NaN  ...  0.164320  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
42     0    NaN  ...  0.447021  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
43     0    NaN  ...  0.134666  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
44     0    NaN  ...  0.218983  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
45     0    NaN  ...  0.145089  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
46     0    NaN  ...  0.183802  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
47     0    NaN  ...  0.135577  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
48     0    NaN  ...  0.217672  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
49     0    NaN  ...  0.138665  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
50     0    NaN  ...  0.166536  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
51     0    NaN  ...  0.126511  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
52     0    NaN  ...  0.175329  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
53     0    NaN  ...  0.121327  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
54     0    NaN  ...  0.178675  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
55     0    NaN  ...  0.290835  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
56     0    NaN  ...  0.289795  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
57     0    NaN  ...  0.108126  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
58     0    NaN  ...  0.144923  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[59 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.21435546875, 0.794921875]
[0.439453125, 0.6552734375]
[0.29638671875, 0.7470703125]
[0.310791015625, 0.8447265625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.21435546875, 0.794921875]
[0.439453125, 0.6552734375]
[0.29638671875, 0.7470703125]
[0.310791015625, 0.8447265625]
This is the real loss :  tensor(0.1380, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
5      0    NaN  ...  0.357425  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
6      0    NaN  ...  0.292690  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
7      0    NaN  ...  0.233464  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
8      0    NaN  ...  0.226529  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
9      0    NaN  ...  0.246639  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
10     0    NaN  ...  0.141001  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
11     0    NaN  ...  0.222310  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
12     0    NaN  ...  0.255007  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
13     0    NaN  ...  0.208179  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
14     0    NaN  ...  0.207714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
15     0    NaN  ...  0.252177  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
16     0    NaN  ...  0.145381  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
17     0    NaN  ...  0.133421  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
18     0    NaN  ...  0.099011  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
19     0    NaN  ...  0.157440  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
20     0    NaN  ...  0.089408  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
21     0    NaN  ...  0.152762  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
22     0    NaN  ...  0.064357  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
23     0    NaN  ...  0.354363  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
24     0    NaN  ...  0.148419  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
25     0    NaN  ...  0.216769  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
26     0    NaN  ...  0.491602  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.119946  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
28     0    NaN  ...  0.288627  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.439450  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
30     0    NaN  ...  0.076138  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
31     0    NaN  ...  0.116721  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
32     0    NaN  ...  0.253024  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
33     0    NaN  ...  0.226335  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
34     0    NaN  ...  0.229582  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
35     0    NaN  ...  0.287768  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
36     0    NaN  ...  0.174679  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
37     0    NaN  ...  0.111280  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
38     0    NaN  ...  0.350286  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.145338  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
40     0    NaN  ...  0.157576  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
41     0    NaN  ...  0.164320  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
42     0    NaN  ...  0.447021  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
43     0    NaN  ...  0.134666  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
44     0    NaN  ...  0.218983  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
45     0    NaN  ...  0.145089  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
46     0    NaN  ...  0.183802  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
47     0    NaN  ...  0.135577  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
48     0    NaN  ...  0.217672  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
49     0    NaN  ...  0.138665  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
50     0    NaN  ...  0.166536  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
51     0    NaN  ...  0.126511  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
52     0    NaN  ...  0.175329  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
53     0    NaN  ...  0.121327  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
54     0    NaN  ...  0.178675  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
55     0    NaN  ...  0.290835  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
56     0    NaN  ...  0.289795  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
57     0    NaN  ...  0.108126  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
58     0    NaN  ...  0.144923  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
59     0    NaN  ...  0.138015  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[60 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.192138671875, 0.7626953125]
[0.39453125, 0.6494140625]
[0.2119140625, 0.740234375]
[0.298095703125, 0.634765625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.192138671875, 0.7626953125]
[0.39453125, 0.6494140625]
[0.2119140625, 0.740234375]
[0.298095703125, 0.634765625]
This is the real loss :  tensor(0.2841, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
56     0    NaN  ...  0.289795  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
57     0    NaN  ...  0.108126  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
58     0    NaN  ...  0.144923  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
59     0    NaN  ...  0.138015  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
60     0    NaN  ...  0.284106  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[61 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.09326171875, 0.77734375]
[0.257568359375, 0.6533203125]
[0.428955078125, 0.5859375]
[0.1925048828125, 0.712890625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.09326171875, 0.77734375]
[0.257568359375, 0.6533203125]
[0.428955078125, 0.5859375]
[0.1925048828125, 0.712890625]
This is the real loss :  tensor(0.1889, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
57     0    NaN  ...  0.108126  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
58     0    NaN  ...  0.144923  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
59     0    NaN  ...  0.138015  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
60     0    NaN  ...  0.284106  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
61     0    NaN  ...  0.188906  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[62 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.29345703125, 0.6484375]
[0.5693359375, 0.63525390625]
[0.239013671875, 0.62255859375]
[-0.083740234375, 1.015625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.29345703125, 0.6484375]
[0.5693359375, 0.63525390625]
[0.239013671875, 0.62255859375]
[-0.083740234375, 1.015625]
This is the real loss :  tensor(0.2938, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
58     0    NaN  ...  0.144923  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
59     0    NaN  ...  0.138015  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
60     0    NaN  ...  0.284106  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
61     0    NaN  ...  0.188906  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
62     0    NaN  ...  0.293849  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[63 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.321044921875, 0.68994140625]
[0.346923828125, 0.72265625]
[0.25146484375, 0.70751953125]
[0.1685791015625, 0.8017578125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.321044921875, 0.68994140625]
[0.346923828125, 0.72265625]
[0.25146484375, 0.70751953125]
[0.1685791015625, 0.8017578125]
This is the real loss :  tensor(0.0766, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
59     0    NaN  ...  0.138015  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
60     0    NaN  ...  0.284106  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
61     0    NaN  ...  0.188906  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
62     0    NaN  ...  0.293849  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
63     0    NaN  ...  0.076623  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[64 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.00020837783813476562, 0.77099609375]
[0.5107421875, 0.4951171875]
[0.2666015625, 0.6689453125]
[0.100830078125, 0.80908203125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.00020837783813476562, 0.77099609375]
[0.5107421875, 0.4951171875]
[0.2666015625, 0.6689453125]
[0.100830078125, 0.80908203125]
This is the real loss :  tensor(0.0955, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
60     0    NaN  ...  0.284106  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
61     0    NaN  ...  0.188906  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
62     0    NaN  ...  0.293849  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
63     0    NaN  ...  0.076623  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
64     0    NaN  ...  0.095531  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[65 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[-0.061309814453125, 0.95361328125]
[0.28271484375, 0.7197265625]
[0.53955078125, 0.58056640625]
[0.160400390625, 0.63330078125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.061309814453125, 0.95361328125]
[0.28271484375, 0.7197265625]
[0.53955078125, 0.58056640625]
[0.160400390625, 0.63330078125]
This is the real loss :  tensor(0.4709, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
61     0    NaN  ...  0.188906  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
62     0    NaN  ...  0.293849  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
63     0    NaN  ...  0.076623  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
64     0    NaN  ...  0.095531  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
65     0    NaN  ...  0.470909  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[66 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[-0.0804443359375, 1.1025390625]
[0.306884765625, 0.63623046875]
[0.4306640625, 0.603515625]
[0.29541015625, 0.63037109375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.0804443359375, 1.1025390625]
[0.306884765625, 0.63623046875]
[0.4306640625, 0.603515625]
[0.29541015625, 0.63037109375]
This is the real loss :  tensor(0.1013, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
62     0    NaN  ...  0.293849  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
63     0    NaN  ...  0.076623  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
64     0    NaN  ...  0.095531  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
65     0    NaN  ...  0.470909  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
66     0    NaN  ...  0.101257  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[67 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.41455078125, 0.50634765625]
[0.12890625, 0.83251953125]
[0.089599609375, 0.8935546875]
[0.28955078125, 0.6787109375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.41455078125, 0.50634765625]
[0.12890625, 0.83251953125]
[0.089599609375, 0.8935546875]
[0.28955078125, 0.6787109375]
This is the real loss :  tensor(0.0833, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
63     0    NaN  ...  0.076623  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
64     0    NaN  ...  0.095531  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
65     0    NaN  ...  0.470909  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
66     0    NaN  ...  0.101257  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
67     0    NaN  ...  0.083330  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[68 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.300048828125, 0.57861328125]
[0.06378173828125, 0.91455078125]
[0.469482421875, 0.580078125]
[0.058349609375, 0.8828125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.300048828125, 0.57861328125]
[0.06378173828125, 0.91455078125]
[0.469482421875, 0.580078125]
[0.058349609375, 0.8828125]
This is the real loss :  tensor(0.1839, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
64     0    NaN  ...  0.095531  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
65     0    NaN  ...  0.470909  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
66     0    NaN  ...  0.101257  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
67     0    NaN  ...  0.083330  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
68     0    NaN  ...  0.183896  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[69 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.329345703125, 0.6416015625]
[0.1873779296875, 0.8857421875]
[0.09185791015625, 0.90087890625]
[0.37939453125, 0.583984375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.329345703125, 0.6416015625]
[0.1873779296875, 0.8857421875]
[0.09185791015625, 0.90087890625]
[0.37939453125, 0.583984375]
This is the real loss :  tensor(0.1556, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
65     0    NaN  ...  0.470909  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
66     0    NaN  ...  0.101257  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
67     0    NaN  ...  0.083330  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
68     0    NaN  ...  0.183896  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
69     0    NaN  ...  0.155608  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[70 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.323974609375, 0.59765625]
[0.132568359375, 0.8212890625]
[0.430908203125, 0.6103515625]
[0.033416748046875, 0.94970703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.323974609375, 0.59765625]
[0.132568359375, 0.8212890625]
[0.430908203125, 0.6103515625]
[0.033416748046875, 0.94970703125]
This is the real loss :  tensor(0.0822, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
66     0    NaN  ...  0.101257  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
67     0    NaN  ...  0.083330  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
68     0    NaN  ...  0.183896  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
69     0    NaN  ...  0.155608  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
70     0    NaN  ...  0.082188  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[71 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.313720703125, 0.697265625]
[0.34912109375, 0.634765625]
[-0.087158203125, 1.041015625]
[0.34765625, 0.70458984375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.313720703125, 0.697265625]
[0.34912109375, 0.634765625]
[-0.087158203125, 1.041015625]
[0.34765625, 0.70458984375]
This is the real loss :  tensor(0.1543, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
67     0    NaN  ...  0.083330  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
68     0    NaN  ...  0.183896  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
69     0    NaN  ...  0.155608  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
70     0    NaN  ...  0.082188  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
71     0    NaN  ...  0.154256  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[72 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.316650390625, 0.6513671875]
[-0.0738525390625, 0.9765625]
[0.228271484375, 0.63916015625]
[0.356689453125, 0.630859375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.316650390625, 0.6513671875]
[-0.0738525390625, 0.9765625]
[0.228271484375, 0.63916015625]
[0.356689453125, 0.630859375]
This is the real loss :  tensor(0.2706, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
68     0    NaN  ...  0.183896  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
69     0    NaN  ...  0.155608  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
70     0    NaN  ...  0.082188  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
71     0    NaN  ...  0.154256  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
72     0    NaN  ...  0.270604  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[73 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.311279296875, 0.67333984375]
[0.294189453125, 0.7626953125]
[0.179931640625, 0.78076171875]
[0.214599609375, 0.8046875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.311279296875, 0.67333984375]
[0.294189453125, 0.7626953125]
[0.179931640625, 0.78076171875]
[0.214599609375, 0.8046875]
This is the real loss :  tensor(0.1544, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
69     0    NaN  ...  0.155608  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
70     0    NaN  ...  0.082188  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
71     0    NaN  ...  0.154256  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
72     0    NaN  ...  0.270604  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
73     0    NaN  ...  0.154403  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[74 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.386474609375, 0.58447265625]
[0.06292724609375, 0.904296875]
[0.306640625, 0.61962890625]
[0.12066650390625, 0.83154296875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.386474609375, 0.58447265625]
[0.06292724609375, 0.904296875]
[0.306640625, 0.61962890625]
[0.12066650390625, 0.83154296875]
This is the real loss :  tensor(0.0771, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
70     0    NaN  ...  0.082188  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
71     0    NaN  ...  0.154256  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
72     0    NaN  ...  0.270604  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
73     0    NaN  ...  0.154403  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
74     0    NaN  ...  0.077099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[75 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.393798828125, 0.57958984375]
[0.034637451171875, 1.021484375]
[0.10638427734375, 0.88720703125]
[0.34130859375, 0.5595703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.393798828125, 0.57958984375]
[0.034637451171875, 1.021484375]
[0.10638427734375, 0.88720703125]
[0.34130859375, 0.5595703125]
This is the real loss :  tensor(0.1845, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
71     0    NaN  ...  0.154256  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
72     0    NaN  ...  0.270604  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
73     0    NaN  ...  0.154403  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
74     0    NaN  ...  0.077099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
75     0    NaN  ...  0.184512  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[76 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.155029296875, 0.755859375]
[0.32666015625, 0.7021484375]
[0.2193603515625, 0.6806640625]
[0.216064453125, 0.744140625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.155029296875, 0.755859375]
[0.32666015625, 0.7021484375]
[0.2193603515625, 0.6806640625]
[0.216064453125, 0.744140625]
This is the real loss :  tensor(0.0677, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
72     0    NaN  ...  0.270604  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
73     0    NaN  ...  0.154403  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
74     0    NaN  ...  0.077099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
75     0    NaN  ...  0.184512  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
76     0    NaN  ...  0.067663  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[77 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.40869140625, 0.61865234375]
[0.1187744140625, 0.86767578125]
[0.10955810546875, 0.96923828125]
[0.245361328125, 0.55078125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.40869140625, 0.61865234375]
[0.1187744140625, 0.86767578125]
[0.10955810546875, 0.96923828125]
[0.245361328125, 0.55078125]
This is the real loss :  tensor(0.1537, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
73     0    NaN  ...  0.154403  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
74     0    NaN  ...  0.077099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
75     0    NaN  ...  0.184512  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
76     0    NaN  ...  0.067663  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
77     0    NaN  ...  0.153733  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[78 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.19775390625, 0.75244140625]
[0.15283203125, 0.73828125]
[0.383056640625, 0.57421875]
[0.115478515625, 0.85693359375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.19775390625, 0.75244140625]
[0.15283203125, 0.73828125]
[0.383056640625, 0.57421875]
[0.115478515625, 0.85693359375]
This is the real loss :  tensor(0.1170, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
74     0    NaN  ...  0.077099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
75     0    NaN  ...  0.184512  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
76     0    NaN  ...  0.067663  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
77     0    NaN  ...  0.153733  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
78     0    NaN  ...  0.117049  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[79 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.33154296875, 0.8095703125]
[0.2283935546875, 0.74365234375]
[0.207763671875, 0.72802734375]
[0.2025146484375, 0.7841796875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.33154296875, 0.8095703125]
[0.2283935546875, 0.74365234375]
[0.207763671875, 0.72802734375]
[0.2025146484375, 0.7841796875]
This is the real loss :  tensor(0.0586, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
75     0    NaN  ...  0.184512  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
76     0    NaN  ...  0.067663  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
77     0    NaN  ...  0.153733  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
78     0    NaN  ...  0.117049  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
79     0    NaN  ...  0.058598  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[80 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.4150390625, 0.580078125]
[0.338623046875, 0.60400390625]
[0.08599853515625, 0.93798828125]
[-0.0172576904296875, 1.0830078125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.4150390625, 0.580078125]
[0.338623046875, 0.60400390625]
[0.08599853515625, 0.93798828125]
[-0.0172576904296875, 1.0830078125]
This is the real loss :  tensor(0.1874, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
76     0    NaN  ...  0.067663  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
77     0    NaN  ...  0.153733  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
78     0    NaN  ...  0.117049  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
79     0    NaN  ...  0.058598  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
80     0    NaN  ...  0.187417  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[81 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.306396484375, 0.70166015625]
[-0.10107421875, 1.1923828125]
[0.39892578125, 0.67138671875]
[0.341796875, 0.66015625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.306396484375, 0.70166015625]
[-0.10107421875, 1.1923828125]
[0.39892578125, 0.67138671875]
[0.341796875, 0.66015625]
This is the real loss :  tensor(0.5134, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
77     0    NaN  ...  0.153733  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
78     0    NaN  ...  0.117049  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
79     0    NaN  ...  0.058598  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
80     0    NaN  ...  0.187417  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
81     0    NaN  ...  0.513375  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[82 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.211669921875, 0.8173828125]
[0.1446533203125, 0.87060546875]
[0.11358642578125, 0.90234375]
[0.466064453125, 0.56298828125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.211669921875, 0.8173828125]
[0.1446533203125, 0.87060546875]
[0.11358642578125, 0.90234375]
[0.466064453125, 0.56298828125]
This is the real loss :  tensor(0.0683, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
78     0    NaN  ...  0.117049  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
79     0    NaN  ...  0.058598  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
80     0    NaN  ...  0.187417  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
81     0    NaN  ...  0.513375  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
82     0    NaN  ...  0.068307  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[83 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.51171875, 0.50146484375]
[0.062042236328125, 0.9443359375]
[0.1427001953125, 0.87548828125]
[0.195068359375, 0.806640625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.51171875, 0.50146484375]
[0.062042236328125, 0.9443359375]
[0.1427001953125, 0.87548828125]
[0.195068359375, 0.806640625]
This is the real loss :  tensor(0.0786, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
79     0    NaN  ...  0.058598  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
80     0    NaN  ...  0.187417  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
81     0    NaN  ...  0.513375  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
82     0    NaN  ...  0.068307  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
83     0    NaN  ...  0.078581  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[84 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.23193359375, 0.77685546875]
[0.2347412109375, 0.75341796875]
[0.2237548828125, 0.8212890625]
[0.2802734375, 0.73193359375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.23193359375, 0.77685546875]
[0.2347412109375, 0.75341796875]
[0.2237548828125, 0.8212890625]
[0.2802734375, 0.73193359375]
This is the real loss :  tensor(0.3421, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
80     0    NaN  ...  0.187417  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
81     0    NaN  ...  0.513375  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
82     0    NaN  ...  0.068307  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
83     0    NaN  ...  0.078581  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
84     0    NaN  ...  0.342103  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[85 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.06829833984375, 0.99658203125]
[0.045562744140625, 0.9970703125]
[0.357421875, 0.61181640625]
[0.43115234375, 0.57568359375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.06829833984375, 0.99658203125]
[0.045562744140625, 0.9970703125]
[0.357421875, 0.61181640625]
[0.43115234375, 0.57568359375]
This is the real loss :  tensor(0.1450, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
81     0    NaN  ...  0.513375  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
82     0    NaN  ...  0.068307  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
83     0    NaN  ...  0.078581  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
84     0    NaN  ...  0.342103  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
85     0    NaN  ...  0.144990  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[86 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.12646484375, 0.88427734375]
[0.470947265625, 0.48583984375]
[0.17724609375, 0.853515625]
[0.103271484375, 0.92041015625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.12646484375, 0.88427734375]
[0.470947265625, 0.48583984375]
[0.17724609375, 0.853515625]
[0.103271484375, 0.92041015625]
This is the real loss :  tensor(0.0732, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
82     0    NaN  ...  0.068307  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
83     0    NaN  ...  0.078581  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
84     0    NaN  ...  0.342103  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
85     0    NaN  ...  0.144990  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
86     0    NaN  ...  0.073176  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[87 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[-0.03936767578125, 1.0458984375]
[0.27294921875, 0.7451171875]
[0.34912109375, 0.66650390625]
[0.275390625, 0.705078125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.03936767578125, 1.0458984375]
[0.27294921875, 0.7451171875]
[0.34912109375, 0.66650390625]
[0.275390625, 0.705078125]
This is the real loss :  tensor(0.1748, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
83     0    NaN  ...  0.078581  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
84     0    NaN  ...  0.342103  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
85     0    NaN  ...  0.144990  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
86     0    NaN  ...  0.073176  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
87     0    NaN  ...  0.174803  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[88 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[-0.0269775390625, 1.08203125]
[0.31201171875, 0.69482421875]
[0.291259765625, 0.75390625]
[0.36572265625, 0.6923828125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.0269775390625, 1.08203125]
[0.31201171875, 0.69482421875]
[0.291259765625, 0.75390625]
[0.36572265625, 0.6923828125]
This is the real loss :  tensor(0.0715, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
84     0    NaN  ...  0.342103  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
85     0    NaN  ...  0.144990  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
86     0    NaN  ...  0.073176  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
87     0    NaN  ...  0.174803  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
88     0    NaN  ...  0.071465  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[89 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.30322265625, 0.73974609375]
[0.330078125, 0.64892578125]
[0.2802734375, 0.720703125]
[-0.0308837890625, 1.0234375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.30322265625, 0.73974609375]
[0.330078125, 0.64892578125]
[0.2802734375, 0.720703125]
[-0.0308837890625, 1.0234375]
This is the real loss :  tensor(0.2576, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
85     0    NaN  ...  0.144990  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
86     0    NaN  ...  0.073176  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
87     0    NaN  ...  0.174803  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
88     0    NaN  ...  0.071465  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
89     0    NaN  ...  0.257586  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[90 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.36083984375, 0.6484375]
[0.0298614501953125, 0.90576171875]
[0.00937652587890625, 0.92333984375]
[0.427001953125, 0.5849609375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.36083984375, 0.6484375]
[0.0298614501953125, 0.90576171875]
[0.00937652587890625, 0.92333984375]
[0.427001953125, 0.5849609375]
This is the real loss :  tensor(0.0780, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
86     0    NaN  ...  0.073176  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
87     0    NaN  ...  0.174803  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
88     0    NaN  ...  0.071465  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
89     0    NaN  ...  0.257586  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
90     0    NaN  ...  0.078016  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[91 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.11468505859375, 0.83837890625]
[0.447265625, 0.60595703125]
[0.09564208984375, 0.92724609375]
[0.1947021484375, 0.79052734375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.11468505859375, 0.83837890625]
[0.447265625, 0.60595703125]
[0.09564208984375, 0.92724609375]
[0.1947021484375, 0.79052734375]
This is the real loss :  tensor(0.0614, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
87     0    NaN  ...  0.174803  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
88     0    NaN  ...  0.071465  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
89     0    NaN  ...  0.257586  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
90     0    NaN  ...  0.078016  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
91     0    NaN  ...  0.061352  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[92 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.066162109375, 0.8408203125]
[0.408447265625, 0.5908203125]
[0.134033203125, 0.8056640625]
[0.141845703125, 0.84326171875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.066162109375, 0.8408203125]
[0.408447265625, 0.5908203125]
[0.134033203125, 0.8056640625]
[0.141845703125, 0.84326171875]
This is the real loss :  tensor(0.0580, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
88     0    NaN  ...  0.071465  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
89     0    NaN  ...  0.257586  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
90     0    NaN  ...  0.078016  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
91     0    NaN  ...  0.061352  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
92     0    NaN  ...  0.058049  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[93 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[-0.0034656524658203125, 0.9951171875]
[0.34814453125, 0.64990234375]
[0.3583984375, 0.62939453125]
[0.1123046875, 0.88671875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.0034656524658203125, 0.9951171875]
[0.34814453125, 0.64990234375]
[0.3583984375, 0.62939453125]
[0.1123046875, 0.88671875]
This is the real loss :  tensor(0.1346, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
89     0    NaN  ...  0.257586  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
90     0    NaN  ...  0.078016  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
91     0    NaN  ...  0.061352  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
92     0    NaN  ...  0.058049  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
93     0    NaN  ...  0.134630  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[94 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.388427734375, 0.61962890625]
[0.1783447265625, 0.81884765625]
[0.055267333984375, 0.943359375]
[0.15576171875, 0.83251953125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.388427734375, 0.61962890625]
[0.1783447265625, 0.81884765625]
[0.055267333984375, 0.943359375]
[0.15576171875, 0.83251953125]
This is the real loss :  tensor(0.1101, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
90     0    NaN  ...  0.078016  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
91     0    NaN  ...  0.061352  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
92     0    NaN  ...  0.058049  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
93     0    NaN  ...  0.134630  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
94     0    NaN  ...  0.110145  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[95 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.12274169921875, 0.86279296875]
[0.130859375, 0.86865234375]
[0.11663818359375, 0.85693359375]
[0.390625, 0.61767578125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.12274169921875, 0.86279296875]
[0.130859375, 0.86865234375]
[0.11663818359375, 0.85693359375]
[0.390625, 0.61767578125]
This is the real loss :  tensor(0.2346, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
91     0    NaN  ...  0.061352  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
92     0    NaN  ...  0.058049  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
93     0    NaN  ...  0.134630  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
94     0    NaN  ...  0.110145  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
95     0    NaN  ...  0.234586  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[96 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.2386474609375, 0.71923828125]
[0.1744384765625, 0.7880859375]
[0.2303466796875, 0.9091796875]
[0.1939697265625, 0.7451171875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.2386474609375, 0.71923828125]
[0.1744384765625, 0.7880859375]
[0.2303466796875, 0.9091796875]
[0.1939697265625, 0.7451171875]
This is the real loss :  tensor(0.1847, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
92     0    NaN  ...  0.058049  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
93     0    NaN  ...  0.134630  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
94     0    NaN  ...  0.110145  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
95     0    NaN  ...  0.234586  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
96     0    NaN  ...  0.184664  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[97 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.1053466796875, 0.87841796875]
[0.311279296875, 0.61083984375]
[0.0931396484375, 0.9169921875]
[0.1883544921875, 0.8291015625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1053466796875, 0.87841796875]
[0.311279296875, 0.61083984375]
[0.0931396484375, 0.9169921875]
[0.1883544921875, 0.8291015625]
This is the real loss :  tensor(0.0443, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
93     0    NaN  ...  0.134630  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
94     0    NaN  ...  0.110145  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
95     0    NaN  ...  0.234586  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
96     0    NaN  ...  0.184664  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
97     0    NaN  ...  0.044309  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[98 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.1766357421875, 0.7529296875]
[0.1304931640625, 0.78564453125]
[0.2269287109375, 0.78076171875]
[0.2705078125, 0.82568359375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1766357421875, 0.7529296875]
[0.1304931640625, 0.78564453125]
[0.2269287109375, 0.78076171875]
[0.2705078125, 0.82568359375]
This is the real loss :  tensor(0.0448, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
94     0    NaN  ...  0.110145  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
95     0    NaN  ...  0.234586  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
96     0    NaN  ...  0.184664  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
97     0    NaN  ...  0.044309  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
98     0    NaN  ...  0.044793  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[99 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.2685546875, 0.73193359375]
[0.0921630859375, 0.87109375]
[0.1920166015625, 0.775390625]
[0.1239013671875, 0.83935546875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.2685546875, 0.73193359375]
[0.0921630859375, 0.87109375]
[0.1920166015625, 0.775390625]
[0.1239013671875, 0.83935546875]
This is the real loss :  tensor(0.0372, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1      0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2      0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4      0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..   ...    ...  ...       ...                                               ...
95     0    NaN  ...  0.234586  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
96     0    NaN  ...  0.184664  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
97     0    NaN  ...  0.044309  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
98     0    NaN  ...  0.044793  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
99     0    NaN  ...  0.037196  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[100 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.2333984375, 0.75537109375]
[0.266357421875, 0.70751953125]
[0.2353515625, 0.7666015625]
[-0.0307159423828125, 1.0244140625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.2333984375, 0.75537109375]
[0.266357421875, 0.70751953125]
[0.2353515625, 0.7666015625]
[-0.0307159423828125, 1.0244140625]
This is the real loss :  tensor(0.4214, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
96      0    NaN  ...  0.184664  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
97      0    NaN  ...  0.044309  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
98      0    NaN  ...  0.044793  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
99      0    NaN  ...  0.037196  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
100     0    NaN  ...  0.421373  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[101 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.1612548828125, 0.82373046875]
[0.0928955078125, 0.9375]
[0.09027099609375, 0.94140625]
[0.304931640625, 0.61181640625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1612548828125, 0.82373046875]
[0.0928955078125, 0.9375]
[0.09027099609375, 0.94140625]
[0.304931640625, 0.61181640625]
This is the real loss :  tensor(0.0406, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
97      0    NaN  ...  0.044309  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
98      0    NaN  ...  0.044793  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
99      0    NaN  ...  0.037196  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
100     0    NaN  ...  0.421373  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
101     0    NaN  ...  0.040608  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[102 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.236328125, 0.77001953125]
[0.279541015625, 0.70849609375]
[0.19775390625, 0.75244140625]
[-0.039764404296875, 1.015625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.236328125, 0.77001953125]
[0.279541015625, 0.70849609375]
[0.19775390625, 0.75244140625]
[-0.039764404296875, 1.015625]
This is the real loss :  tensor(0.1540, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
98      0    NaN  ...  0.044793  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
99      0    NaN  ...  0.037196  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
100     0    NaN  ...  0.421373  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
101     0    NaN  ...  0.040608  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
102     0    NaN  ...  0.153998  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[103 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.08966064453125, 0.78173828125]
[0.030853271484375, 0.84033203125]
[0.11181640625, 0.75927734375]
[0.35498046875, 0.69677734375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.08966064453125, 0.78173828125]
[0.030853271484375, 0.84033203125]
[0.11181640625, 0.75927734375]
[0.35498046875, 0.69677734375]
This is the real loss :  tensor(0.1318, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
99      0    NaN  ...  0.037196  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
100     0    NaN  ...  0.421373  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
101     0    NaN  ...  0.040608  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
102     0    NaN  ...  0.153998  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
103     0    NaN  ...  0.131765  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[104 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.2408447265625, 0.6982421875]
[-0.09490966796875, 1.0419921875]
[0.276611328125, 0.73291015625]
[0.26513671875, 0.77392578125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.2408447265625, 0.6982421875]
[-0.09490966796875, 1.0419921875]
[0.276611328125, 0.73291015625]
[0.26513671875, 0.77392578125]
This is the real loss :  tensor(0.4093, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
100     0    NaN  ...  0.421373  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
101     0    NaN  ...  0.040608  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
102     0    NaN  ...  0.153998  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
103     0    NaN  ...  0.131765  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
104     0    NaN  ...  0.409258  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[105 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.249755859375, 0.73388671875]
[0.28076171875, 0.73193359375]
[0.261962890625, 0.7099609375]
[-0.081787109375, 1.0810546875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.249755859375, 0.73388671875]
[0.28076171875, 0.73193359375]
[0.261962890625, 0.7099609375]
[-0.081787109375, 1.0810546875]
This is the real loss :  tensor(0.1773, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
101     0    NaN  ...  0.040608  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
102     0    NaN  ...  0.153998  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
103     0    NaN  ...  0.131765  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
104     0    NaN  ...  0.409258  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
105     0    NaN  ...  0.177269  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[106 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[-0.045806884765625, 1.0341796875]
[0.2193603515625, 0.7470703125]
[0.312255859375, 0.70166015625]
[0.286865234375, 0.73681640625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.045806884765625, 1.0341796875]
[0.2193603515625, 0.7470703125]
[0.312255859375, 0.70166015625]
[0.286865234375, 0.73681640625]
This is the real loss :  tensor(0.1540, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
102     0    NaN  ...  0.153998  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
103     0    NaN  ...  0.131765  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
104     0    NaN  ...  0.409258  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
105     0    NaN  ...  0.177269  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
106     0    NaN  ...  0.154029  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[107 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.29833984375, 0.66259765625]
[0.38427734375, 0.66162109375]
[0.0809326171875, 0.89404296875]
[-0.01099395751953125, 0.97998046875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.29833984375, 0.66259765625]
[0.38427734375, 0.66162109375]
[0.0809326171875, 0.89404296875]
[-0.01099395751953125, 0.97998046875]
This is the real loss :  tensor(0.0604, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
103     0    NaN  ...  0.131765  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
104     0    NaN  ...  0.409258  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
105     0    NaN  ...  0.177269  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
106     0    NaN  ...  0.154029  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
107     0    NaN  ...  0.060414  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[108 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.298828125, 0.607421875]
[0.0132598876953125, 1.0107421875]
[0.33251953125, 0.72021484375]
[0.09881591796875, 0.9111328125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.298828125, 0.607421875]
[0.0132598876953125, 1.0107421875]
[0.33251953125, 0.72021484375]
[0.09881591796875, 0.9111328125]
This is the real loss :  tensor(0.1532, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
104     0    NaN  ...  0.409258  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
105     0    NaN  ...  0.177269  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
106     0    NaN  ...  0.154029  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
107     0    NaN  ...  0.060414  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
108     0    NaN  ...  0.153201  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[109 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.1002197265625, 0.92529296875]
[0.3310546875, 0.5849609375]
[0.1285400390625, 0.88623046875]
[0.16357421875, 0.87451171875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1002197265625, 0.92529296875]
[0.3310546875, 0.5849609375]
[0.1285400390625, 0.88623046875]
[0.16357421875, 0.87451171875]
This is the real loss :  tensor(0.2524, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
105     0    NaN  ...  0.177269  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
106     0    NaN  ...  0.154029  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
107     0    NaN  ...  0.060414  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
108     0    NaN  ...  0.153201  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
109     0    NaN  ...  0.252450  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[110 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.055877685546875, 0.8935546875]
[0.1337890625, 0.89599609375]
[0.40673828125, 0.62548828125]
[0.1925048828125, 0.79296875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.055877685546875, 0.8935546875]
[0.1337890625, 0.89599609375]
[0.40673828125, 0.62548828125]
[0.1925048828125, 0.79296875]
This is the real loss :  tensor(0.0536, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
106     0    NaN  ...  0.154029  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
107     0    NaN  ...  0.060414  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
108     0    NaN  ...  0.153201  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
109     0    NaN  ...  0.252450  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
110     0    NaN  ...  0.053598  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[111 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.293212890625, 0.73876953125]
[0.304931640625, 0.69580078125]
[0.311279296875, 0.6650390625]
[-0.028167724609375, 1.1328125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.293212890625, 0.73876953125]
[0.304931640625, 0.69580078125]
[0.311279296875, 0.6650390625]
[-0.028167724609375, 1.1328125]
This is the real loss :  tensor(0.1593, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
107     0    NaN  ...  0.060414  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
108     0    NaN  ...  0.153201  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
109     0    NaN  ...  0.252450  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
110     0    NaN  ...  0.053598  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
111     0    NaN  ...  0.159348  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[112 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.375244140625, 0.6845703125]
[0.07049560546875, 0.94775390625]
[0.3173828125, 0.64501953125]
[0.0980224609375, 0.95068359375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.375244140625, 0.6845703125]
[0.07049560546875, 0.94775390625]
[0.3173828125, 0.64501953125]
[0.0980224609375, 0.95068359375]
This is the real loss :  tensor(0.3621, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
108     0    NaN  ...  0.153201  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
109     0    NaN  ...  0.252450  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
110     0    NaN  ...  0.053598  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
111     0    NaN  ...  0.159348  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
112     0    NaN  ...  0.362072  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[113 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.370361328125, 0.7158203125]
[0.3134765625, 0.59326171875]
[0.10400390625, 0.9130859375]
[0.0269927978515625, 0.955078125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.370361328125, 0.7158203125]
[0.3134765625, 0.59326171875]
[0.10400390625, 0.9130859375]
[0.0269927978515625, 0.955078125]
This is the real loss :  tensor(0.1328, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
109     0    NaN  ...  0.252450  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
110     0    NaN  ...  0.053598  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
111     0    NaN  ...  0.159348  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
112     0    NaN  ...  0.362072  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
113     0    NaN  ...  0.132790  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[114 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.39404296875, 0.62109375]
[0.0889892578125, 0.9404296875]
[0.042938232421875, 1.01171875]
[0.322265625, 0.60400390625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.39404296875, 0.62109375]
[0.0889892578125, 0.9404296875]
[0.042938232421875, 1.01171875]
[0.322265625, 0.60400390625]
This is the real loss :  tensor(0.1421, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
110     0    NaN  ...  0.053598  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
111     0    NaN  ...  0.159348  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
112     0    NaN  ...  0.362072  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
113     0    NaN  ...  0.132790  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
114     0    NaN  ...  0.142054  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[115 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.054229736328125, 0.99609375]
[0.28759765625, 0.6943359375]
[0.315185546875, 0.73388671875]
[0.266357421875, 0.69384765625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.054229736328125, 0.99609375]
[0.28759765625, 0.6943359375]
[0.315185546875, 0.73388671875]
[0.266357421875, 0.69384765625]
This is the real loss :  tensor(0.2758, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
111     0    NaN  ...  0.159348  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
112     0    NaN  ...  0.362072  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
113     0    NaN  ...  0.132790  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
114     0    NaN  ...  0.142054  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
115     0    NaN  ...  0.275789  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[116 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.022308349609375, 0.9013671875]
[0.40478515625, 0.62939453125]
[0.340576171875, 0.5966796875]
[0.050567626953125, 0.896484375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.022308349609375, 0.9013671875]
[0.40478515625, 0.62939453125]
[0.340576171875, 0.5966796875]
[0.050567626953125, 0.896484375]
This is the real loss :  tensor(0.1394, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
112     0    NaN  ...  0.362072  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
113     0    NaN  ...  0.132790  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
114     0    NaN  ...  0.142054  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
115     0    NaN  ...  0.275789  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
116     0    NaN  ...  0.139446  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[117 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.1610107421875, 0.91455078125]
[0.166015625, 0.87890625]
[0.1795654296875, 0.8115234375]
[0.377197265625, 0.53759765625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1610107421875, 0.91455078125]
[0.166015625, 0.87890625]
[0.1795654296875, 0.8115234375]
[0.377197265625, 0.53759765625]
This is the real loss :  tensor(0.2605, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
113     0    NaN  ...  0.132790  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
114     0    NaN  ...  0.142054  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
115     0    NaN  ...  0.275789  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
116     0    NaN  ...  0.139446  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
117     0    NaN  ...  0.260504  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[118 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.01806640625, 0.990234375]
[0.383544921875, 0.60302734375]
[0.35546875, 0.69873046875]
[0.181396484375, 0.78076171875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.01806640625, 0.990234375]
[0.383544921875, 0.60302734375]
[0.35546875, 0.69873046875]
[0.181396484375, 0.78076171875]
This is the real loss :  tensor(0.1303, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
114     0    NaN  ...  0.142054  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
115     0    NaN  ...  0.275789  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
116     0    NaN  ...  0.139446  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
117     0    NaN  ...  0.260504  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
118     0    NaN  ...  0.130272  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[119 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.07232666015625, 0.90087890625]
[0.40576171875, 0.62060546875]
[0.340087890625, 0.54833984375]
[0.08038330078125, 0.91064453125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.07232666015625, 0.90087890625]
[0.40576171875, 0.62060546875]
[0.340087890625, 0.54833984375]
[0.08038330078125, 0.91064453125]
This is the real loss :  tensor(0.1343, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
115     0    NaN  ...  0.275789  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
116     0    NaN  ...  0.139446  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
117     0    NaN  ...  0.260504  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
118     0    NaN  ...  0.130272  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
119     0    NaN  ...  0.134281  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[120 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.4765625, 0.5185546875]
[0.108154296875, 0.908203125]
[0.2281494140625, 0.8056640625]
[0.1605224609375, 0.86376953125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.4765625, 0.5185546875]
[0.108154296875, 0.908203125]
[0.2281494140625, 0.8056640625]
[0.1605224609375, 0.86376953125]
This is the real loss :  tensor(0.0766, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
116     0    NaN  ...  0.139446  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
117     0    NaN  ...  0.260504  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
118     0    NaN  ...  0.130272  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
119     0    NaN  ...  0.134281  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
120     0    NaN  ...  0.076646  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[121 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.3154296875, 0.73681640625]
[-0.01361083984375, 0.96533203125]
[0.326904296875, 0.64599609375]
[0.427734375, 0.64013671875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.3154296875, 0.73681640625]
[-0.01361083984375, 0.96533203125]
[0.326904296875, 0.64599609375]
[0.427734375, 0.64013671875]
This is the real loss :  tensor(0.1424, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
117     0    NaN  ...  0.260504  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
118     0    NaN  ...  0.130272  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
119     0    NaN  ...  0.134281  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
120     0    NaN  ...  0.076646  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
121     0    NaN  ...  0.142450  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[122 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[-0.0207366943359375, 0.9970703125]
[0.290771484375, 0.69091796875]
[0.42919921875, 0.59814453125]
[0.3212890625, 0.6572265625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.0207366943359375, 0.9970703125]
[0.290771484375, 0.69091796875]
[0.42919921875, 0.59814453125]
[0.3212890625, 0.6572265625]
This is the real loss :  tensor(0.1356, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
118     0    NaN  ...  0.130272  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
119     0    NaN  ...  0.134281  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
120     0    NaN  ...  0.076646  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
121     0    NaN  ...  0.142450  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
122     0    NaN  ...  0.135604  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[123 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.478515625, 0.603515625]
[0.0556640625, 0.92724609375]
[0.3251953125, 0.560546875]
[0.1190185546875, 0.91015625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.478515625, 0.603515625]
[0.0556640625, 0.92724609375]
[0.3251953125, 0.560546875]
[0.1190185546875, 0.91015625]
This is the real loss :  tensor(0.0895, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
119     0    NaN  ...  0.134281  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
120     0    NaN  ...  0.076646  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
121     0    NaN  ...  0.142450  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
122     0    NaN  ...  0.135604  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
123     0    NaN  ...  0.089460  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[124 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.36083984375, 0.6494140625]
[0.45751953125, 0.50830078125]
[0.0221710205078125, 0.95263671875]
[0.148193359375, 0.9111328125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.36083984375, 0.6494140625]
[0.45751953125, 0.50830078125]
[0.0221710205078125, 0.95263671875]
[0.148193359375, 0.9111328125]
This is the real loss :  tensor(0.1769, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
120     0    NaN  ...  0.076646  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
121     0    NaN  ...  0.142450  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
122     0    NaN  ...  0.135604  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
123     0    NaN  ...  0.089460  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
124     0    NaN  ...  0.176939  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[125 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.478515625, 0.55029296875]
[0.0270233154296875, 0.9794921875]
[0.471923828125, 0.5732421875]
[0.0726318359375, 0.95556640625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.478515625, 0.55029296875]
[0.0270233154296875, 0.9794921875]
[0.471923828125, 0.5732421875]
[0.0726318359375, 0.95556640625]
This is the real loss :  tensor(0.1056, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
121     0    NaN  ...  0.142450  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
122     0    NaN  ...  0.135604  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
123     0    NaN  ...  0.089460  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
124     0    NaN  ...  0.176939  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
125     0    NaN  ...  0.105556  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[126 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.0836181640625, 0.8974609375]
[0.362548828125, 0.61181640625]
[0.5224609375, 0.57275390625]
[0.0309906005859375, 0.92626953125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.0836181640625, 0.8974609375]
[0.362548828125, 0.61181640625]
[0.5224609375, 0.57275390625]
[0.0309906005859375, 0.92626953125]
This is the real loss :  tensor(0.0952, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
122     0    NaN  ...  0.135604  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
123     0    NaN  ...  0.089460  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
124     0    NaN  ...  0.176939  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
125     0    NaN  ...  0.105556  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
126     0    NaN  ...  0.095192  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[127 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.55322265625, 0.466796875]
[0.1243896484375, 0.8798828125]
[0.1292724609375, 0.8525390625]
[0.11358642578125, 0.87255859375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.55322265625, 0.466796875]
[0.1243896484375, 0.8798828125]
[0.1292724609375, 0.8525390625]
[0.11358642578125, 0.87255859375]
This is the real loss :  tensor(0.0644, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
123     0    NaN  ...  0.089460  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
124     0    NaN  ...  0.176939  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
125     0    NaN  ...  0.105556  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
126     0    NaN  ...  0.095192  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
127     0    NaN  ...  0.064376  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[128 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.1912841796875, 0.81982421875]
[0.305419921875, 0.7314453125]
[0.1895751953125, 0.810546875]
[0.2509765625, 0.703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1912841796875, 0.81982421875]
[0.305419921875, 0.7314453125]
[0.1895751953125, 0.810546875]
[0.2509765625, 0.703125]
This is the real loss :  tensor(0.0572, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
124     0    NaN  ...  0.176939  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
125     0    NaN  ...  0.105556  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
126     0    NaN  ...  0.095192  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
127     0    NaN  ...  0.064376  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
128     0    NaN  ...  0.057176  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[129 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.275390625, 0.5966796875]
[0.47216796875, 0.61669921875]
[0.299560546875, 0.62939453125]
[-0.1112060546875, 1.0478515625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.275390625, 0.5966796875]
[0.47216796875, 0.61669921875]
[0.299560546875, 0.62939453125]
[-0.1112060546875, 1.0478515625]
This is the real loss :  tensor(0.2249, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
125     0    NaN  ...  0.105556  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
126     0    NaN  ...  0.095192  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
127     0    NaN  ...  0.064376  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
128     0    NaN  ...  0.057176  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
129     0    NaN  ...  0.224855  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[130 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.5078125, 0.5625]
[0.274658203125, 0.791015625]
[-0.0292816162109375, 1.0283203125]
[0.1890869140625, 0.7421875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.5078125, 0.5625]
[0.274658203125, 0.791015625]
[-0.0292816162109375, 1.0283203125]
[0.1890869140625, 0.7421875]
This is the real loss :  tensor(0.0977, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
126     0    NaN  ...  0.095192  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
127     0    NaN  ...  0.064376  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
128     0    NaN  ...  0.057176  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
129     0    NaN  ...  0.224855  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
130     0    NaN  ...  0.097706  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[131 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.58447265625, 0.4951171875]
[-0.0186614990234375, 0.97900390625]
[0.06951904296875, 0.93505859375]
[0.2227783203125, 0.69140625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.58447265625, 0.4951171875]
[-0.0186614990234375, 0.97900390625]
[0.06951904296875, 0.93505859375]
[0.2227783203125, 0.69140625]
This is the real loss :  tensor(0.0716, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
127     0    NaN  ...  0.064376  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
128     0    NaN  ...  0.057176  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
129     0    NaN  ...  0.224855  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
130     0    NaN  ...  0.097706  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
131     0    NaN  ...  0.071563  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[132 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[-0.10382080078125, 1.0712890625]
[0.456298828125, 0.5576171875]
[0.42431640625, 0.587890625]
[0.11962890625, 0.82080078125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.10382080078125, 1.0712890625]
[0.456298828125, 0.5576171875]
[0.42431640625, 0.587890625]
[0.11962890625, 0.82080078125]
This is the real loss :  tensor(0.1429, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
128     0    NaN  ...  0.057176  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
129     0    NaN  ...  0.224855  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
130     0    NaN  ...  0.097706  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
131     0    NaN  ...  0.071563  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
132     0    NaN  ...  0.142903  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[133 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.0660400390625, 0.982421875]
[0.07257080078125, 0.92431640625]
[0.136962890625, 0.853515625]
[0.57958984375, 0.39599609375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.0660400390625, 0.982421875]
[0.07257080078125, 0.92431640625]
[0.136962890625, 0.853515625]
[0.57958984375, 0.39599609375]
This is the real loss :  tensor(0.0946, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
129     0    NaN  ...  0.224855  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
130     0    NaN  ...  0.097706  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
131     0    NaN  ...  0.071563  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
132     0    NaN  ...  0.142903  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
133     0    NaN  ...  0.094578  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[134 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.43505859375, 0.66748046875]
[0.46728515625, 0.54736328125]
[0.177001953125, 0.8046875]
[-0.1220703125, 1.1318359375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.43505859375, 0.66748046875]
[0.46728515625, 0.54736328125]
[0.177001953125, 0.8046875]
[-0.1220703125, 1.1318359375]
This is the real loss :  tensor(0.1812, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
130     0    NaN  ...  0.097706  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
131     0    NaN  ...  0.071563  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
132     0    NaN  ...  0.142903  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
133     0    NaN  ...  0.094578  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
134     0    NaN  ...  0.181230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[135 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.44482421875, 0.50830078125]
[0.1488037109375, 0.8623046875]
[-0.1689453125, 1.1396484375]
[0.47998046875, 0.52294921875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.44482421875, 0.50830078125]
[0.1488037109375, 0.8623046875]
[-0.1689453125, 1.1396484375]
[0.47998046875, 0.52294921875]
This is the real loss :  tensor(0.1341, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
131     0    NaN  ...  0.071563  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
132     0    NaN  ...  0.142903  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
133     0    NaN  ...  0.094578  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
134     0    NaN  ...  0.181230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
135     0    NaN  ...  0.134085  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[136 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.1595458984375, 0.8681640625]
[0.298583984375, 0.724609375]
[0.06097412109375, 0.91650390625]
[0.24658203125, 0.69384765625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1595458984375, 0.8681640625]
[0.298583984375, 0.724609375]
[0.06097412109375, 0.91650390625]
[0.24658203125, 0.69384765625]
This is the real loss :  tensor(0.0466, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
132     0    NaN  ...  0.142903  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
133     0    NaN  ...  0.094578  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
134     0    NaN  ...  0.181230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
135     0    NaN  ...  0.134085  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
136     0    NaN  ...  0.046631  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[137 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.0771484375, 0.943359375]
[-0.045867919921875, 1.0458984375]
[0.5986328125, 0.4208984375]
[0.2476806640625, 0.74462890625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.0771484375, 0.943359375]
[-0.045867919921875, 1.0458984375]
[0.5986328125, 0.4208984375]
[0.2476806640625, 0.74462890625]
This is the real loss :  tensor(0.1042, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
133     0    NaN  ...  0.094578  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
134     0    NaN  ...  0.181230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
135     0    NaN  ...  0.134085  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
136     0    NaN  ...  0.046631  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
137     0    NaN  ...  0.104206  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[138 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.38623046875, 0.56787109375]
[0.06781005859375, 1.015625]
[0.429443359375, 0.56689453125]
[0.04742431640625, 0.99951171875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.38623046875, 0.56787109375]
[0.06781005859375, 1.015625]
[0.429443359375, 0.56689453125]
[0.04742431640625, 0.99951171875]
This is the real loss :  tensor(0.4072, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
134     0    NaN  ...  0.181230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
135     0    NaN  ...  0.134085  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
136     0    NaN  ...  0.046631  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
137     0    NaN  ...  0.104206  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
138     0    NaN  ...  0.407170  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[139 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[-0.1934814453125, 1.1923828125]
[0.2437744140625, 0.75048828125]
[0.440673828125, 0.60107421875]
[0.429443359375, 0.599609375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.1934814453125, 1.1923828125]
[0.2437744140625, 0.75048828125]
[0.440673828125, 0.60107421875]
[0.429443359375, 0.599609375]
This is the real loss :  tensor(0.2810, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
135     0    NaN  ...  0.134085  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
136     0    NaN  ...  0.046631  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
137     0    NaN  ...  0.104206  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
138     0    NaN  ...  0.407170  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
139     0    NaN  ...  0.280995  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[140 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.37744140625, 0.65869140625]
[0.298583984375, 0.697265625]
[-0.05535888671875, 1.0908203125]
[0.342529296875, 0.66845703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.37744140625, 0.65869140625]
[0.298583984375, 0.697265625]
[-0.05535888671875, 1.0908203125]
[0.342529296875, 0.66845703125]
This is the real loss :  tensor(0.4528, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
136     0    NaN  ...  0.046631  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
137     0    NaN  ...  0.104206  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
138     0    NaN  ...  0.407170  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
139     0    NaN  ...  0.280995  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
140     0    NaN  ...  0.452816  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[141 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.2412109375, 0.806640625]
[0.266357421875, 0.77685546875]
[0.517578125, 0.47314453125]
[-0.08184814453125, 1.0859375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.2412109375, 0.806640625]
[0.266357421875, 0.77685546875]
[0.517578125, 0.47314453125]
[-0.08184814453125, 1.0859375]
This is the real loss :  tensor(0.0970, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
137     0    NaN  ...  0.104206  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
138     0    NaN  ...  0.407170  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
139     0    NaN  ...  0.280995  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
140     0    NaN  ...  0.452816  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
141     0    NaN  ...  0.096982  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[142 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.08575439453125, 0.9091796875]
[0.57568359375, 0.4287109375]
[0.215087890625, 0.80712890625]
[0.0204925537109375, 0.955078125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.08575439453125, 0.9091796875]
[0.57568359375, 0.4287109375]
[0.215087890625, 0.80712890625]
[0.0204925537109375, 0.955078125]
This is the real loss :  tensor(0.0582, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
138     0    NaN  ...  0.407170  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
139     0    NaN  ...  0.280995  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
140     0    NaN  ...  0.452816  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
141     0    NaN  ...  0.096982  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
142     0    NaN  ...  0.058167  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[143 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.26123046875, 0.74072265625]
[0.369384765625, 0.61767578125]
[0.1429443359375, 0.7685546875]
[0.0670166015625, 0.94287109375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.26123046875, 0.74072265625]
[0.369384765625, 0.61767578125]
[0.1429443359375, 0.7685546875]
[0.0670166015625, 0.94287109375]
This is the real loss :  tensor(0.1246, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
139     0    NaN  ...  0.280995  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
140     0    NaN  ...  0.452816  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
141     0    NaN  ...  0.096982  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
142     0    NaN  ...  0.058167  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
143     0    NaN  ...  0.124553  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[144 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.0079345703125, 1.015625]
[0.50048828125, 0.56005859375]
[0.195556640625, 0.7373046875]
[0.331298828125, 0.63330078125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.0079345703125, 1.015625]
[0.50048828125, 0.56005859375]
[0.195556640625, 0.7373046875]
[0.331298828125, 0.63330078125]
This is the real loss :  tensor(0.1899, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
140     0    NaN  ...  0.452816  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
141     0    NaN  ...  0.096982  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
142     0    NaN  ...  0.058167  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
143     0    NaN  ...  0.124553  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
144     0    NaN  ...  0.189871  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[145 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.310791015625, 0.56005859375]
[0.0271759033203125, 0.876953125]
[0.1702880859375, 0.8544921875]
[0.468994140625, 0.5361328125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.310791015625, 0.56005859375]
[0.0271759033203125, 0.876953125]
[0.1702880859375, 0.8544921875]
[0.468994140625, 0.5361328125]
This is the real loss :  tensor(0.0989, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
141     0    NaN  ...  0.096982  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
142     0    NaN  ...  0.058167  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
143     0    NaN  ...  0.124553  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
144     0    NaN  ...  0.189871  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
145     0    NaN  ...  0.098915  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[146 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.4931640625, 0.354248046875]
[0.06402587890625, 0.82666015625]
[0.04388427734375, 0.88720703125]
[0.295166015625, 0.69580078125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.4931640625, 0.354248046875]
[0.06402587890625, 0.82666015625]
[0.04388427734375, 0.88720703125]
[0.295166015625, 0.69580078125]
This is the real loss :  tensor(0.1765, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
142     0    NaN  ...  0.058167  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
143     0    NaN  ...  0.124553  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
144     0    NaN  ...  0.189871  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
145     0    NaN  ...  0.098915  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
146     0    NaN  ...  0.176512  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[147 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.35400390625, 0.61767578125]
[0.55908203125, 0.424072265625]
[-0.00914764404296875, 0.9697265625]
[0.1396484375, 0.84765625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.35400390625, 0.61767578125]
[0.55908203125, 0.424072265625]
[-0.00914764404296875, 0.9697265625]
[0.1396484375, 0.84765625]
This is the real loss :  tensor(0.1521, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
143     0    NaN  ...  0.124553  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
144     0    NaN  ...  0.189871  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
145     0    NaN  ...  0.098915  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
146     0    NaN  ...  0.176512  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
147     0    NaN  ...  0.152099  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[148 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.45166015625, 0.5341796875]
[-0.04425048828125, 1.0380859375]
[0.332275390625, 0.599609375]
[0.38671875, 0.56396484375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.45166015625, 0.5341796875]
[-0.04425048828125, 1.0380859375]
[0.332275390625, 0.599609375]
[0.38671875, 0.56396484375]
This is the real loss :  tensor(0.1962, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
144     0    NaN  ...  0.189871  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
145     0    NaN  ...  0.098915  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
146     0    NaN  ...  0.176512  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
147     0    NaN  ...  0.152099  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
148     0    NaN  ...  0.196182  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[149 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.285400390625, 0.7119140625]
[0.1475830078125, 0.8388671875]
[0.3017578125, 0.6884765625]
[0.339111328125, 0.677734375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.285400390625, 0.7119140625]
[0.1475830078125, 0.8388671875]
[0.3017578125, 0.6884765625]
[0.339111328125, 0.677734375]
This is the real loss :  tensor(0.0774, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
145     0    NaN  ...  0.098915  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
146     0    NaN  ...  0.176512  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
147     0    NaN  ...  0.152099  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
148     0    NaN  ...  0.196182  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
149     0    NaN  ...  0.077393  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[150 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.279052734375, 0.65966796875]
[0.1788330078125, 0.81201171875]
[0.427734375, 0.465576171875]
[0.1307373046875, 0.88232421875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.279052734375, 0.65966796875]
[0.1788330078125, 0.81201171875]
[0.427734375, 0.465576171875]
[0.1307373046875, 0.88232421875]
This is the real loss :  tensor(0.1045, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
146     0    NaN  ...  0.176512  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
147     0    NaN  ...  0.152099  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
148     0    NaN  ...  0.196182  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
149     0    NaN  ...  0.077393  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
150     0    NaN  ...  0.104526  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[151 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.42138671875, 0.56298828125]
[0.54931640625, 0.474365234375]
[0.30126953125, 0.66357421875]
[-0.031402587890625, 1.068359375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.42138671875, 0.56298828125]
[0.54931640625, 0.474365234375]
[0.30126953125, 0.66357421875]
[-0.031402587890625, 1.068359375]
This is the real loss :  tensor(0.1799, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
147     0    NaN  ...  0.152099  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
148     0    NaN  ...  0.196182  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
149     0    NaN  ...  0.077393  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
150     0    NaN  ...  0.104526  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
151     0    NaN  ...  0.179924  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[152 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.654296875, 0.318359375]
[-0.0033283233642578125, 1.0361328125]
[0.16796875, 0.80859375]
[0.3544921875, 0.61279296875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.654296875, 0.318359375]
[-0.0033283233642578125, 1.0361328125]
[0.16796875, 0.80859375]
[0.3544921875, 0.61279296875]
This is the real loss :  tensor(0.0703, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
148     0    NaN  ...  0.196182  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
149     0    NaN  ...  0.077393  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
150     0    NaN  ...  0.104526  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
151     0    NaN  ...  0.179924  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
152     0    NaN  ...  0.070328  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[153 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.14599609375, 0.8271484375]
[0.37255859375, 0.57763671875]
[0.175537109375, 0.7900390625]
[0.438232421875, 0.60400390625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.14599609375, 0.8271484375]
[0.37255859375, 0.57763671875]
[0.175537109375, 0.7900390625]
[0.438232421875, 0.60400390625]
This is the real loss :  tensor(0.0990, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
149     0    NaN  ...  0.077393  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
150     0    NaN  ...  0.104526  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
151     0    NaN  ...  0.179924  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
152     0    NaN  ...  0.070328  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
153     0    NaN  ...  0.099018  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[154 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.59228515625, 0.509765625]
[0.031402587890625, 0.94140625]
[0.123291015625, 0.798828125]
[0.349365234375, 0.56494140625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.59228515625, 0.509765625]
[0.031402587890625, 0.94140625]
[0.123291015625, 0.798828125]
[0.349365234375, 0.56494140625]
This is the real loss :  tensor(0.1536, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
150     0    NaN  ...  0.104526  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
151     0    NaN  ...  0.179924  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
152     0    NaN  ...  0.070328  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
153     0    NaN  ...  0.099018  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
154     0    NaN  ...  0.153583  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[155 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[-0.0015459060668945312, 0.96728515625]
[0.454345703125, 0.5087890625]
[0.281982421875, 0.66943359375]
[0.3671875, 0.54541015625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.0015459060668945312, 0.96728515625]
[0.454345703125, 0.5087890625]
[0.281982421875, 0.66943359375]
[0.3671875, 0.54541015625]
This is the real loss :  tensor(0.1224, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
151     0    NaN  ...  0.179924  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
152     0    NaN  ...  0.070328  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
153     0    NaN  ...  0.099018  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
154     0    NaN  ...  0.153583  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
155     0    NaN  ...  0.122382  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[156 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.5791015625, 0.440673828125]
[-0.048492431640625, 1.1123046875]
[0.609375, 0.462890625]
[0.134033203125, 0.8095703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.5791015625, 0.440673828125]
[-0.048492431640625, 1.1123046875]
[0.609375, 0.462890625]
[0.134033203125, 0.8095703125]
This is the real loss :  tensor(0.3410, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
152     0    NaN  ...  0.070328  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
153     0    NaN  ...  0.099018  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
154     0    NaN  ...  0.153583  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
155     0    NaN  ...  0.122382  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
156     0    NaN  ...  0.341037  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[157 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.09783935546875, 0.85400390625]
[0.15380859375, 0.7763671875]
[0.4423828125, 0.4794921875]
[0.282958984375, 0.623046875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.09783935546875, 0.85400390625]
[0.15380859375, 0.7763671875]
[0.4423828125, 0.4794921875]
[0.282958984375, 0.623046875]
This is the real loss :  tensor(0.1084, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
153     0    NaN  ...  0.099018  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
154     0    NaN  ...  0.153583  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
155     0    NaN  ...  0.122382  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
156     0    NaN  ...  0.341037  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
157     0    NaN  ...  0.108446  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[158 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.0170135498046875, 1.046875]
[0.26025390625, 0.7470703125]
[0.63916015625, 0.4033203125]
[0.22412109375, 0.73583984375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.0170135498046875, 1.046875]
[0.26025390625, 0.7470703125]
[0.63916015625, 0.4033203125]
[0.22412109375, 0.73583984375]
This is the real loss :  tensor(0.3770, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
154     0    NaN  ...  0.153583  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
155     0    NaN  ...  0.122382  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
156     0    NaN  ...  0.341037  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
157     0    NaN  ...  0.108446  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
158     0    NaN  ...  0.376978  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[159 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.04449462890625, 1.009765625]
[0.6591796875, 0.28759765625]
[0.266845703125, 0.69287109375]
[0.111572265625, 0.859375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.04449462890625, 1.009765625]
[0.6591796875, 0.28759765625]
[0.266845703125, 0.69287109375]
[0.111572265625, 0.859375]
This is the real loss :  tensor(0.1427, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
155     0    NaN  ...  0.122382  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
156     0    NaN  ...  0.341037  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
157     0    NaN  ...  0.108446  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
158     0    NaN  ...  0.376978  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
159     0    NaN  ...  0.142734  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[160 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.546875, 0.470703125]
[-0.07354736328125, 1.0537109375]
[0.107177734375, 0.7763671875]
[0.609375, 0.434326171875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.546875, 0.470703125]
[-0.07354736328125, 1.0537109375]
[0.107177734375, 0.7763671875]
[0.609375, 0.434326171875]
This is the real loss :  tensor(0.1238, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
156     0    NaN  ...  0.341037  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
157     0    NaN  ...  0.108446  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
158     0    NaN  ...  0.376978  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
159     0    NaN  ...  0.142734  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
160     0    NaN  ...  0.123781  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[161 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.2425537109375, 0.64208984375]
[0.0748291015625, 0.89599609375]
[0.81640625, 0.32568359375]
[-0.0426025390625, 1.009765625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.2425537109375, 0.64208984375]
[0.0748291015625, 0.89599609375]
[0.81640625, 0.32568359375]
[-0.0426025390625, 1.009765625]
This is the real loss :  tensor(0.2657, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
157     0    NaN  ...  0.108446  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
158     0    NaN  ...  0.376978  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
159     0    NaN  ...  0.142734  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
160     0    NaN  ...  0.123781  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
161     0    NaN  ...  0.265694  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[162 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.3369140625, 0.63818359375]
[0.365478515625, 0.60205078125]
[0.1683349609375, 0.8046875]
[0.194091796875, 0.81103515625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.3369140625, 0.63818359375]
[0.365478515625, 0.60205078125]
[0.1683349609375, 0.8046875]
[0.194091796875, 0.81103515625]
This is the real loss :  tensor(0.3732, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
158     0    NaN  ...  0.376978  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
159     0    NaN  ...  0.142734  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
160     0    NaN  ...  0.123781  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
161     0    NaN  ...  0.265694  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
162     0    NaN  ...  0.373224  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[163 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.05389404296875, 0.955078125]
[0.43603515625, 0.583984375]
[0.59326171875, 0.318603515625]
[-0.046783447265625, 1.005859375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.05389404296875, 0.955078125]
[0.43603515625, 0.583984375]
[0.59326171875, 0.318603515625]
[-0.046783447265625, 1.005859375]
This is the real loss :  tensor(0.1483, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
159     0    NaN  ...  0.142734  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
160     0    NaN  ...  0.123781  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
161     0    NaN  ...  0.265694  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
162     0    NaN  ...  0.373224  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
163     0    NaN  ...  0.148325  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[164 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.1390380859375, 0.74951171875]
[0.044464111328125, 1.0205078125]
[0.131103515625, 0.841796875]
[0.72412109375, 0.33203125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1390380859375, 0.74951171875]
[0.044464111328125, 1.0205078125]
[0.131103515625, 0.841796875]
[0.72412109375, 0.33203125]
This is the real loss :  tensor(0.0391, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
160     0    NaN  ...  0.123781  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
161     0    NaN  ...  0.265694  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
162     0    NaN  ...  0.373224  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
163     0    NaN  ...  0.148325  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
164     0    NaN  ...  0.039130  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[165 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.1256103515625, 0.8564453125]
[0.10986328125, 0.8427734375]
[0.1395263671875, 0.9345703125]
[0.65869140625, 0.341552734375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1256103515625, 0.8564453125]
[0.10986328125, 0.8427734375]
[0.1395263671875, 0.9345703125]
[0.65869140625, 0.341552734375]
This is the real loss :  tensor(0.0413, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
161     0    NaN  ...  0.265694  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
162     0    NaN  ...  0.373224  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
163     0    NaN  ...  0.148325  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
164     0    NaN  ...  0.039130  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
165     0    NaN  ...  0.041259  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[166 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.2359619140625, 0.720703125]
[0.341796875, 0.61376953125]
[0.2098388671875, 0.87890625]
[0.1737060546875, 0.74658203125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.2359619140625, 0.720703125]
[0.341796875, 0.61376953125]
[0.2098388671875, 0.87890625]
[0.1737060546875, 0.74658203125]
This is the real loss :  tensor(0.0691, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
162     0    NaN  ...  0.373224  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
163     0    NaN  ...  0.148325  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
164     0    NaN  ...  0.039130  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
165     0    NaN  ...  0.041259  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
166     0    NaN  ...  0.069097  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[167 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.242431640625, 0.67236328125]
[0.1641845703125, 0.81396484375]
[0.5478515625, 0.475830078125]
[0.028411865234375, 0.98046875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.242431640625, 0.67236328125]
[0.1641845703125, 0.81396484375]
[0.5478515625, 0.475830078125]
[0.028411865234375, 0.98046875]
This is the real loss :  tensor(0.1899, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
163     0    NaN  ...  0.148325  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
164     0    NaN  ...  0.039130  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
165     0    NaN  ...  0.041259  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
166     0    NaN  ...  0.069097  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
167     0    NaN  ...  0.189949  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[168 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.61962890625, 0.437744140625]
[0.040924072265625, 0.93359375]
[0.21435546875, 0.7861328125]
[0.112060546875, 0.8720703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.61962890625, 0.437744140625]
[0.040924072265625, 0.93359375]
[0.21435546875, 0.7861328125]
[0.112060546875, 0.8720703125]
This is the real loss :  tensor(0.1033, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
164     0    NaN  ...  0.039130  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
165     0    NaN  ...  0.041259  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
166     0    NaN  ...  0.069097  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
167     0    NaN  ...  0.189949  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
168     0    NaN  ...  0.103346  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[169 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.476806640625, 0.54736328125]
[0.004207611083984375, 1.033203125]
[0.227783203125, 0.78662109375]
[0.32177734375, 0.77001953125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.476806640625, 0.54736328125]
[0.004207611083984375, 1.033203125]
[0.227783203125, 0.78662109375]
[0.32177734375, 0.77001953125]
This is the real loss :  tensor(0.1980, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
165     0    NaN  ...  0.041259  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
166     0    NaN  ...  0.069097  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
167     0    NaN  ...  0.189949  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
168     0    NaN  ...  0.103346  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
169     0    NaN  ...  0.197960  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[170 rows x 5 columns]checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3

Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.42529296875, 0.5078125]
[0.037689208984375, 0.9443359375]
[0.0208740234375, 0.96142578125]
[0.51708984375, 0.53125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.42529296875, 0.5078125]
[0.037689208984375, 0.9443359375]
[0.0208740234375, 0.96142578125]
[0.51708984375, 0.53125]
This is the real loss :  tensor(0.3739, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
166     0    NaN  ...  0.069097  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
167     0    NaN  ...  0.189949  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
168     0    NaN  ...  0.103346  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
169     0    NaN  ...  0.197960  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
170     0    NaN  ...  0.373892  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[171 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.3193359375, 0.64404296875]
[0.1614990234375, 0.8486328125]
[0.36962890625, 0.6513671875]
[0.151123046875, 0.84130859375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.3193359375, 0.64404296875]
[0.1614990234375, 0.8486328125]
[0.36962890625, 0.6513671875]
[0.151123046875, 0.84130859375]
This is the real loss :  tensor(0.3267, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
167     0    NaN  ...  0.189949  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
168     0    NaN  ...  0.103346  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
169     0    NaN  ...  0.197960  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
170     0    NaN  ...  0.373892  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
171     0    NaN  ...  0.326706  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[172 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.418701171875, 0.65234375]
[0.358154296875, 0.67919921875]
[0.016937255859375, 1.072265625]
[0.34375, 0.7509765625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.418701171875, 0.65234375]
[0.358154296875, 0.67919921875]
[0.016937255859375, 1.072265625]
[0.34375, 0.7509765625]
This is the real loss :  tensor(0.1694, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
168     0    NaN  ...  0.103346  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
169     0    NaN  ...  0.197960  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
170     0    NaN  ...  0.373892  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
171     0    NaN  ...  0.326706  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
172     0    NaN  ...  0.169392  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[173 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.008575439453125, 0.87744140625]
[0.28466796875, 0.51318359375]
[0.51806640625, 0.51220703125]
[0.07318115234375, 0.82421875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.008575439453125, 0.87744140625]
[0.28466796875, 0.51318359375]
[0.51806640625, 0.51220703125]
[0.07318115234375, 0.82421875]
This is the real loss :  tensor(0.1095, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
169     0    NaN  ...  0.197960  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
170     0    NaN  ...  0.373892  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
171     0    NaN  ...  0.326706  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
172     0    NaN  ...  0.169392  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
173     0    NaN  ...  0.109464  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[174 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.067626953125, 0.9140625]
[0.0738525390625, 0.95263671875]
[0.513671875, 0.49755859375]
[0.427978515625, 0.58740234375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.067626953125, 0.9140625]
[0.0738525390625, 0.95263671875]
[0.513671875, 0.49755859375]
[0.427978515625, 0.58740234375]
This is the real loss :  tensor(0.1071, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
170     0    NaN  ...  0.373892  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
171     0    NaN  ...  0.326706  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
172     0    NaN  ...  0.169392  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
173     0    NaN  ...  0.109464  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
174     0    NaN  ...  0.107142  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[175 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.2237548828125, 0.7392578125]
[0.223876953125, 0.79833984375]
[0.2939453125, 0.74365234375]
[0.26025390625, 0.77783203125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.2237548828125, 0.7392578125]
[0.223876953125, 0.79833984375]
[0.2939453125, 0.74365234375]
[0.26025390625, 0.77783203125]
This is the real loss :  tensor(0.0598, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
171     0    NaN  ...  0.326706  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
172     0    NaN  ...  0.169392  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
173     0    NaN  ...  0.109464  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
174     0    NaN  ...  0.107142  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
175     0    NaN  ...  0.059756  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[176 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.1424560546875, 0.8974609375]
[0.1712646484375, 0.9130859375]
[0.53076171875, 0.406005859375]
[0.183837890625, 0.87939453125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1424560546875, 0.8974609375]
[0.1712646484375, 0.9130859375]
[0.53076171875, 0.406005859375]
[0.183837890625, 0.87939453125]
This is the real loss :  tensor(0.0626, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
172     0    NaN  ...  0.169392  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
173     0    NaN  ...  0.109464  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
174     0    NaN  ...  0.107142  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
175     0    NaN  ...  0.059756  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
176     0    NaN  ...  0.062633  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[177 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.50048828125, 0.50048828125]
[0.332275390625, 0.61279296875]
[0.06134033203125, 0.9697265625]
[0.07470703125, 0.90380859375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.50048828125, 0.50048828125]
[0.332275390625, 0.61279296875]
[0.06134033203125, 0.9697265625]
[0.07470703125, 0.90380859375]
This is the real loss :  tensor(0.1676, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
173     0    NaN  ...  0.109464  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
174     0    NaN  ...  0.107142  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
175     0    NaN  ...  0.059756  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
176     0    NaN  ...  0.062633  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
177     0    NaN  ...  0.167611  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[178 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.1199951171875, 0.91943359375]
[0.2142333984375, 0.7763671875]
[0.51171875, 0.4638671875]
[0.154052734375, 0.88134765625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1199951171875, 0.91943359375]
[0.2142333984375, 0.7763671875]
[0.51171875, 0.4638671875]
[0.154052734375, 0.88134765625]
This is the real loss :  tensor(0.2285, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
174     0    NaN  ...  0.107142  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
175     0    NaN  ...  0.059756  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
176     0    NaN  ...  0.062633  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
177     0    NaN  ...  0.167611  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
178     0    NaN  ...  0.228521  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[179 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.273681640625, 0.78515625]
[0.4140625, 0.6591796875]
[0.021209716796875, 0.990234375]
[0.40966796875, 0.64599609375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.273681640625, 0.78515625]
[0.4140625, 0.6591796875]
[0.021209716796875, 0.990234375]
[0.40966796875, 0.64599609375]
This is the real loss :  tensor(0.2082, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
175     0    NaN  ...  0.059756  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
176     0    NaN  ...  0.062633  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
177     0    NaN  ...  0.167611  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
178     0    NaN  ...  0.228521  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
179     0    NaN  ...  0.208156  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[180 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.1478271484375, 0.86376953125]
[0.2098388671875, 0.76806640625]
[0.10009765625, 0.896484375]
[0.56005859375, 0.484619140625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1478271484375, 0.86376953125]
[0.2098388671875, 0.76806640625]
[0.10009765625, 0.896484375]
[0.56005859375, 0.484619140625]
This is the real loss :  tensor(0.0734, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
176     0    NaN  ...  0.062633  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
177     0    NaN  ...  0.167611  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
178     0    NaN  ...  0.228521  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
179     0    NaN  ...  0.208156  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
180     0    NaN  ...  0.073422  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[181 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.303466796875, 0.71337890625]
[0.29150390625, 0.65185546875]
[-0.0290679931640625, 1.017578125]
[0.497802734375, 0.5703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.303466796875, 0.71337890625]
[0.29150390625, 0.65185546875]
[-0.0290679931640625, 1.017578125]
[0.497802734375, 0.5703125]
This is the real loss :  tensor(0.1918, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
177     0    NaN  ...  0.167611  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
178     0    NaN  ...  0.228521  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
179     0    NaN  ...  0.208156  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
180     0    NaN  ...  0.073422  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
181     0    NaN  ...  0.191840  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[182 rows x 5 columns]
Epoch 2/50
Logits shape before squeeze: torch.Size([4, 2])
[0.472412109375, 0.51611328125]
[0.030242919921875, 0.90673828125]
[0.421630859375, 0.626953125]
[0.07281494140625, 0.9072265625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.472412109375, 0.51611328125]
[0.030242919921875, 0.90673828125]
[0.421630859375, 0.626953125]
[0.07281494140625, 0.9072265625]
This is the real loss :  tensor(0.3298, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
178     0    NaN  ...  0.228521  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
179     0    NaN  ...  0.208156  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
180     0    NaN  ...  0.073422  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
181     0    NaN  ...  0.191840  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
182     1    NaN  ...  0.329771  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[183 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.11749267578125, 0.89111328125]
[0.67333984375, 0.380615234375]
[0.1331787109375, 0.88916015625]
[0.1495361328125, 0.87744140625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.11749267578125, 0.89111328125]
[0.67333984375, 0.380615234375]
[0.1331787109375, 0.88916015625]
[0.1495361328125, 0.87744140625]
This is the real loss :  tensor(0.3053, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
179     0    NaN  ...  0.208156  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
180     0    NaN  ...  0.073422  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
181     0    NaN  ...  0.191840  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
182     1    NaN  ...  0.329771  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
183     1    NaN  ...  0.305256  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[184 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.34375, 0.6669921875]
[0.3623046875, 0.74658203125]
[-0.037017822265625, 1.0400390625]
[0.46337890625, 0.5751953125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.34375, 0.6669921875]
[0.3623046875, 0.74658203125]
[-0.037017822265625, 1.0400390625]
[0.46337890625, 0.5751953125]
This is the real loss :  tensor(0.1308, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
180     0    NaN  ...  0.073422  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
181     0    NaN  ...  0.191840  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
182     1    NaN  ...  0.329771  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
183     1    NaN  ...  0.305256  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
184     1    NaN  ...  0.130791  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[185 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.495849609375, 0.360107421875]
[0.1571044921875, 0.8544921875]
[0.1787109375, 0.8603515625]
[0.1541748046875, 0.87939453125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.495849609375, 0.360107421875]
[0.1571044921875, 0.8544921875]
[0.1787109375, 0.8603515625]
[0.1541748046875, 0.87939453125]
This is the real loss :  tensor(0.0989, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
181     0    NaN  ...  0.191840  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
182     1    NaN  ...  0.329771  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
183     1    NaN  ...  0.305256  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
184     1    NaN  ...  0.130791  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
185     1    NaN  ...  0.098867  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[186 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.405517578125, 0.58984375]
[0.295654296875, 0.76123046875]
[0.307861328125, 0.86474609375]
[0.2119140625, 0.83642578125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.405517578125, 0.58984375]
[0.295654296875, 0.76123046875]
[0.307861328125, 0.86474609375]
[0.2119140625, 0.83642578125]
This is the real loss :  tensor(0.3383, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
182     1    NaN  ...  0.329771  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
183     1    NaN  ...  0.305256  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
184     1    NaN  ...  0.130791  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
185     1    NaN  ...  0.098867  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
186     1    NaN  ...  0.338344  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[187 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.4755859375, 0.464599609375]
[0.388427734375, 0.5576171875]
[0.098388671875, 0.93505859375]
[0.132080078125, 0.92431640625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.4755859375, 0.464599609375]
[0.388427734375, 0.5576171875]
[0.098388671875, 0.93505859375]
[0.132080078125, 0.92431640625]
This is the real loss :  tensor(0.1121, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
183     1    NaN  ...  0.305256  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
184     1    NaN  ...  0.130791  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
185     1    NaN  ...  0.098867  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
186     1    NaN  ...  0.338344  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
187     1    NaN  ...  0.112061  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[188 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.1612548828125, 0.84423828125]
[0.147705078125, 0.87255859375]
[0.546875, 0.42041015625]
[0.2137451171875, 0.798828125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1612548828125, 0.84423828125]
[0.147705078125, 0.87255859375]
[0.546875, 0.42041015625]
[0.2137451171875, 0.798828125]
This is the real loss :  tensor(0.0696, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
184     1    NaN  ...  0.130791  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
185     1    NaN  ...  0.098867  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
186     1    NaN  ...  0.338344  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
187     1    NaN  ...  0.112061  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
188     1    NaN  ...  0.069568  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[189 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.0224761962890625, 0.90869140625]
[0.3671875, 0.5458984375]
[0.24072265625, 0.7646484375]
[0.432373046875, 0.556640625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.0224761962890625, 0.90869140625]
[0.3671875, 0.5458984375]
[0.24072265625, 0.7646484375]
[0.432373046875, 0.556640625]
This is the real loss :  tensor(0.2679, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
185     1    NaN  ...  0.098867  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
186     1    NaN  ...  0.338344  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
187     1    NaN  ...  0.112061  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
188     1    NaN  ...  0.069568  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
189     1    NaN  ...  0.267889  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[190 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.1805419921875, 0.779296875]
[0.455078125, 0.568359375]
[0.35595703125, 0.62890625]
[0.10723876953125, 0.87158203125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1805419921875, 0.779296875]
[0.455078125, 0.568359375]
[0.35595703125, 0.62890625]
[0.10723876953125, 0.87158203125]
This is the real loss :  tensor(0.1924, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
186     1    NaN  ...  0.338344  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
187     1    NaN  ...  0.112061  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
188     1    NaN  ...  0.069568  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
189     1    NaN  ...  0.267889  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
190     1    NaN  ...  0.192448  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[191 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.52783203125, 0.45703125]
[0.34423828125, 0.48779296875]
[0.0899658203125, 0.88330078125]
[0.0972900390625, 0.86083984375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.52783203125, 0.45703125]
[0.34423828125, 0.48779296875]
[0.0899658203125, 0.88330078125]
[0.0972900390625, 0.86083984375]
This is the real loss :  tensor(0.1079, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
187     1    NaN  ...  0.112061  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
188     1    NaN  ...  0.069568  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
189     1    NaN  ...  0.267889  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
190     1    NaN  ...  0.192448  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
191     1    NaN  ...  0.107902  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[192 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.08001708984375, 0.7958984375]
[0.2305908203125, 0.6201171875]
[0.455078125, 0.57861328125]
[0.2489013671875, 0.6201171875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.08001708984375, 0.7958984375]
[0.2305908203125, 0.6201171875]
[0.455078125, 0.57861328125]
[0.2489013671875, 0.6201171875]
This is the real loss :  tensor(0.1354, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
188     1    NaN  ...  0.069568  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
189     1    NaN  ...  0.267889  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
190     1    NaN  ...  0.192448  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
191     1    NaN  ...  0.107902  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
192     1    NaN  ...  0.135442  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[193 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.275634765625, 0.7177734375]
[0.212158203125, 0.77392578125]
[0.3193359375, 0.6923828125]
[0.265380859375, 0.7265625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.275634765625, 0.7177734375]
[0.212158203125, 0.77392578125]
[0.3193359375, 0.6923828125]
[0.265380859375, 0.7265625]
This is the real loss :  tensor(0.1675, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
189     1    NaN  ...  0.267889  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
190     1    NaN  ...  0.192448  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
191     1    NaN  ...  0.107902  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
192     1    NaN  ...  0.135442  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
193     1    NaN  ...  0.167455  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[194 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.3232421875, 0.61181640625]
[0.257080078125, 0.63916015625]
[0.1888427734375, 0.65576171875]
[0.231201171875, 0.6826171875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.3232421875, 0.61181640625]
[0.257080078125, 0.63916015625]
[0.1888427734375, 0.65576171875]
[0.231201171875, 0.6826171875]
This is the real loss :  tensor(0.0950, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
190     1    NaN  ...  0.192448  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
191     1    NaN  ...  0.107902  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
192     1    NaN  ...  0.135442  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
193     1    NaN  ...  0.167455  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
194     1    NaN  ...  0.094977  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[195 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.180908203125, 0.8798828125]
[0.1585693359375, 0.828125]
[0.2469482421875, 0.7724609375]
[0.5107421875, 0.408447265625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.180908203125, 0.8798828125]
[0.1585693359375, 0.828125]
[0.2469482421875, 0.7724609375]
[0.5107421875, 0.408447265625]
This is the real loss :  tensor(0.1032, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
191     1    NaN  ...  0.107902  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
192     1    NaN  ...  0.135442  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
193     1    NaN  ...  0.167455  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
194     1    NaN  ...  0.094977  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
195     1    NaN  ...  0.103174  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[196 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.12353515625, 0.7890625]
[0.03936767578125, 0.9501953125]
[0.360595703125, 0.5263671875]
[0.50634765625, 0.451416015625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.12353515625, 0.7890625]
[0.03936767578125, 0.9501953125]
[0.360595703125, 0.5263671875]
[0.50634765625, 0.451416015625]
This is the real loss :  tensor(0.1219, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
192     1    NaN  ...  0.135442  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
193     1    NaN  ...  0.167455  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
194     1    NaN  ...  0.094977  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
195     1    NaN  ...  0.103174  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
196     1    NaN  ...  0.121934  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[197 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[-0.03076171875, 0.9599609375]
[0.389404296875, 0.6787109375]
[0.303466796875, 0.51318359375]
[0.393310546875, 0.50146484375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.03076171875, 0.9599609375]
[0.389404296875, 0.6787109375]
[0.303466796875, 0.51318359375]
[0.393310546875, 0.50146484375]
This is the real loss :  tensor(0.1761, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
193     1    NaN  ...  0.167455  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
194     1    NaN  ...  0.094977  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
195     1    NaN  ...  0.103174  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
196     1    NaN  ...  0.121934  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
197     1    NaN  ...  0.176145  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[198 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.430419921875, 0.53515625]
[0.12078857421875, 0.83203125]
[0.46533203125, 0.599609375]
[0.0438232421875, 0.783203125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.430419921875, 0.53515625]
[0.12078857421875, 0.83203125]
[0.46533203125, 0.599609375]
[0.0438232421875, 0.783203125]
This is the real loss :  tensor(0.1087, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
194     1    NaN  ...  0.094977  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
195     1    NaN  ...  0.103174  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
196     1    NaN  ...  0.121934  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
197     1    NaN  ...  0.176145  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
198     1    NaN  ...  0.108739  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[199 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.045166015625, 0.90283203125]
[0.50439453125, 0.5546875]
[0.1116943359375, 0.81298828125]
[0.41015625, 0.52734375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.045166015625, 0.90283203125]
[0.50439453125, 0.5546875]
[0.1116943359375, 0.81298828125]
[0.41015625, 0.52734375]
This is the real loss :  tensor(0.1129, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
195     1    NaN  ...  0.103174  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
196     1    NaN  ...  0.121934  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
197     1    NaN  ...  0.176145  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
198     1    NaN  ...  0.108739  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
199     1    NaN  ...  0.112910  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[200 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.1868896484375, 0.81591796875]
[0.480712890625, 0.4990234375]
[0.1859130859375, 0.87353515625]
[0.16845703125, 0.8740234375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1868896484375, 0.81591796875]
[0.480712890625, 0.4990234375]
[0.1859130859375, 0.87353515625]
[0.16845703125, 0.8740234375]
This is the real loss :  tensor(0.0807, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
196     1    NaN  ...  0.121934  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
197     1    NaN  ...  0.176145  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
198     1    NaN  ...  0.108739  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
199     1    NaN  ...  0.112910  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
200     1    NaN  ...  0.080710  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[201 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.424072265625, 0.63037109375]
[-0.0088958740234375, 1.021484375]
[0.379638671875, 0.6318359375]
[0.313232421875, 0.76708984375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.424072265625, 0.63037109375]
[-0.0088958740234375, 1.021484375]
[0.379638671875, 0.6318359375]
[0.313232421875, 0.76708984375]
This is the real loss :  tensor(0.1452, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
197     1    NaN  ...  0.176145  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
198     1    NaN  ...  0.108739  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
199     1    NaN  ...  0.112910  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
200     1    NaN  ...  0.080710  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
201     1    NaN  ...  0.145204  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[202 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.33984375, 0.64501953125]
[0.378173828125, 0.65869140625]
[-0.08624267578125, 1.04296875]
[0.4365234375, 0.64013671875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.33984375, 0.64501953125]
[0.378173828125, 0.65869140625]
[-0.08624267578125, 1.04296875]
[0.4365234375, 0.64013671875]
This is the real loss :  tensor(0.1038, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
198     1    NaN  ...  0.108739  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
199     1    NaN  ...  0.112910  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
200     1    NaN  ...  0.080710  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
201     1    NaN  ...  0.145204  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
202     1    NaN  ...  0.103794  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[203 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.406494140625, 0.64404296875]
[0.469482421875, 0.55322265625]
[1.1861324310302734e-05, 1.0234375]
[0.08251953125, 0.9345703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.406494140625, 0.64404296875]
[0.469482421875, 0.55322265625]
[1.1861324310302734e-05, 1.0234375]
[0.08251953125, 0.9345703125]
This is the real loss :  tensor(0.1114, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
199     1    NaN  ...  0.112910  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
200     1    NaN  ...  0.080710  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
201     1    NaN  ...  0.145204  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
202     1    NaN  ...  0.103794  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
203     1    NaN  ...  0.111386  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[204 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.114990234375, 0.904296875]
[0.322998046875, 0.69921875]
[-0.0007462501525878906, 1.0498046875]
[0.4990234375, 0.580078125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.114990234375, 0.904296875]
[0.322998046875, 0.69921875]
[-0.0007462501525878906, 1.0498046875]
[0.4990234375, 0.580078125]
This is the real loss :  tensor(0.0806, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
200     1    NaN  ...  0.080710  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
201     1    NaN  ...  0.145204  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
202     1    NaN  ...  0.103794  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
203     1    NaN  ...  0.111386  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
204     1    NaN  ...  0.080627  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[205 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.3046875, 0.748046875]
[0.2076416015625, 0.77783203125]
[0.284912109375, 0.8095703125]
[0.182861328125, 0.9296875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.3046875, 0.748046875]
[0.2076416015625, 0.77783203125]
[0.284912109375, 0.8095703125]
[0.182861328125, 0.9296875]
This is the real loss :  tensor(0.3243, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
201     1    NaN  ...  0.145204  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
202     1    NaN  ...  0.103794  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
203     1    NaN  ...  0.111386  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
204     1    NaN  ...  0.080627  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
205     1    NaN  ...  0.324288  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[206 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.416259765625, 0.61572265625]
[0.409423828125, 0.67431640625]
[0.024627685546875, 1.021484375]
[0.002605438232421875, 0.982421875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.416259765625, 0.61572265625]
[0.409423828125, 0.67431640625]
[0.024627685546875, 1.021484375]
[0.002605438232421875, 0.982421875]
This is the real loss :  tensor(0.1407, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
202     1    NaN  ...  0.103794  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
203     1    NaN  ...  0.111386  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
204     1    NaN  ...  0.080627  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
205     1    NaN  ...  0.324288  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
206     1    NaN  ...  0.140726  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[207 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.1951904296875, 0.7861328125]
[0.307861328125, 0.671875]
[0.36962890625, 0.64794921875]
[-0.10040283203125, 1.0947265625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1951904296875, 0.7861328125]
[0.307861328125, 0.671875]
[0.36962890625, 0.64794921875]
[-0.10040283203125, 1.0947265625]
This is the real loss :  tensor(0.1617, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
203     1    NaN  ...  0.111386  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
204     1    NaN  ...  0.080627  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
205     1    NaN  ...  0.324288  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
206     1    NaN  ...  0.140726  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
207     1    NaN  ...  0.161741  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[208 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[-0.0740966796875, 1.1005859375]
[0.219482421875, 0.76220703125]
[0.3642578125, 0.740234375]
[0.31982421875, 0.68505859375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.0740966796875, 1.1005859375]
[0.219482421875, 0.76220703125]
[0.3642578125, 0.740234375]
[0.31982421875, 0.68505859375]
This is the real loss :  tensor(0.3862, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
204     1    NaN  ...  0.080627  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
205     1    NaN  ...  0.324288  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
206     1    NaN  ...  0.140726  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
207     1    NaN  ...  0.161741  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
208     1    NaN  ...  0.386229  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[209 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.1492919921875, 0.84765625]
[0.07659912109375, 0.8935546875]
[0.43310546875, 0.5517578125]
[0.0439453125, 0.99365234375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1492919921875, 0.84765625]
[0.07659912109375, 0.8935546875]
[0.43310546875, 0.5517578125]
[0.0439453125, 0.99365234375]
This is the real loss :  tensor(0.0863, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
205     1    NaN  ...  0.324288  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
206     1    NaN  ...  0.140726  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
207     1    NaN  ...  0.161741  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
208     1    NaN  ...  0.386229  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
209     1    NaN  ...  0.086309  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[210 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[-0.1134033203125, 1.1171875]
[0.336181640625, 0.69384765625]
[0.3349609375, 0.712890625]
[0.338623046875, 0.72216796875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.1134033203125, 1.1171875]
[0.336181640625, 0.69384765625]
[0.3349609375, 0.712890625]
[0.338623046875, 0.72216796875]
This is the real loss :  tensor(0.2614, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
206     1    NaN  ...  0.140726  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
207     1    NaN  ...  0.161741  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
208     1    NaN  ...  0.386229  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
209     1    NaN  ...  0.086309  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
210     1    NaN  ...  0.261377  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[211 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.00716400146484375, 0.9765625]
[0.303466796875, 0.708984375]
[0.0265960693359375, 0.98291015625]
[0.4150390625, 0.56884765625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.00716400146484375, 0.9765625]
[0.303466796875, 0.708984375]
[0.0265960693359375, 0.98291015625]
[0.4150390625, 0.56884765625]
This is the real loss :  tensor(0.4492, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
207     1    NaN  ...  0.161741  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
208     1    NaN  ...  0.386229  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
209     1    NaN  ...  0.086309  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
210     1    NaN  ...  0.261377  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
211     1    NaN  ...  0.449248  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[212 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.0235443115234375, 1.0087890625]
[0.30322265625, 0.65087890625]
[0.4267578125, 0.57861328125]
[0.0311737060546875, 0.984375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.0235443115234375, 1.0087890625]
[0.30322265625, 0.65087890625]
[0.4267578125, 0.57861328125]
[0.0311737060546875, 0.984375]
This is the real loss :  tensor(0.0719, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
208     1    NaN  ...  0.386229  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
209     1    NaN  ...  0.086309  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
210     1    NaN  ...  0.261377  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
211     1    NaN  ...  0.449248  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
212     1    NaN  ...  0.071921  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[213 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.4111328125, 0.56982421875]
[0.396484375, 0.64111328125]
[0.023284912109375, 1.0107421875]
[0.045867919921875, 0.96728515625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.4111328125, 0.56982421875]
[0.396484375, 0.64111328125]
[0.023284912109375, 1.0107421875]
[0.045867919921875, 0.96728515625]
This is the real loss :  tensor(0.1202, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
209     1    NaN  ...  0.086309  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
210     1    NaN  ...  0.261377  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
211     1    NaN  ...  0.449248  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
212     1    NaN  ...  0.071921  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
213     1    NaN  ...  0.120162  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[214 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.40380859375, 0.5537109375]
[0.0609130859375, 0.9912109375]
[0.287109375, 0.734375]
[0.126953125, 0.90771484375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.40380859375, 0.5537109375]
[0.0609130859375, 0.9912109375]
[0.287109375, 0.734375]
[0.126953125, 0.90771484375]
This is the real loss :  tensor(0.2631, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
210     1    NaN  ...  0.261377  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
211     1    NaN  ...  0.449248  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
212     1    NaN  ...  0.071921  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
213     1    NaN  ...  0.120162  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
214     1    NaN  ...  0.263146  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[215 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.498779296875, 0.55712890625]
[0.07666015625, 0.890625]
[0.0057830810546875, 0.97802734375]
[0.293701171875, 0.560546875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.498779296875, 0.55712890625]
[0.07666015625, 0.890625]
[0.0057830810546875, 0.97802734375]
[0.293701171875, 0.560546875]
This is the real loss :  tensor(0.1741, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
211     1    NaN  ...  0.449248  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
212     1    NaN  ...  0.071921  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
213     1    NaN  ...  0.120162  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
214     1    NaN  ...  0.263146  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
215     1    NaN  ...  0.174130  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[216 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.3232421875, 0.64794921875]
[0.062744140625, 1.0029296875]
[0.324462890625, 0.6943359375]
[0.26220703125, 0.74951171875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.3232421875, 0.64794921875]
[0.062744140625, 1.0029296875]
[0.324462890625, 0.6943359375]
[0.26220703125, 0.74951171875]
This is the real loss :  tensor(0.1515, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
212     1    NaN  ...  0.071921  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
213     1    NaN  ...  0.120162  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
214     1    NaN  ...  0.263146  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
215     1    NaN  ...  0.174130  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
216     1    NaN  ...  0.151499  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[217 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.27294921875, 0.72802734375]
[0.0276641845703125, 1.0048828125]
[0.294189453125, 0.6357421875]
[0.412353515625, 0.580078125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.27294921875, 0.72802734375]
[0.0276641845703125, 1.0048828125]
[0.294189453125, 0.6357421875]
[0.412353515625, 0.580078125]
This is the real loss :  tensor(0.2031, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
213     1    NaN  ...  0.120162  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
214     1    NaN  ...  0.263146  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
215     1    NaN  ...  0.174130  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
216     1    NaN  ...  0.151499  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
217     1    NaN  ...  0.203127  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[218 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.383544921875, 0.54638671875]
[0.07269287109375, 0.95751953125]
[0.380859375, 0.630859375]
[0.126953125, 0.86572265625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.383544921875, 0.54638671875]
[0.07269287109375, 0.95751953125]
[0.380859375, 0.630859375]
[0.126953125, 0.86572265625]
This is the real loss :  tensor(0.1876, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
214     1    NaN  ...  0.263146  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
215     1    NaN  ...  0.174130  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
216     1    NaN  ...  0.151499  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
217     1    NaN  ...  0.203127  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
218     1    NaN  ...  0.187639  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[219 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.07244873046875, 0.91650390625]
[0.430419921875, 0.52294921875]
[0.10009765625, 0.84326171875]
[0.421142578125, 0.59423828125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.07244873046875, 0.91650390625]
[0.430419921875, 0.52294921875]
[0.10009765625, 0.84326171875]
[0.421142578125, 0.59423828125]
This is the real loss :  tensor(0.1233, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
215     1    NaN  ...  0.174130  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
216     1    NaN  ...  0.151499  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
217     1    NaN  ...  0.203127  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
218     1    NaN  ...  0.187639  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
219     1    NaN  ...  0.123338  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[220 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.22119140625, 0.77880859375]
[0.46337890625, 0.55517578125]
[0.302978515625, 0.63427734375]
[0.1396484375, 0.7939453125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.22119140625, 0.77880859375]
[0.46337890625, 0.55517578125]
[0.302978515625, 0.63427734375]
[0.1396484375, 0.7939453125]
This is the real loss :  tensor(0.3691, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
216     1    NaN  ...  0.151499  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
217     1    NaN  ...  0.203127  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
218     1    NaN  ...  0.187639  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
219     1    NaN  ...  0.123338  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
220     1    NaN  ...  0.369092  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[221 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.1715087890625, 0.8564453125]
[0.5107421875, 0.4921875]
[0.283935546875, 0.67724609375]
[0.11224365234375, 0.89599609375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1715087890625, 0.8564453125]
[0.5107421875, 0.4921875]
[0.283935546875, 0.67724609375]
[0.11224365234375, 0.89599609375]
This is the real loss :  tensor(0.0971, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
217     1    NaN  ...  0.203127  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
218     1    NaN  ...  0.187639  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
219     1    NaN  ...  0.123338  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
220     1    NaN  ...  0.369092  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
221     1    NaN  ...  0.097120  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[222 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.53271484375, 0.46728515625]
[0.1658935546875, 0.83642578125]
[0.09100341796875, 0.935546875]
[0.33837890625, 0.69140625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.53271484375, 0.46728515625]
[0.1658935546875, 0.83642578125]
[0.09100341796875, 0.935546875]
[0.33837890625, 0.69140625]
This is the real loss :  tensor(0.1055, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
218     1    NaN  ...  0.187639  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
219     1    NaN  ...  0.123338  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
220     1    NaN  ...  0.369092  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
221     1    NaN  ...  0.097120  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
222     1    NaN  ...  0.105502  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[223 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.310546875, 0.68896484375]
[0.258056640625, 0.72607421875]
[0.290771484375, 0.68310546875]
[0.2607421875, 0.7529296875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.310546875, 0.68896484375]
[0.258056640625, 0.72607421875]
[0.290771484375, 0.68310546875]
[0.2607421875, 0.7529296875]
This is the real loss :  tensor(0.1792, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
219     1    NaN  ...  0.123338  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
220     1    NaN  ...  0.369092  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
221     1    NaN  ...  0.097120  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
222     1    NaN  ...  0.105502  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
223     1    NaN  ...  0.179185  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[224 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.1195068359375, 0.84228515625]
[0.46484375, 0.5166015625]
[0.1319580078125, 0.86328125]
[0.43212890625, 0.568359375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1195068359375, 0.84228515625]
[0.46484375, 0.5166015625]
[0.1319580078125, 0.86328125]
[0.43212890625, 0.568359375]
This is the real loss :  tensor(0.5228, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
220     1    NaN  ...  0.369092  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
221     1    NaN  ...  0.097120  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
222     1    NaN  ...  0.105502  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
223     1    NaN  ...  0.179185  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
224     1    NaN  ...  0.522780  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[225 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.198486328125, 0.81494140625]
[0.2054443359375, 0.787109375]
[0.264404296875, 0.70458984375]
[0.45751953125, 0.49609375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.198486328125, 0.81494140625]
[0.2054443359375, 0.787109375]
[0.264404296875, 0.70458984375]
[0.45751953125, 0.49609375]
This is the real loss :  tensor(0.0977, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
221     1    NaN  ...  0.097120  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
222     1    NaN  ...  0.105502  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
223     1    NaN  ...  0.179185  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
224     1    NaN  ...  0.522780  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
225     1    NaN  ...  0.097699  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[226 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.400634765625, 0.6337890625]
[0.4052734375, 0.59765625]
[0.04534912109375, 0.98291015625]
[0.482421875, 0.53173828125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.400634765625, 0.6337890625]
[0.4052734375, 0.59765625]
[0.04534912109375, 0.98291015625]
[0.482421875, 0.53173828125]
This is the real loss :  tensor(0.1825, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
222     1    NaN  ...  0.105502  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
223     1    NaN  ...  0.179185  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
224     1    NaN  ...  0.522780  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
225     1    NaN  ...  0.097699  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
226     1    NaN  ...  0.182482  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[227 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.2366943359375, 0.79052734375]
[0.5263671875, 0.432861328125]
[0.276123046875, 0.7099609375]
[0.1275634765625, 0.85009765625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.2366943359375, 0.79052734375]
[0.5263671875, 0.432861328125]
[0.276123046875, 0.7099609375]
[0.1275634765625, 0.85009765625]
This is the real loss :  tensor(0.1122, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
223     1    NaN  ...  0.179185  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
224     1    NaN  ...  0.522780  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
225     1    NaN  ...  0.097699  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
226     1    NaN  ...  0.182482  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
227     1    NaN  ...  0.112215  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[228 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.169189453125, 0.7998046875]
[0.43359375, 0.5283203125]
[0.471923828125, 0.517578125]
[0.10980224609375, 0.8671875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.169189453125, 0.7998046875]
[0.43359375, 0.5283203125]
[0.471923828125, 0.517578125]
[0.10980224609375, 0.8671875]
This is the real loss :  tensor(0.1442, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
224     1    NaN  ...  0.522780  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
225     1    NaN  ...  0.097699  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
226     1    NaN  ...  0.182482  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
227     1    NaN  ...  0.112215  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
228     1    NaN  ...  0.144223  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[229 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.1253662109375, 0.9208984375]
[0.5849609375, 0.52880859375]
[0.3310546875, 0.560546875]
[0.207275390625, 0.798828125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1253662109375, 0.9208984375]
[0.5849609375, 0.52880859375]
[0.3310546875, 0.560546875]
[0.207275390625, 0.798828125]
This is the real loss :  tensor(0.1075, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
225     1    NaN  ...  0.097699  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
226     1    NaN  ...  0.182482  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
227     1    NaN  ...  0.112215  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
228     1    NaN  ...  0.144223  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
229     1    NaN  ...  0.107502  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[230 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.053375244140625, 0.921875]
[0.36083984375, 0.6171875]
[0.40380859375, 0.61572265625]
[0.48681640625, 0.55224609375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.053375244140625, 0.921875]
[0.36083984375, 0.6171875]
[0.40380859375, 0.61572265625]
[0.48681640625, 0.55224609375]
This is the real loss :  tensor(0.2097, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
226     1    NaN  ...  0.182482  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
227     1    NaN  ...  0.112215  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
228     1    NaN  ...  0.144223  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
229     1    NaN  ...  0.107502  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
230     1    NaN  ...  0.209683  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[231 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.1727294921875, 0.85791015625]
[0.2205810546875, 0.77587890625]
[0.53857421875, 0.497802734375]
[0.35009765625, 0.6689453125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1727294921875, 0.85791015625]
[0.2205810546875, 0.77587890625]
[0.53857421875, 0.497802734375]
[0.35009765625, 0.6689453125]
This is the real loss :  tensor(0.1052, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
227     1    NaN  ...  0.112215  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
228     1    NaN  ...  0.144223  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
229     1    NaN  ...  0.107502  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
230     1    NaN  ...  0.209683  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
231     1    NaN  ...  0.105225  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[232 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.438720703125, 0.509765625]
[0.48583984375, 0.50439453125]
[0.0684814453125, 0.87353515625]
[0.2083740234375, 0.72021484375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.438720703125, 0.509765625]
[0.48583984375, 0.50439453125]
[0.0684814453125, 0.87353515625]
[0.2083740234375, 0.72021484375]
This is the real loss :  tensor(0.1367, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
228     1    NaN  ...  0.144223  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
229     1    NaN  ...  0.107502  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
230     1    NaN  ...  0.209683  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
231     1    NaN  ...  0.105225  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
232     1    NaN  ...  0.136745  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[233 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.266845703125, 0.7763671875]
[0.253662109375, 0.8017578125]
[0.1864013671875, 0.8564453125]
[0.49169921875, 0.431640625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.266845703125, 0.7763671875]
[0.253662109375, 0.8017578125]
[0.1864013671875, 0.8564453125]
[0.49169921875, 0.431640625]
This is the real loss :  tensor(0.1056, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
229     1    NaN  ...  0.107502  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
230     1    NaN  ...  0.209683  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
231     1    NaN  ...  0.105225  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
232     1    NaN  ...  0.136745  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
233     1    NaN  ...  0.105627  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[234 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.47705078125, 0.5068359375]
[0.046295166015625, 0.92138671875]
[0.138671875, 0.81689453125]
[0.54248046875, 0.49951171875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.47705078125, 0.5068359375]
[0.046295166015625, 0.92138671875]
[0.138671875, 0.81689453125]
[0.54248046875, 0.49951171875]
This is the real loss :  tensor(0.1420, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
230     1    NaN  ...  0.209683  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
231     1    NaN  ...  0.105225  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
232     1    NaN  ...  0.136745  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
233     1    NaN  ...  0.105627  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
234     1    NaN  ...  0.142027  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[235 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.1138916015625, 0.80078125]
[0.53173828125, 0.5009765625]
[0.4228515625, 0.494140625]
[0.06793212890625, 0.86181640625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1138916015625, 0.80078125]
[0.53173828125, 0.5009765625]
[0.4228515625, 0.494140625]
[0.06793212890625, 0.86181640625]
This is the real loss :  tensor(0.1304, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
231     1    NaN  ...  0.105225  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
232     1    NaN  ...  0.136745  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
233     1    NaN  ...  0.105627  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
234     1    NaN  ...  0.142027  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
235     1    NaN  ...  0.130354  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[236 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.332763671875, 0.67626953125]
[0.16748046875, 0.8623046875]
[0.54931640625, 0.41943359375]
[0.08624267578125, 0.9306640625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.332763671875, 0.67626953125]
[0.16748046875, 0.8623046875]
[0.54931640625, 0.41943359375]
[0.08624267578125, 0.9306640625]
This is the real loss :  tensor(0.2001, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
232     1    NaN  ...  0.136745  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
233     1    NaN  ...  0.105627  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
234     1    NaN  ...  0.142027  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
235     1    NaN  ...  0.130354  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
236     1    NaN  ...  0.200076  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[237 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.4189453125, 0.57275390625]
[0.37841796875, 0.72119140625]
[0.0418701171875, 0.98681640625]
[0.43017578125, 0.60107421875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.4189453125, 0.57275390625]
[0.37841796875, 0.72119140625]
[0.0418701171875, 0.98681640625]
[0.43017578125, 0.60107421875]
This is the real loss :  tensor(0.2398, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
233     1    NaN  ...  0.105627  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
234     1    NaN  ...  0.142027  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
235     1    NaN  ...  0.130354  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
236     1    NaN  ...  0.200076  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
237     1    NaN  ...  0.239784  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[238 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.55419921875, 0.57763671875]
[0.2427978515625, 0.64404296875]
[0.28857421875, 0.71044921875]
[0.06549072265625, 0.87060546875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.55419921875, 0.57763671875]
[0.2427978515625, 0.64404296875]
[0.28857421875, 0.71044921875]
[0.06549072265625, 0.87060546875]
This is the real loss :  tensor(0.2136, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
234     1    NaN  ...  0.142027  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
235     1    NaN  ...  0.130354  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
236     1    NaN  ...  0.200076  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
237     1    NaN  ...  0.239784  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
238     1    NaN  ...  0.213587  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[239 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.0693359375, 0.93701171875]
[0.41796875, 0.6123046875]
[0.1385498046875, 0.90234375]
[0.466552734375, 0.49658203125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.0693359375, 0.93701171875]
[0.41796875, 0.6123046875]
[0.1385498046875, 0.90234375]
[0.466552734375, 0.49658203125]
This is the real loss :  tensor(0.1042, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
235     1    NaN  ...  0.130354  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
236     1    NaN  ...  0.200076  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
237     1    NaN  ...  0.239784  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
238     1    NaN  ...  0.213587  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
239     1    NaN  ...  0.104202  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[240 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.1583251953125, 0.77685546875]
[0.1466064453125, 0.87939453125]
[0.397216796875, 0.615234375]
[0.394287109375, 0.671875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1583251953125, 0.77685546875]
[0.1466064453125, 0.87939453125]
[0.397216796875, 0.615234375]
[0.394287109375, 0.671875]
This is the real loss :  tensor(0.1544, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
236     1    NaN  ...  0.200076  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
237     1    NaN  ...  0.239784  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
238     1    NaN  ...  0.213587  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
239     1    NaN  ...  0.104202  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
240     1    NaN  ...  0.154379  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[241 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.18359375, 0.81640625]
[0.474365234375, 0.51953125]
[0.1556396484375, 0.76708984375]
[0.20703125, 0.869140625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.18359375, 0.81640625]
[0.474365234375, 0.51953125]
[0.1556396484375, 0.76708984375]
[0.20703125, 0.869140625]
This is the real loss :  tensor(0.0940, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
237     1    NaN  ...  0.239784  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
238     1    NaN  ...  0.213587  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
239     1    NaN  ...  0.104202  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
240     1    NaN  ...  0.154379  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
241     1    NaN  ...  0.094009  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[242 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.22314453125, 0.744140625]
[0.29150390625, 0.61669921875]
[0.271240234375, 0.70654296875]
[0.2330322265625, 0.7119140625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.22314453125, 0.744140625]
[0.29150390625, 0.61669921875]
[0.271240234375, 0.70654296875]
[0.2330322265625, 0.7119140625]
This is the real loss :  tensor(0.2706, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
238     1    NaN  ...  0.213587  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
239     1    NaN  ...  0.104202  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
240     1    NaN  ...  0.154379  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
241     1    NaN  ...  0.094009  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
242     1    NaN  ...  0.270642  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[243 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.0160675048828125, 0.89697265625]
[0.2332763671875, 0.6103515625]
[0.428955078125, 0.52392578125]
[0.2479248046875, 0.62646484375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.0160675048828125, 0.89697265625]
[0.2332763671875, 0.6103515625]
[0.428955078125, 0.52392578125]
[0.2479248046875, 0.62646484375]
This is the real loss :  tensor(0.1979, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
239     1    NaN  ...  0.104202  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
240     1    NaN  ...  0.154379  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
241     1    NaN  ...  0.094009  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
242     1    NaN  ...  0.270642  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
243     1    NaN  ...  0.197864  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[244 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.3447265625, 0.59716796875]
[0.51171875, 0.5283203125]
[0.32080078125, 0.64404296875]
[-0.06292724609375, 1.091796875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.3447265625, 0.59716796875]
[0.51171875, 0.5283203125]
[0.32080078125, 0.64404296875]
[-0.06292724609375, 1.091796875]
This is the real loss :  tensor(0.2699, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
240     1    NaN  ...  0.154379  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
241     1    NaN  ...  0.094009  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
242     1    NaN  ...  0.270642  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
243     1    NaN  ...  0.197864  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
244     1    NaN  ...  0.269853  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[245 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.38623046875, 0.63916015625]
[0.373779296875, 0.669921875]
[0.283447265625, 0.6103515625]
[0.162841796875, 0.9013671875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.38623046875, 0.63916015625]
[0.373779296875, 0.669921875]
[0.283447265625, 0.6103515625]
[0.162841796875, 0.9013671875]
This is the real loss :  tensor(0.0996, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
241     1    NaN  ...  0.094009  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
242     1    NaN  ...  0.270642  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
243     1    NaN  ...  0.197864  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
244     1    NaN  ...  0.269853  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
245     1    NaN  ...  0.099557  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[246 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.00910186767578125, 0.90673828125]
[0.51806640625, 0.443603515625]
[0.36181640625, 0.5576171875]
[0.0950927734375, 0.9091796875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.00910186767578125, 0.90673828125]
[0.51806640625, 0.443603515625]
[0.36181640625, 0.5576171875]
[0.0950927734375, 0.9091796875]
This is the real loss :  tensor(0.0977, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
242     1    NaN  ...  0.270642  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
243     1    NaN  ...  0.197864  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
244     1    NaN  ...  0.269853  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
245     1    NaN  ...  0.099557  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
246     1    NaN  ...  0.097716  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[247 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[-0.004566192626953125, 0.984375]
[0.3515625, 0.71044921875]
[0.5126953125, 0.49560546875]
[0.23486328125, 0.6982421875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.004566192626953125, 0.984375]
[0.3515625, 0.71044921875]
[0.5126953125, 0.49560546875]
[0.23486328125, 0.6982421875]
This is the real loss :  tensor(0.4720, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
243     1    NaN  ...  0.197864  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
244     1    NaN  ...  0.269853  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
245     1    NaN  ...  0.099557  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
246     1    NaN  ...  0.097716  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
247     1    NaN  ...  0.471979  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[248 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[-0.07696533203125, 1.150390625]
[0.361083984375, 0.6220703125]
[0.5048828125, 0.51953125]
[0.3369140625, 0.615234375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.07696533203125, 1.150390625]
[0.361083984375, 0.6220703125]
[0.5048828125, 0.51953125]
[0.3369140625, 0.615234375]
This is the real loss :  tensor(0.1311, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
244     1    NaN  ...  0.269853  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
245     1    NaN  ...  0.099557  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
246     1    NaN  ...  0.097716  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
247     1    NaN  ...  0.471979  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
248     1    NaN  ...  0.131133  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[249 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.460693359375, 0.4521484375]
[0.11663818359375, 0.88720703125]
[0.10015869140625, 0.90234375]
[0.32861328125, 0.66162109375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.460693359375, 0.4521484375]
[0.11663818359375, 0.88720703125]
[0.10015869140625, 0.90234375]
[0.32861328125, 0.66162109375]
This is the real loss :  tensor(0.0976, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
245     1    NaN  ...  0.099557  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
246     1    NaN  ...  0.097716  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
247     1    NaN  ...  0.471979  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
248     1    NaN  ...  0.131133  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
249     1    NaN  ...  0.097595  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[250 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.421142578125, 0.5302734375]
[0.09490966796875, 0.9287109375]
[0.473388671875, 0.5927734375]
[0.0755615234375, 0.927734375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.421142578125, 0.5302734375]
[0.09490966796875, 0.9287109375]
[0.473388671875, 0.5927734375]
[0.0755615234375, 0.927734375]
This is the real loss :  tensor(0.1587, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
246     1    NaN  ...  0.097716  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
247     1    NaN  ...  0.471979  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
248     1    NaN  ...  0.131133  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
249     1    NaN  ...  0.097595  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
250     1    NaN  ...  0.158748  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[251 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.42041015625, 0.58349609375]
[0.0494384765625, 1.0400390625]
[0.1668701171875, 0.8564453125]
[0.4560546875, 0.53125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.42041015625, 0.58349609375]
[0.0494384765625, 1.0400390625]
[0.1668701171875, 0.8564453125]
[0.4560546875, 0.53125]
This is the real loss :  tensor(0.1446, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
247     1    NaN  ...  0.471979  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
248     1    NaN  ...  0.131133  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
249     1    NaN  ...  0.097595  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
250     1    NaN  ...  0.158748  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
251     1    NaN  ...  0.144576  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[252 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.35595703125, 0.55419921875]
[0.083984375, 0.8837890625]
[0.481689453125, 0.556640625]
[0.09259033203125, 0.92431640625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.35595703125, 0.55419921875]
[0.083984375, 0.8837890625]
[0.481689453125, 0.556640625]
[0.09259033203125, 0.92431640625]
This is the real loss :  tensor(0.0986, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
248     1    NaN  ...  0.131133  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
249     1    NaN  ...  0.097595  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
250     1    NaN  ...  0.158748  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
251     1    NaN  ...  0.144576  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
252     1    NaN  ...  0.098612  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[253 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.359619140625, 0.7041015625]
[0.43896484375, 0.5751953125]
[-0.059722900390625, 1.0087890625]
[0.3720703125, 0.67724609375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.359619140625, 0.7041015625]
[0.43896484375, 0.5751953125]
[-0.059722900390625, 1.0087890625]
[0.3720703125, 0.67724609375]
This is the real loss :  tensor(0.1386, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
249     1    NaN  ...  0.097595  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
250     1    NaN  ...  0.158748  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
251     1    NaN  ...  0.144576  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
252     1    NaN  ...  0.098612  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
253     1    NaN  ...  0.138593  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[254 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.34228515625, 0.6484375]
[-0.04132080078125, 1.0078125]
[0.341064453125, 0.5751953125]
[0.402099609375, 0.57763671875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.34228515625, 0.6484375]
[-0.04132080078125, 1.0078125]
[0.341064453125, 0.5751953125]
[0.402099609375, 0.57763671875]
This is the real loss :  tensor(0.2450, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
250     1    NaN  ...  0.158748  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
251     1    NaN  ...  0.144576  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
252     1    NaN  ...  0.098612  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
253     1    NaN  ...  0.138593  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
254     1    NaN  ...  0.244994  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[255 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.37255859375, 0.59814453125]
[0.412109375, 0.6123046875]
[0.1566162109375, 0.859375]
[0.245361328125, 0.82080078125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.37255859375, 0.59814453125]
[0.412109375, 0.6123046875]
[0.1566162109375, 0.859375]
[0.245361328125, 0.82080078125]
This is the real loss :  tensor(0.1510, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
251     1    NaN  ...  0.144576  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
252     1    NaN  ...  0.098612  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
253     1    NaN  ...  0.138593  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
254     1    NaN  ...  0.244994  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
255     1    NaN  ...  0.151028  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[256 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.41455078125, 0.5263671875]
[0.052276611328125, 0.9150390625]
[0.37109375, 0.58203125]
[0.10101318359375, 0.865234375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.41455078125, 0.5263671875]
[0.052276611328125, 0.9150390625]
[0.37109375, 0.58203125]
[0.10101318359375, 0.865234375]
This is the real loss :  tensor(0.0934, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
252     1    NaN  ...  0.098612  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
253     1    NaN  ...  0.138593  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
254     1    NaN  ...  0.244994  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
255     1    NaN  ...  0.151028  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
256     1    NaN  ...  0.093363  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[257 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.443603515625, 0.55419921875]
[0.0782470703125, 0.9755859375]
[0.04931640625, 0.9140625]
[0.40869140625, 0.5283203125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.443603515625, 0.55419921875]
[0.0782470703125, 0.9755859375]
[0.04931640625, 0.9140625]
[0.40869140625, 0.5283203125]
This is the real loss :  tensor(0.1578, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
253     1    NaN  ...  0.138593  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
254     1    NaN  ...  0.244994  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
255     1    NaN  ...  0.151028  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
256     1    NaN  ...  0.093363  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
257     1    NaN  ...  0.157752  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[258 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.1175537109375, 0.81689453125]
[0.28955078125, 0.64599609375]
[0.216552734375, 0.79638671875]
[0.293701171875, 0.5556640625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1175537109375, 0.81689453125]
[0.28955078125, 0.64599609375]
[0.216552734375, 0.79638671875]
[0.293701171875, 0.5556640625]
This is the real loss :  tensor(0.0786, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
254     1    NaN  ...  0.244994  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
255     1    NaN  ...  0.151028  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
256     1    NaN  ...  0.093363  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
257     1    NaN  ...  0.157752  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
258     1    NaN  ...  0.078569  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[259 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.467529296875, 0.55517578125]
[0.09747314453125, 0.86376953125]
[0.0550537109375, 0.93115234375]
[0.315185546875, 0.5556640625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.467529296875, 0.55517578125]
[0.09747314453125, 0.86376953125]
[0.0550537109375, 0.93115234375]
[0.315185546875, 0.5556640625]
This is the real loss :  tensor(0.1538, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
255     1    NaN  ...  0.151028  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
256     1    NaN  ...  0.093363  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
257     1    NaN  ...  0.157752  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
258     1    NaN  ...  0.078569  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
259     1    NaN  ...  0.153752  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[260 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.111572265625, 0.80712890625]
[0.1497802734375, 0.779296875]
[0.395263671875, 0.52880859375]
[0.1436767578125, 0.763671875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.111572265625, 0.80712890625]
[0.1497802734375, 0.779296875]
[0.395263671875, 0.52880859375]
[0.1436767578125, 0.763671875]
This is the real loss :  tensor(0.1053, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
256     1    NaN  ...  0.093363  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
257     1    NaN  ...  0.157752  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
258     1    NaN  ...  0.078569  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
259     1    NaN  ...  0.153752  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
260     1    NaN  ...  0.105329  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[261 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.232421875, 0.80029296875]
[0.162841796875, 0.814453125]
[0.1689453125, 0.81640625]
[0.40966796875, 0.58251953125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.232421875, 0.80029296875]
[0.162841796875, 0.814453125]
[0.1689453125, 0.81640625]
[0.40966796875, 0.58251953125]
This is the real loss :  tensor(0.0699, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
257     1    NaN  ...  0.157752  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
258     1    NaN  ...  0.078569  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
259     1    NaN  ...  0.153752  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
260     1    NaN  ...  0.105329  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
261     1    NaN  ...  0.069902  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[262 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.465087890625, 0.576171875]
[0.296875, 0.646484375]
[0.279541015625, 0.712890625]
[-0.0733642578125, 1.0859375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.465087890625, 0.576171875]
[0.296875, 0.646484375]
[0.279541015625, 0.712890625]
[-0.0733642578125, 1.0859375]
This is the real loss :  tensor(0.2130, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
258     1    NaN  ...  0.078569  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
259     1    NaN  ...  0.153752  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
260     1    NaN  ...  0.105329  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
261     1    NaN  ...  0.069902  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
262     1    NaN  ...  0.212972  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[263 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.389892578125, 0.6416015625]
[-0.1715087890625, 1.130859375]
[0.419921875, 0.63525390625]
[0.396240234375, 0.6484375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.389892578125, 0.6416015625]
[-0.1715087890625, 1.130859375]
[0.419921875, 0.63525390625]
[0.396240234375, 0.6484375]
This is the real loss :  tensor(0.5031, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
259     1    NaN  ...  0.153752  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
260     1    NaN  ...  0.105329  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
261     1    NaN  ...  0.069902  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
262     1    NaN  ...  0.212972  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
263     1    NaN  ...  0.503142  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[264 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.134521484375, 0.83935546875]
[0.08709716796875, 0.865234375]
[0.093505859375, 0.91015625]
[0.52685546875, 0.51904296875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.134521484375, 0.83935546875]
[0.08709716796875, 0.865234375]
[0.093505859375, 0.91015625]
[0.52685546875, 0.51904296875]
This is the real loss :  tensor(0.0744, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
260     1    NaN  ...  0.105329  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
261     1    NaN  ...  0.069902  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
262     1    NaN  ...  0.212972  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
263     1    NaN  ...  0.503142  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
264     1    NaN  ...  0.074420  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[265 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.5322265625, 0.5029296875]
[0.2071533203125, 0.89697265625]
[0.068603515625, 0.85595703125]
[0.10076904296875, 0.85400390625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.5322265625, 0.5029296875]
[0.2071533203125, 0.89697265625]
[0.068603515625, 0.85595703125]
[0.10076904296875, 0.85400390625]
This is the real loss :  tensor(0.0801, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
261     1    NaN  ...  0.069902  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
262     1    NaN  ...  0.212972  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
263     1    NaN  ...  0.503142  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
264     1    NaN  ...  0.074420  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
265     1    NaN  ...  0.080099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[266 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.339599609375, 0.66064453125]
[0.1854248046875, 0.83056640625]
[0.3359375, 0.67626953125]
[0.2265625, 0.8056640625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.339599609375, 0.66064453125]
[0.1854248046875, 0.83056640625]
[0.3359375, 0.67626953125]
[0.2265625, 0.8056640625]
This is the real loss :  tensor(0.2404, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
262     1    NaN  ...  0.212972  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
263     1    NaN  ...  0.503142  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
264     1    NaN  ...  0.074420  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
265     1    NaN  ...  0.080099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
266     1    NaN  ...  0.240386  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[267 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.0355224609375, 1.009765625]
[0.050079345703125, 0.97314453125]
[0.386474609375, 0.640625]
[0.46142578125, 0.56103515625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.0355224609375, 1.009765625]
[0.050079345703125, 0.97314453125]
[0.386474609375, 0.640625]
[0.46142578125, 0.56103515625]
This is the real loss :  tensor(0.1496, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
263     1    NaN  ...  0.503142  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
264     1    NaN  ...  0.074420  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
265     1    NaN  ...  0.080099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
266     1    NaN  ...  0.240386  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
267     1    NaN  ...  0.149625  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[268 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.0677490234375, 0.9326171875]
[0.477294921875, 0.49755859375]
[0.2232666015625, 0.84619140625]
[0.1065673828125, 0.8994140625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.0677490234375, 0.9326171875]
[0.477294921875, 0.49755859375]
[0.2232666015625, 0.84619140625]
[0.1065673828125, 0.8994140625]
This is the real loss :  tensor(0.0730, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
264     1    NaN  ...  0.074420  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
265     1    NaN  ...  0.080099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
266     1    NaN  ...  0.240386  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
267     1    NaN  ...  0.149625  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
268     1    NaN  ...  0.073046  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[269 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[-0.006988525390625, 1.013671875]
[0.402587890625, 0.654296875]
[0.31884765625, 0.70556640625]
[0.281005859375, 0.802734375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.006988525390625, 1.013671875]
[0.402587890625, 0.654296875]
[0.31884765625, 0.70556640625]
[0.281005859375, 0.802734375]
This is the real loss :  tensor(0.2039, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
265     1    NaN  ...  0.080099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
266     1    NaN  ...  0.240386  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
267     1    NaN  ...  0.149625  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
268     1    NaN  ...  0.073046  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
269     1    NaN  ...  0.203939  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[270 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[-0.056640625, 1.1318359375]
[0.333984375, 0.7177734375]
[0.358642578125, 0.70361328125]
[0.408935546875, 0.6552734375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.056640625, 1.1318359375]
[0.333984375, 0.7177734375]
[0.358642578125, 0.70361328125]
[0.408935546875, 0.6552734375]
This is the real loss :  tensor(0.0893, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
266     1    NaN  ...  0.240386  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
267     1    NaN  ...  0.149625  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
268     1    NaN  ...  0.073046  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
269     1    NaN  ...  0.203939  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
270     1    NaN  ...  0.089290  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[271 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.37353515625, 0.66455078125]
[0.34814453125, 0.65283203125]
[0.3671875, 0.6630859375]
[-0.07366943359375, 1.0771484375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.37353515625, 0.66455078125]
[0.34814453125, 0.65283203125]
[0.3671875, 0.6630859375]
[-0.07366943359375, 1.0771484375]
This is the real loss :  tensor(0.2431, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
267     1    NaN  ...  0.149625  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
268     1    NaN  ...  0.073046  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
269     1    NaN  ...  0.203939  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
270     1    NaN  ...  0.089290  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
271     1    NaN  ...  0.243113  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[272 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.394287109375, 0.60400390625]
[0.049774169921875, 0.92919921875]
[0.01329803466796875, 0.958984375]
[0.40185546875, 0.59912109375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.394287109375, 0.60400390625]
[0.049774169921875, 0.92919921875]
[0.01329803466796875, 0.958984375]
[0.40185546875, 0.59912109375]
This is the real loss :  tensor(0.0805, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
268     1    NaN  ...  0.073046  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
269     1    NaN  ...  0.203939  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
270     1    NaN  ...  0.089290  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
271     1    NaN  ...  0.243113  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
272     1    NaN  ...  0.080477  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[273 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.10980224609375, 0.87451171875]
[0.453857421875, 0.564453125]
[0.092529296875, 0.9560546875]
[0.1168212890625, 0.81396484375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.10980224609375, 0.87451171875]
[0.453857421875, 0.564453125]
[0.092529296875, 0.9560546875]
[0.1168212890625, 0.81396484375]
This is the real loss :  tensor(0.0603, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
269     1    NaN  ...  0.203939  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
270     1    NaN  ...  0.089290  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
271     1    NaN  ...  0.243113  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
272     1    NaN  ...  0.080477  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
273     1    NaN  ...  0.060280  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[274 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.119384765625, 0.87841796875]
[0.3369140625, 0.55712890625]
[0.127685546875, 0.8173828125]
[0.11126708984375, 0.8720703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.119384765625, 0.87841796875]
[0.3369140625, 0.55712890625]
[0.127685546875, 0.8173828125]
[0.11126708984375, 0.8720703125]
This is the real loss :  tensor(0.0521, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
270     1    NaN  ...  0.089290  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
271     1    NaN  ...  0.243113  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
272     1    NaN  ...  0.080477  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
273     1    NaN  ...  0.060280  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
274     1    NaN  ...  0.052135  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[275 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.03875732421875, 1.0439453125]
[0.371337890625, 0.6181640625]
[0.330322265625, 0.623046875]
[0.05572509765625, 0.93408203125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.03875732421875, 1.0439453125]
[0.371337890625, 0.6181640625]
[0.330322265625, 0.623046875]
[0.05572509765625, 0.93408203125]
This is the real loss :  tensor(0.1414, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
271     1    NaN  ...  0.243113  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
272     1    NaN  ...  0.080477  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
273     1    NaN  ...  0.060280  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
274     1    NaN  ...  0.052135  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
275     1    NaN  ...  0.141404  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[276 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.396484375, 0.59716796875]
[0.060333251953125, 0.8369140625]
[0.08740234375, 0.96484375]
[0.134765625, 0.87841796875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.396484375, 0.59716796875]
[0.060333251953125, 0.8369140625]
[0.08740234375, 0.96484375]
[0.134765625, 0.87841796875]
This is the real loss :  tensor(0.0991, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
272     1    NaN  ...  0.080477  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
273     1    NaN  ...  0.060280  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
274     1    NaN  ...  0.052135  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
275     1    NaN  ...  0.141404  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
276     1    NaN  ...  0.099112  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[277 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.06903076171875, 0.861328125]
[0.105224609375, 0.876953125]
[0.105712890625, 0.93798828125]
[0.381103515625, 0.60498046875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.06903076171875, 0.861328125]
[0.105224609375, 0.876953125]
[0.105712890625, 0.93798828125]
[0.381103515625, 0.60498046875]
This is the real loss :  tensor(0.2387, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
273     1    NaN  ...  0.060280  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
274     1    NaN  ...  0.052135  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
275     1    NaN  ...  0.141404  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
276     1    NaN  ...  0.099112  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
277     1    NaN  ...  0.238746  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[278 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.25927734375, 0.66748046875]
[0.1798095703125, 0.81005859375]
[0.2347412109375, 0.91455078125]
[0.258544921875, 0.73046875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.25927734375, 0.66748046875]
[0.1798095703125, 0.81005859375]
[0.2347412109375, 0.91455078125]
[0.258544921875, 0.73046875]
This is the real loss :  tensor(0.1740, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
274     1    NaN  ...  0.052135  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
275     1    NaN  ...  0.141404  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
276     1    NaN  ...  0.099112  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
277     1    NaN  ...  0.238746  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
278     1    NaN  ...  0.173994  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[279 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.0982666015625, 0.9091796875]
[0.328857421875, 0.63134765625]
[0.07049560546875, 0.951171875]
[0.1412353515625, 0.84423828125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.0982666015625, 0.9091796875]
[0.328857421875, 0.63134765625]
[0.07049560546875, 0.951171875]
[0.1412353515625, 0.84423828125]
This is the real loss :  tensor(0.0392, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
275     1    NaN  ...  0.141404  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
276     1    NaN  ...  0.099112  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
277     1    NaN  ...  0.238746  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
278     1    NaN  ...  0.173994  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
279     1    NaN  ...  0.039190  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[280 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.2447509765625, 0.71337890625]
[0.11956787109375, 0.84814453125]
[0.259033203125, 0.6953125]
[0.277587890625, 0.83935546875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.2447509765625, 0.71337890625]
[0.11956787109375, 0.84814453125]
[0.259033203125, 0.6953125]
[0.277587890625, 0.83935546875]
This is the real loss :  tensor(0.0553, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
276     1    NaN  ...  0.099112  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
277     1    NaN  ...  0.238746  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
278     1    NaN  ...  0.173994  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
279     1    NaN  ...  0.039190  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
280     1    NaN  ...  0.055276  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[281 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.26513671875, 0.6767578125]
[0.12493896484375, 0.873046875]
[0.171630859375, 0.80126953125]
[0.1309814453125, 0.8818359375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.26513671875, 0.6767578125]
[0.12493896484375, 0.873046875]
[0.171630859375, 0.80126953125]
[0.1309814453125, 0.8818359375]
This is the real loss :  tensor(0.0383, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
277     1    NaN  ...  0.238746  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
278     1    NaN  ...  0.173994  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
279     1    NaN  ...  0.039190  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
280     1    NaN  ...  0.055276  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
281     1    NaN  ...  0.038322  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[282 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.283203125, 0.732421875]
[0.28466796875, 0.6923828125]
[0.302001953125, 0.728515625]
[-0.080078125, 1.0654296875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.283203125, 0.732421875]
[0.28466796875, 0.6923828125]
[0.302001953125, 0.728515625]
[-0.080078125, 1.0654296875]
This is the real loss :  tensor(0.3837, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
278     1    NaN  ...  0.173994  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
279     1    NaN  ...  0.039190  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
280     1    NaN  ...  0.055276  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
281     1    NaN  ...  0.038322  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
282     1    NaN  ...  0.383745  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[283 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.11572265625, 0.8388671875]
[0.0845947265625, 0.923828125]
[0.07958984375, 0.9873046875]
[0.3134765625, 0.66162109375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.11572265625, 0.8388671875]
[0.0845947265625, 0.923828125]
[0.07958984375, 0.9873046875]
[0.3134765625, 0.66162109375]
This is the real loss :  tensor(0.0339, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
279     1    NaN  ...  0.039190  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
280     1    NaN  ...  0.055276  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
281     1    NaN  ...  0.038322  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
282     1    NaN  ...  0.383745  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
283     1    NaN  ...  0.033947  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[284 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.29345703125, 0.71826171875]
[0.318359375, 0.697265625]
[0.26513671875, 0.771484375]
[-0.1102294921875, 1.0458984375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.29345703125, 0.71826171875]
[0.318359375, 0.697265625]
[0.26513671875, 0.771484375]
[-0.1102294921875, 1.0458984375]
This is the real loss :  tensor(0.1566, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
280     1    NaN  ...  0.055276  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
281     1    NaN  ...  0.038322  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
282     1    NaN  ...  0.383745  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
283     1    NaN  ...  0.033947  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
284     1    NaN  ...  0.156635  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[285 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.07928466796875, 0.81201171875]
[0.04669189453125, 0.83251953125]
[0.074951171875, 0.8115234375]
[0.3115234375, 0.68359375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.07928466796875, 0.81201171875]
[0.04669189453125, 0.83251953125]
[0.074951171875, 0.8115234375]
[0.3115234375, 0.68359375]
This is the real loss :  tensor(0.1318, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
281     1    NaN  ...  0.038322  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
282     1    NaN  ...  0.383745  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
283     1    NaN  ...  0.033947  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
284     1    NaN  ...  0.156635  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
285     1    NaN  ...  0.131787  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[286 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.308837890625, 0.68896484375]
[-0.1605224609375, 1.0791015625]
[0.291015625, 0.72509765625]
[0.302490234375, 0.7548828125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.308837890625, 0.68896484375]
[-0.1605224609375, 1.0791015625]
[0.291015625, 0.72509765625]
[0.302490234375, 0.7548828125]
This is the real loss :  tensor(0.3836, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
282     1    NaN  ...  0.383745  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
283     1    NaN  ...  0.033947  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
284     1    NaN  ...  0.156635  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
285     1    NaN  ...  0.131787  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
286     1    NaN  ...  0.383649  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[287 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.3046875, 0.72412109375]
[0.32470703125, 0.72314453125]
[0.300048828125, 0.7099609375]
[-0.1068115234375, 1.0947265625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.3046875, 0.72412109375]
[0.32470703125, 0.72314453125]
[0.300048828125, 0.7099609375]
[-0.1068115234375, 1.0947265625]
This is the real loss :  tensor(0.1731, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
283     1    NaN  ...  0.033947  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
284     1    NaN  ...  0.156635  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
285     1    NaN  ...  0.131787  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
286     1    NaN  ...  0.383649  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
287     1    NaN  ...  0.173054  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[288 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[-0.08673095703125, 1.0537109375]
[0.292236328125, 0.69482421875]
[0.340576171875, 0.7041015625]
[0.324462890625, 0.72607421875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.08673095703125, 1.0537109375]
[0.292236328125, 0.69482421875]
[0.340576171875, 0.7041015625]
[0.324462890625, 0.72607421875]
This is the real loss :  tensor(0.1625, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
284     1    NaN  ...  0.156635  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
285     1    NaN  ...  0.131787  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
286     1    NaN  ...  0.383649  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
287     1    NaN  ...  0.173054  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
288     1    NaN  ...  0.162481  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[289 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.3369140625, 0.63720703125]
[0.3818359375, 0.6826171875]
[0.032196044921875, 0.91845703125]
[-0.034820556640625, 0.9765625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.3369140625, 0.63720703125]
[0.3818359375, 0.6826171875]
[0.032196044921875, 0.91845703125]
[-0.034820556640625, 0.9765625]
This is the real loss :  tensor(0.0626, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
285     1    NaN  ...  0.131787  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
286     1    NaN  ...  0.383649  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
287     1    NaN  ...  0.173054  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
288     1    NaN  ...  0.162481  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
289     1    NaN  ...  0.062638  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[290 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.32470703125, 0.6533203125]
[-0.01050567626953125, 1.0166015625]
[0.385009765625, 0.67333984375]
[0.03662109375, 0.95166015625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.32470703125, 0.6533203125]
[-0.01050567626953125, 1.0166015625]
[0.385009765625, 0.67333984375]
[0.03662109375, 0.95166015625]
This is the real loss :  tensor(0.1327, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
286     1    NaN  ...  0.383649  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
287     1    NaN  ...  0.173054  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
288     1    NaN  ...  0.162481  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
289     1    NaN  ...  0.062638  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
290     1    NaN  ...  0.132661  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[291 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.1282958984375, 0.87109375]
[0.331298828125, 0.65087890625]
[0.150390625, 0.85791015625]
[0.08880615234375, 0.986328125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1282958984375, 0.87109375]
[0.331298828125, 0.65087890625]
[0.150390625, 0.85791015625]
[0.08880615234375, 0.986328125]
This is the real loss :  tensor(0.2251, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
287     1    NaN  ...  0.173054  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
288     1    NaN  ...  0.162481  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
289     1    NaN  ...  0.062638  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
290     1    NaN  ...  0.132661  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
291     1    NaN  ...  0.225150  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[292 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.041534423828125, 0.92138671875]
[0.11419677734375, 0.9111328125]
[0.416015625, 0.64208984375]
[0.2005615234375, 0.76708984375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.041534423828125, 0.92138671875]
[0.11419677734375, 0.9111328125]
[0.416015625, 0.64208984375]
[0.2005615234375, 0.76708984375]
This is the real loss :  tensor(0.0531, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
288     1    NaN  ...  0.162481  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
289     1    NaN  ...  0.062638  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
290     1    NaN  ...  0.132661  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
291     1    NaN  ...  0.225150  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
292     1    NaN  ...  0.053061  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[293 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.310791015625, 0.7578125]
[0.345458984375, 0.66748046875]
[0.337158203125, 0.65576171875]
[-0.057159423828125, 1.1357421875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.310791015625, 0.7578125]
[0.345458984375, 0.66748046875]
[0.337158203125, 0.65576171875]
[-0.057159423828125, 1.1357421875]
This is the real loss :  tensor(0.1595, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
289     1    NaN  ...  0.062638  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
290     1    NaN  ...  0.132661  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
291     1    NaN  ...  0.225150  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
292     1    NaN  ...  0.053061  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
293     1    NaN  ...  0.159529  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[294 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.384765625, 0.6611328125]
[0.030548095703125, 0.966796875]
[0.348876953125, 0.6259765625]
[0.055206298828125, 0.9833984375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.384765625, 0.6611328125]
[0.030548095703125, 0.966796875]
[0.348876953125, 0.6259765625]
[0.055206298828125, 0.9833984375]
This is the real loss :  tensor(0.3696, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
290     1    NaN  ...  0.132661  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
291     1    NaN  ...  0.225150  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
292     1    NaN  ...  0.053061  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
293     1    NaN  ...  0.159529  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
294     1    NaN  ...  0.369568  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[295 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.382080078125, 0.6865234375]
[0.321533203125, 0.6083984375]
[0.07171630859375, 0.95703125]
[0.035736083984375, 0.9931640625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.382080078125, 0.6865234375]
[0.321533203125, 0.6083984375]
[0.07171630859375, 0.95703125]
[0.035736083984375, 0.9931640625]
This is the real loss :  tensor(0.1354, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
291     1    NaN  ...  0.225150  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
292     1    NaN  ...  0.053061  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
293     1    NaN  ...  0.159529  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
294     1    NaN  ...  0.369568  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
295     1    NaN  ...  0.135379  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[296 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.37939453125, 0.62158203125]
[0.1131591796875, 0.9345703125]
[0.0134124755859375, 1.052734375]
[0.330810546875, 0.58154296875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.37939453125, 0.62158203125]
[0.1131591796875, 0.9345703125]
[0.0134124755859375, 1.052734375]
[0.330810546875, 0.58154296875]
This is the real loss :  tensor(0.1366, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
292     1    NaN  ...  0.053061  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
293     1    NaN  ...  0.159529  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
294     1    NaN  ...  0.369568  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
295     1    NaN  ...  0.135379  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
296     1    NaN  ...  0.136649  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[297 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[-0.004852294921875, 1.041015625]
[0.296630859375, 0.70849609375]
[0.369384765625, 0.66357421875]
[0.32373046875, 0.64794921875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.004852294921875, 1.041015625]
[0.296630859375, 0.70849609375]
[0.369384765625, 0.66357421875]
[0.32373046875, 0.64794921875]
This is the real loss :  tensor(0.2362, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
293     1    NaN  ...  0.159529  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
294     1    NaN  ...  0.369568  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
295     1    NaN  ...  0.135379  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
296     1    NaN  ...  0.136649  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
297     1    NaN  ...  0.236232  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[298 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.0552978515625, 0.90478515625]
[0.384765625, 0.63232421875]
[0.324462890625, 0.59716796875]
[0.058563232421875, 0.92138671875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.0552978515625, 0.90478515625]
[0.384765625, 0.63232421875]
[0.324462890625, 0.59716796875]
[0.058563232421875, 0.92138671875]
This is the real loss :  tensor(0.1397, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
294     1    NaN  ...  0.369568  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
295     1    NaN  ...  0.135379  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
296     1    NaN  ...  0.136649  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
297     1    NaN  ...  0.236232  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
298     1    NaN  ...  0.139740  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[299 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.1090087890625, 0.9453125]
[0.13134765625, 0.91357421875]
[0.2467041015625, 0.76123046875]
[0.348876953125, 0.5478515625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1090087890625, 0.9453125]
[0.13134765625, 0.91357421875]
[0.2467041015625, 0.76123046875]
[0.348876953125, 0.5478515625]
This is the real loss :  tensor(0.2388, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
295     1    NaN  ...  0.135379  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
296     1    NaN  ...  0.136649  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
297     1    NaN  ...  0.236232  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
298     1    NaN  ...  0.139740  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
299     1    NaN  ...  0.238828  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[300 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.0031871795654296875, 1.0341796875]
[0.406982421875, 0.59326171875]
[0.371337890625, 0.6494140625]
[0.18701171875, 0.791015625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.0031871795654296875, 1.0341796875]
[0.406982421875, 0.59326171875]
[0.371337890625, 0.6494140625]
[0.18701171875, 0.791015625]
This is the real loss :  tensor(0.1305, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
296     1    NaN  ...  0.136649  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
297     1    NaN  ...  0.236232  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
298     1    NaN  ...  0.139740  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
299     1    NaN  ...  0.238828  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
300     1    NaN  ...  0.130532  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[301 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.032470703125, 0.978515625]
[0.3955078125, 0.5732421875]
[0.34130859375, 0.568359375]
[0.11309814453125, 0.90185546875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.032470703125, 0.978515625]
[0.3955078125, 0.5732421875]
[0.34130859375, 0.568359375]
[0.11309814453125, 0.90185546875]
This is the real loss :  tensor(0.1399, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
297     1    NaN  ...  0.236232  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
298     1    NaN  ...  0.139740  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
299     1    NaN  ...  0.238828  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
300     1    NaN  ...  0.130532  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
301     1    NaN  ...  0.139924  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[302 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.414794921875, 0.5654296875]
[0.134765625, 0.8984375]
[0.2080078125, 0.82421875]
[0.1580810546875, 0.8603515625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.414794921875, 0.5654296875]
[0.134765625, 0.8984375]
[0.2080078125, 0.82421875]
[0.1580810546875, 0.8603515625]
This is the real loss :  tensor(0.0635, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
298     1    NaN  ...  0.139740  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
299     1    NaN  ...  0.238828  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
300     1    NaN  ...  0.130532  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
301     1    NaN  ...  0.139924  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
302     1    NaN  ...  0.063505  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[303 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.3603515625, 0.650390625]
[-0.0081024169921875, 0.9599609375]
[0.28369140625, 0.67578125]
[0.4521484375, 0.62744140625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.3603515625, 0.650390625]
[-0.0081024169921875, 0.9599609375]
[0.28369140625, 0.67578125]
[0.4521484375, 0.62744140625]
This is the real loss :  tensor(0.1416, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
299     1    NaN  ...  0.238828  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
300     1    NaN  ...  0.130532  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
301     1    NaN  ...  0.139924  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
302     1    NaN  ...  0.063505  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
303     1    NaN  ...  0.141646  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[304 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[-0.023101806640625, 1.013671875]
[0.32080078125, 0.654296875]
[0.44482421875, 0.58154296875]
[0.32080078125, 0.65771484375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.023101806640625, 1.013671875]
[0.32080078125, 0.654296875]
[0.44482421875, 0.58154296875]
[0.32080078125, 0.65771484375]
This is the real loss :  tensor(0.1362, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
300     1    NaN  ...  0.130532  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
301     1    NaN  ...  0.139924  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
302     1    NaN  ...  0.063505  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
303     1    NaN  ...  0.141646  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
304     1    NaN  ...  0.136204  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[305 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.4560546875, 0.5859375]
[0.10107421875, 0.9248046875]
[0.326171875, 0.5810546875]
[0.09344482421875, 0.919921875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.4560546875, 0.5859375]
[0.10107421875, 0.9248046875]
[0.326171875, 0.5810546875]
[0.09344482421875, 0.919921875]
This is the real loss :  tensor(0.0865, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
301     1    NaN  ...  0.139924  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
302     1    NaN  ...  0.063505  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
303     1    NaN  ...  0.141646  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
304     1    NaN  ...  0.136204  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
305     1    NaN  ...  0.086544  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[306 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.396728515625, 0.62109375]
[0.43115234375, 0.496337890625]
[0.057159423828125, 0.99658203125]
[0.1004638671875, 0.9296875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.396728515625, 0.62109375]
[0.43115234375, 0.496337890625]
[0.057159423828125, 0.99658203125]
[0.1004638671875, 0.9296875]
This is the real loss :  tensor(0.1672, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
302     1    NaN  ...  0.063505  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
303     1    NaN  ...  0.141646  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
304     1    NaN  ...  0.136204  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
305     1    NaN  ...  0.086544  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
306     1    NaN  ...  0.167244  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[307 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.466796875, 0.5693359375]
[0.05279541015625, 0.935546875]
[0.458984375, 0.57080078125]
[0.06573486328125, 0.9384765625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.466796875, 0.5693359375]
[0.05279541015625, 0.935546875]
[0.458984375, 0.57080078125]
[0.06573486328125, 0.9384765625]
This is the real loss :  tensor(0.1017, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
303     1    NaN  ...  0.141646  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
304     1    NaN  ...  0.136204  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
305     1    NaN  ...  0.086544  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
306     1    NaN  ...  0.167244  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
307     1    NaN  ...  0.101662  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[308 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.0679931640625, 0.89404296875]
[0.377685546875, 0.6171875]
[0.4951171875, 0.5927734375]
[0.0521240234375, 0.91845703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.0679931640625, 0.89404296875]
[0.377685546875, 0.6171875]
[0.4951171875, 0.5927734375]
[0.0521240234375, 0.91845703125]
This is the real loss :  tensor(0.0907, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
304     1    NaN  ...  0.136204  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
305     1    NaN  ...  0.086544  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
306     1    NaN  ...  0.167244  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
307     1    NaN  ...  0.101662  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
308     1    NaN  ...  0.090673  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[309 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.498046875, 0.5341796875]
[0.10748291015625, 0.8427734375]
[0.121826171875, 0.81884765625]
[0.1456298828125, 0.8349609375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.498046875, 0.5341796875]
[0.10748291015625, 0.8427734375]
[0.121826171875, 0.81884765625]
[0.1456298828125, 0.8349609375]
This is the real loss :  tensor(0.0837, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
305     1    NaN  ...  0.086544  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
306     1    NaN  ...  0.167244  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
307     1    NaN  ...  0.101662  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
308     1    NaN  ...  0.090673  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
309     1    NaN  ...  0.083710  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[310 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.25244140625, 0.71533203125]
[0.2998046875, 0.66845703125]
[0.2626953125, 0.73046875]
[0.11138916015625, 0.89111328125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.25244140625, 0.71533203125]
[0.2998046875, 0.66845703125]
[0.2626953125, 0.73046875]
[0.11138916015625, 0.89111328125]
This is the real loss :  tensor(0.0638, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
306     1    NaN  ...  0.167244  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
307     1    NaN  ...  0.101662  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
308     1    NaN  ...  0.090673  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
309     1    NaN  ...  0.083710  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
310     1    NaN  ...  0.063811  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[311 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.2548828125, 0.6279296875]
[0.483642578125, 0.5927734375]
[0.32568359375, 0.583984375]
[-0.05401611328125, 0.99755859375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.2548828125, 0.6279296875]
[0.483642578125, 0.5927734375]
[0.32568359375, 0.583984375]
[-0.05401611328125, 0.99755859375]
This is the real loss :  tensor(0.2025, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
307     1    NaN  ...  0.101662  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
308     1    NaN  ...  0.090673  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
309     1    NaN  ...  0.083710  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
310     1    NaN  ...  0.063811  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
311     1    NaN  ...  0.202509  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[312 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.48876953125, 0.5869140625]
[0.396240234375, 0.6533203125]
[-0.01273345947265625, 1.005859375]
[0.12371826171875, 0.8505859375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.48876953125, 0.5869140625]
[0.396240234375, 0.6533203125]
[-0.01273345947265625, 1.005859375]
[0.12371826171875, 0.8505859375]
This is the real loss :  tensor(0.1151, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
308     1    NaN  ...  0.090673  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
309     1    NaN  ...  0.083710  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
310     1    NaN  ...  0.063811  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
311     1    NaN  ...  0.202509  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
312     1    NaN  ...  0.115106  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[313 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.5458984375, 0.54296875]
[0.041351318359375, 0.951171875]
[0.031280517578125, 0.93017578125]
[0.2724609375, 0.6572265625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.5458984375, 0.54296875]
[0.041351318359375, 0.951171875]
[0.031280517578125, 0.93017578125]
[0.2724609375, 0.6572265625]
This is the real loss :  tensor(0.0878, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
309     1    NaN  ...  0.083710  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
310     1    NaN  ...  0.063811  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
311     1    NaN  ...  0.202509  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
312     1    NaN  ...  0.115106  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
313     1    NaN  ...  0.087837  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[314 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[-0.059356689453125, 1.0068359375]
[0.454345703125, 0.56640625]
[0.4462890625, 0.5966796875]
[0.10400390625, 0.85693359375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.059356689453125, 1.0068359375]
[0.454345703125, 0.56640625]
[0.4462890625, 0.5966796875]
[0.10400390625, 0.85693359375]
This is the real loss :  tensor(0.1365, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
310     1    NaN  ...  0.063811  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
311     1    NaN  ...  0.202509  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
312     1    NaN  ...  0.115106  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
313     1    NaN  ...  0.087837  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
314     1    NaN  ...  0.136489  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[315 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.08868408203125, 0.9375]
[0.0931396484375, 0.8984375]
[0.0943603515625, 0.87255859375]
[0.5146484375, 0.4677734375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.08868408203125, 0.9375]
[0.0931396484375, 0.8984375]
[0.0943603515625, 0.87255859375]
[0.5146484375, 0.4677734375]
This is the real loss :  tensor(0.0755, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
311     1    NaN  ...  0.202509  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
312     1    NaN  ...  0.115106  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
313     1    NaN  ...  0.087837  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
314     1    NaN  ...  0.136489  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
315     1    NaN  ...  0.075504  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[316 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.45703125, 0.61767578125]
[0.43994140625, 0.5966796875]
[0.2314453125, 0.76123046875]
[-0.1048583984375, 1.1083984375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.45703125, 0.61767578125]
[0.43994140625, 0.5966796875]
[0.2314453125, 0.76123046875]
[-0.1048583984375, 1.1083984375]
This is the real loss :  tensor(0.1849, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
312     1    NaN  ...  0.115106  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
313     1    NaN  ...  0.087837  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
314     1    NaN  ...  0.136489  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
315     1    NaN  ...  0.075504  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
316     1    NaN  ...  0.184919  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[317 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.406005859375, 0.55224609375]
[0.275634765625, 0.7353515625]
[-0.1591796875, 1.125]
[0.449951171875, 0.5556640625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.406005859375, 0.55224609375]
[0.275634765625, 0.7353515625]
[-0.1591796875, 1.125]
[0.449951171875, 0.5556640625]
This is the real loss :  tensor(0.1455, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
313     1    NaN  ...  0.087837  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
314     1    NaN  ...  0.136489  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
315     1    NaN  ...  0.075504  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
316     1    NaN  ...  0.184919  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
317     1    NaN  ...  0.145452  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[318 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.1942138671875, 0.7998046875]
[0.20166015625, 0.77197265625]
[0.09600830078125, 0.953125]
[0.236328125, 0.66455078125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1942138671875, 0.7998046875]
[0.20166015625, 0.77197265625]
[0.09600830078125, 0.953125]
[0.236328125, 0.66455078125]
This is the real loss :  tensor(0.0438, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
314     1    NaN  ...  0.136489  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
315     1    NaN  ...  0.075504  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
316     1    NaN  ...  0.184919  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
317     1    NaN  ...  0.145452  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
318     1    NaN  ...  0.043782  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[319 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.006526947021484375, 0.98681640625]
[-0.01139068603515625, 0.99267578125]
[0.5380859375, 0.52001953125]
[0.342041015625, 0.650390625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.006526947021484375, 0.98681640625]
[-0.01139068603515625, 0.99267578125]
[0.5380859375, 0.52001953125]
[0.342041015625, 0.650390625]
This is the real loss :  tensor(0.0949, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
315     1    NaN  ...  0.075504  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
316     1    NaN  ...  0.184919  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
317     1    NaN  ...  0.145452  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
318     1    NaN  ...  0.043782  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
319     1    NaN  ...  0.094942  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[320 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.32080078125, 0.6787109375]
[0.0823974609375, 0.95654296875]
[0.410888671875, 0.59716796875]
[0.133056640625, 0.916015625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.32080078125, 0.6787109375]
[0.0823974609375, 0.95654296875]
[0.410888671875, 0.59716796875]
[0.133056640625, 0.916015625]
This is the real loss :  tensor(0.4031, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
316     1    NaN  ...  0.184919  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
317     1    NaN  ...  0.145452  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
318     1    NaN  ...  0.043782  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
319     1    NaN  ...  0.094942  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
320     1    NaN  ...  0.403122  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[321 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[-0.1832275390625, 1.15234375]
[0.330078125, 0.67236328125]
[0.434814453125, 0.63037109375]
[0.43115234375, 0.6044921875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.1832275390625, 1.15234375]
[0.330078125, 0.67236328125]
[0.434814453125, 0.63037109375]
[0.43115234375, 0.6044921875]
This is the real loss :  tensor(0.2465, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
317     1    NaN  ...  0.145452  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
318     1    NaN  ...  0.043782  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
319     1    NaN  ...  0.094942  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
320     1    NaN  ...  0.403122  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
321     1    NaN  ...  0.246542  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[322 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.3427734375, 0.7099609375]
[0.3505859375, 0.64501953125]
[-0.0182342529296875, 1.033203125]
[0.341796875, 0.67626953125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.3427734375, 0.7099609375]
[0.3505859375, 0.64501953125]
[-0.0182342529296875, 1.033203125]
[0.341796875, 0.67626953125]
This is the real loss :  tensor(0.4307, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
318     1    NaN  ...  0.043782  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
319     1    NaN  ...  0.094942  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
320     1    NaN  ...  0.403122  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
321     1    NaN  ...  0.246542  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
322     1    NaN  ...  0.430677  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[323 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.314208984375, 0.7216796875]
[0.293212890625, 0.76953125]
[0.472900390625, 0.55615234375]
[-0.0980224609375, 1.0908203125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.314208984375, 0.7216796875]
[0.293212890625, 0.76953125]
[0.472900390625, 0.55615234375]
[-0.0980224609375, 1.0908203125]
This is the real loss :  tensor(0.0942, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
319     1    NaN  ...  0.094942  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
320     1    NaN  ...  0.403122  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
321     1    NaN  ...  0.246542  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
322     1    NaN  ...  0.430677  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
323     1    NaN  ...  0.094221  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[324 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.024658203125, 0.9697265625]
[0.533203125, 0.48828125]
[0.34228515625, 0.67724609375]
[0.0027618408203125, 0.97412109375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.024658203125, 0.9697265625]
[0.533203125, 0.48828125]
[0.34228515625, 0.67724609375]
[0.0027618408203125, 0.97412109375]
This is the real loss :  tensor(0.0850, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
320     1    NaN  ...  0.403122  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
321     1    NaN  ...  0.246542  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
322     1    NaN  ...  0.430677  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
323     1    NaN  ...  0.094221  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
324     1    NaN  ...  0.084981  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[325 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.2183837890625, 0.787109375]
[0.349609375, 0.62451171875]
[0.30419921875, 0.62744140625]
[-0.037933349609375, 0.9697265625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.2183837890625, 0.787109375]
[0.349609375, 0.62451171875]
[0.30419921875, 0.62744140625]
[-0.037933349609375, 0.9697265625]
This is the real loss :  tensor(0.1425, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
321     1    NaN  ...  0.246542  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
322     1    NaN  ...  0.430677  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
323     1    NaN  ...  0.094221  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
324     1    NaN  ...  0.084981  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
325     1    NaN  ...  0.142466  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[326 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[-0.06640625, 1.001953125]
[0.462646484375, 0.60546875]
[0.25537109375, 0.7099609375]
[0.359130859375, 0.5947265625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.06640625, 1.001953125]
[0.462646484375, 0.60546875]
[0.25537109375, 0.7099609375]
[0.359130859375, 0.5947265625]
This is the real loss :  tensor(0.1967, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
322     1    NaN  ...  0.430677  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
323     1    NaN  ...  0.094221  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
324     1    NaN  ...  0.084981  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
325     1    NaN  ...  0.142466  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
326     1    NaN  ...  0.196688  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[327 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.33740234375, 0.61962890625]
[-0.052886962890625, 0.92333984375]
[0.2294921875, 0.77294921875]
[0.446533203125, 0.5361328125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.33740234375, 0.61962890625]
[-0.052886962890625, 0.92333984375]
[0.2294921875, 0.77294921875]
[0.446533203125, 0.5361328125]
This is the real loss :  tensor(0.0982, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
323     1    NaN  ...  0.094221  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
324     1    NaN  ...  0.084981  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
325     1    NaN  ...  0.142466  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
326     1    NaN  ...  0.196688  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
327     1    NaN  ...  0.098247  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[328 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.453369140625, 0.42724609375]
[0.0325927734375, 0.8310546875]
[0.02972412109375, 0.95361328125]
[0.341064453125, 0.59716796875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.453369140625, 0.42724609375]
[0.0325927734375, 0.8310546875]
[0.02972412109375, 0.95361328125]
[0.341064453125, 0.59716796875]
This is the real loss :  tensor(0.1631, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
324     1    NaN  ...  0.084981  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
325     1    NaN  ...  0.142466  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
326     1    NaN  ...  0.196688  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
327     1    NaN  ...  0.098247  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
328     1    NaN  ...  0.163099  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[329 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.27001953125, 0.72021484375]
[0.5322265625, 0.45458984375]
[0.04669189453125, 0.92578125]
[0.0845947265625, 0.828125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.27001953125, 0.72021484375]
[0.5322265625, 0.45458984375]
[0.04669189453125, 0.92578125]
[0.0845947265625, 0.828125]
This is the real loss :  tensor(0.1902, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
325     1    NaN  ...  0.142466  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
326     1    NaN  ...  0.196688  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
327     1    NaN  ...  0.098247  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
328     1    NaN  ...  0.163099  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
329     1    NaN  ...  0.190179  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[330 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.39990234375, 0.599609375]
[-0.037078857421875, 0.98486328125]
[0.354248046875, 0.5712890625]
[0.395751953125, 0.54296875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.39990234375, 0.599609375]
[-0.037078857421875, 0.98486328125]
[0.354248046875, 0.5712890625]
[0.395751953125, 0.54296875]
This is the real loss :  tensor(0.1788, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
326     1    NaN  ...  0.196688  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
327     1    NaN  ...  0.098247  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
328     1    NaN  ...  0.163099  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
329     1    NaN  ...  0.190179  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
330     1    NaN  ...  0.178838  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[331 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.268798828125, 0.73486328125]
[0.157470703125, 0.78662109375]
[0.218994140625, 0.71533203125]
[0.2060546875, 0.75537109375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.268798828125, 0.73486328125]
[0.157470703125, 0.78662109375]
[0.218994140625, 0.71533203125]
[0.2060546875, 0.75537109375]
This is the real loss :  tensor(0.0555, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
327     1    NaN  ...  0.098247  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
328     1    NaN  ...  0.163099  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
329     1    NaN  ...  0.190179  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
330     1    NaN  ...  0.178838  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
331     1    NaN  ...  0.055522  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[332 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.1834716796875, 0.73193359375]
[0.16162109375, 0.84765625]
[0.436767578125, 0.41064453125]
[0.08526611328125, 0.91552734375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1834716796875, 0.73193359375]
[0.16162109375, 0.84765625]
[0.436767578125, 0.41064453125]
[0.08526611328125, 0.91552734375]
This is the real loss :  tensor(0.0819, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
328     1    NaN  ...  0.163099  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
329     1    NaN  ...  0.190179  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
330     1    NaN  ...  0.178838  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
331     1    NaN  ...  0.055522  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
332     1    NaN  ...  0.081890  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[333 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.45947265625, 0.5302734375]
[0.4931640625, 0.5244140625]
[0.313232421875, 0.67431640625]
[-0.025421142578125, 1.0166015625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.45947265625, 0.5302734375]
[0.4931640625, 0.5244140625]
[0.313232421875, 0.67431640625]
[-0.025421142578125, 1.0166015625]
This is the real loss :  tensor(0.1560, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
329     1    NaN  ...  0.190179  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
330     1    NaN  ...  0.178838  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
331     1    NaN  ...  0.055522  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
332     1    NaN  ...  0.081890  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
333     1    NaN  ...  0.155982  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[334 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.58056640625, 0.3916015625]
[0.056365966796875, 1.0107421875]
[0.06365966796875, 0.84814453125]
[0.41259765625, 0.57568359375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.58056640625, 0.3916015625]
[0.056365966796875, 1.0107421875]
[0.06365966796875, 0.84814453125]
[0.41259765625, 0.57568359375]
This is the real loss :  tensor(0.0887, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
330     1    NaN  ...  0.178838  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
331     1    NaN  ...  0.055522  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
332     1    NaN  ...  0.081890  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
333     1    NaN  ...  0.155982  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
334     1    NaN  ...  0.088745  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[335 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.1962890625, 0.82080078125]
[0.40087890625, 0.54443359375]
[0.156494140625, 0.8388671875]
[0.33251953125, 0.66943359375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1962890625, 0.82080078125]
[0.40087890625, 0.54443359375]
[0.156494140625, 0.8388671875]
[0.33251953125, 0.66943359375]
This is the real loss :  tensor(0.0886, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
331     1    NaN  ...  0.055522  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
332     1    NaN  ...  0.081890  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
333     1    NaN  ...  0.155982  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
334     1    NaN  ...  0.088745  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
335     1    NaN  ...  0.088648  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[336 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.50439453125, 0.537109375]
[0.048095703125, 0.94189453125]
[0.0909423828125, 0.826171875]
[0.376220703125, 0.56787109375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.50439453125, 0.537109375]
[0.048095703125, 0.94189453125]
[0.0909423828125, 0.826171875]
[0.376220703125, 0.56787109375]
This is the real loss :  tensor(0.1612, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
332     1    NaN  ...  0.081890  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
333     1    NaN  ...  0.155982  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
334     1    NaN  ...  0.088745  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
335     1    NaN  ...  0.088648  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
336     1    NaN  ...  0.161233  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[337 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[-0.0085601806640625, 0.923828125]
[0.434326171875, 0.51171875]
[0.345458984375, 0.6171875]
[0.225830078125, 0.7529296875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.0085601806640625, 0.923828125]
[0.434326171875, 0.51171875]
[0.345458984375, 0.6171875]
[0.225830078125, 0.7529296875]
This is the real loss :  tensor(0.1014, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
333     1    NaN  ...  0.155982  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
334     1    NaN  ...  0.088745  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
335     1    NaN  ...  0.088648  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
336     1    NaN  ...  0.161233  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
337     1    NaN  ...  0.101358  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[338 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.54052734375, 0.47021484375]
[-0.01117706298828125, 1.078125]
[0.5244140625, 0.50927734375]
[0.203369140625, 0.818359375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.54052734375, 0.47021484375]
[-0.01117706298828125, 1.078125]
[0.5244140625, 0.50927734375]
[0.203369140625, 0.818359375]
This is the real loss :  tensor(0.2999, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
334     1    NaN  ...  0.088745  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
335     1    NaN  ...  0.088648  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
336     1    NaN  ...  0.161233  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
337     1    NaN  ...  0.101358  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
338     1    NaN  ...  0.299903  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[339 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.09942626953125, 0.900390625]
[0.0921630859375, 0.80029296875]
[0.44140625, 0.50146484375]
[0.333740234375, 0.59423828125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.09942626953125, 0.900390625]
[0.0921630859375, 0.80029296875]
[0.44140625, 0.50146484375]
[0.333740234375, 0.59423828125]
This is the real loss :  tensor(0.1135, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
335     1    NaN  ...  0.088648  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
336     1    NaN  ...  0.161233  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
337     1    NaN  ...  0.101358  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
338     1    NaN  ...  0.299903  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
339     1    NaN  ...  0.113463  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[340 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.0011472702026367188, 1.0244140625]
[0.296142578125, 0.76123046875]
[0.5478515625, 0.411376953125]
[0.245361328125, 0.77294921875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.0011472702026367188, 1.0244140625]
[0.296142578125, 0.76123046875]
[0.5478515625, 0.411376953125]
[0.245361328125, 0.77294921875]
This is the real loss :  tensor(0.3611, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
336     1    NaN  ...  0.161233  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
337     1    NaN  ...  0.101358  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
338     1    NaN  ...  0.299903  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
339     1    NaN  ...  0.113463  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
340     1    NaN  ...  0.361129  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[341 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.055419921875, 1.001953125]
[0.5283203125, 0.336181640625]
[0.318359375, 0.697265625]
[0.07684326171875, 0.859375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.055419921875, 1.001953125]
[0.5283203125, 0.336181640625]
[0.318359375, 0.697265625]
[0.07684326171875, 0.859375]
This is the real loss :  tensor(0.1177, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
337     1    NaN  ...  0.101358  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
338     1    NaN  ...  0.299903  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
339     1    NaN  ...  0.113463  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
340     1    NaN  ...  0.361129  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
341     1    NaN  ...  0.117692  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[342 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.447021484375, 0.50927734375]
[-0.042724609375, 0.982421875]
[0.1500244140625, 0.7685546875]
[0.5849609375, 0.44091796875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.447021484375, 0.50927734375]
[-0.042724609375, 0.982421875]
[0.1500244140625, 0.7685546875]
[0.5849609375, 0.44091796875]
This is the real loss :  tensor(0.1107, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
338     1    NaN  ...  0.299903  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
339     1    NaN  ...  0.113463  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
340     1    NaN  ...  0.361129  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
341     1    NaN  ...  0.117692  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
342     1    NaN  ...  0.110689  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[343 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.2413330078125, 0.65234375]
[0.0650634765625, 0.90673828125]
[0.71337890625, 0.369873046875]
[0.01477813720703125, 0.95849609375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.2413330078125, 0.65234375]
[0.0650634765625, 0.90673828125]
[0.71337890625, 0.369873046875]
[0.01477813720703125, 0.95849609375]
This is the real loss :  tensor(0.2402, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
339     1    NaN  ...  0.113463  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
340     1    NaN  ...  0.361129  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
341     1    NaN  ...  0.117692  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
342     1    NaN  ...  0.110689  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
343     1    NaN  ...  0.240246  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[344 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.39306640625, 0.53662109375]
[0.33154296875, 0.59326171875]
[0.1343994140625, 0.83935546875]
[0.2457275390625, 0.7978515625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.39306640625, 0.53662109375]
[0.33154296875, 0.59326171875]
[0.1343994140625, 0.83935546875]
[0.2457275390625, 0.7978515625]
This is the real loss :  tensor(0.3381, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
340     1    NaN  ...  0.361129  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
341     1    NaN  ...  0.117692  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
342     1    NaN  ...  0.110689  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
343     1    NaN  ...  0.240246  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
344     1    NaN  ...  0.338061  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[345 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.07318115234375, 0.9501953125]
[0.439697265625, 0.599609375]
[0.55078125, 0.347412109375]
[0.048126220703125, 0.97998046875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.07318115234375, 0.9501953125]
[0.439697265625, 0.599609375]
[0.55078125, 0.347412109375]
[0.048126220703125, 0.97998046875]
This is the real loss :  tensor(0.1367, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
341     1    NaN  ...  0.117692  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
342     1    NaN  ...  0.110689  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
343     1    NaN  ...  0.240246  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
344     1    NaN  ...  0.338061  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
345     1    NaN  ...  0.136679  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[346 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.160888671875, 0.75732421875]
[0.0987548828125, 0.99267578125]
[0.10723876953125, 0.85888671875]
[0.68798828125, 0.342041015625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.160888671875, 0.75732421875]
[0.0987548828125, 0.99267578125]
[0.10723876953125, 0.85888671875]
[0.68798828125, 0.342041015625]
This is the real loss :  tensor(0.0425, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
342     1    NaN  ...  0.110689  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
343     1    NaN  ...  0.240246  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
344     1    NaN  ...  0.338061  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
345     1    NaN  ...  0.136679  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
346     1    NaN  ...  0.042542  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[347 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.1475830078125, 0.84814453125]
[0.11590576171875, 0.84423828125]
[0.088134765625, 0.9521484375]
[0.64111328125, 0.330810546875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1475830078125, 0.84814453125]
[0.11590576171875, 0.84423828125]
[0.088134765625, 0.9521484375]
[0.64111328125, 0.330810546875]
This is the real loss :  tensor(0.0414, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
343     1    NaN  ...  0.240246  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
344     1    NaN  ...  0.338061  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
345     1    NaN  ...  0.136679  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
346     1    NaN  ...  0.042542  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
347     1    NaN  ...  0.041354  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[348 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.2275390625, 0.70751953125]
[0.31591796875, 0.5888671875]
[0.2000732421875, 0.9033203125]
[0.18505859375, 0.7294921875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.2275390625, 0.70751953125]
[0.31591796875, 0.5888671875]
[0.2000732421875, 0.9033203125]
[0.18505859375, 0.7294921875]
This is the real loss :  tensor(0.0704, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
344     1    NaN  ...  0.338061  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
345     1    NaN  ...  0.136679  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
346     1    NaN  ...  0.042542  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
347     1    NaN  ...  0.041354  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
348     1    NaN  ...  0.070369  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[349 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.303466796875, 0.59814453125]
[0.190673828125, 0.80224609375]
[0.5634765625, 0.47607421875]
[-0.01092529296875, 0.9912109375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.303466796875, 0.59814453125]
[0.190673828125, 0.80224609375]
[0.5634765625, 0.47607421875]
[-0.01092529296875, 0.9912109375]
This is the real loss :  tensor(0.1670, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
345     1    NaN  ...  0.136679  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
346     1    NaN  ...  0.042542  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
347     1    NaN  ...  0.041354  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
348     1    NaN  ...  0.070369  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
349     1    NaN  ...  0.166974  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[350 rows x 5 columns]checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3

Logits shape before squeeze: torch.Size([4, 2])
[0.6494140625, 0.445068359375]
[0.0692138671875, 0.91748046875]
[0.1746826171875, 0.7978515625]
[0.101806640625, 0.86962890625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.6494140625, 0.445068359375]
[0.0692138671875, 0.91748046875]
[0.1746826171875, 0.7978515625]
[0.101806640625, 0.86962890625]
This is the real loss :  tensor(0.1050, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
346     1    NaN  ...  0.042542  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
347     1    NaN  ...  0.041354  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
348     1    NaN  ...  0.070369  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
349     1    NaN  ...  0.166974  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
350     1    NaN  ...  0.105003  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[351 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.52587890625, 0.51025390625]
[0.03509521484375, 1.0283203125]
[0.2320556640625, 0.78125]
[0.355712890625, 0.74462890625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.52587890625, 0.51025390625]
[0.03509521484375, 1.0283203125]
[0.2320556640625, 0.78125]
[0.355712890625, 0.74462890625]
This is the real loss :  tensor(0.1987, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
347     1    NaN  ...  0.041354  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
348     1    NaN  ...  0.070369  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
349     1    NaN  ...  0.166974  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
350     1    NaN  ...  0.105003  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
351     1    NaN  ...  0.198714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[352 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.485107421875, 0.468017578125]
[0.038848876953125, 0.955078125]
[0.0254058837890625, 0.9765625]
[0.58056640625, 0.49462890625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.485107421875, 0.468017578125]
[0.038848876953125, 0.955078125]
[0.0254058837890625, 0.9765625]
[0.58056640625, 0.49462890625]
This is the real loss :  tensor(0.3515, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
348     1    NaN  ...  0.070369  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
349     1    NaN  ...  0.166974  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
350     1    NaN  ...  0.105003  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
351     1    NaN  ...  0.198714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
352     1    NaN  ...  0.351472  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[353 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.368408203125, 0.5908203125]
[0.182373046875, 0.87109375]
[0.4541015625, 0.5673828125]
[0.14013671875, 0.84912109375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.368408203125, 0.5908203125]
[0.182373046875, 0.87109375]
[0.4541015625, 0.5673828125]
[0.14013671875, 0.84912109375]
This is the real loss :  tensor(0.3314, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
349     1    NaN  ...  0.166974  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
350     1    NaN  ...  0.105003  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
351     1    NaN  ...  0.198714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
352     1    NaN  ...  0.351472  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
353     1    NaN  ...  0.331449  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[354 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.492431640625, 0.5771484375]
[0.4033203125, 0.6240234375]
[-0.03814697265625, 1.095703125]
[0.43212890625, 0.677734375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.492431640625, 0.5771484375]
[0.4033203125, 0.6240234375]
[-0.03814697265625, 1.095703125]
[0.43212890625, 0.677734375]
This is the real loss :  tensor(0.1835, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
350     1    NaN  ...  0.105003  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
351     1    NaN  ...  0.198714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
352     1    NaN  ...  0.351472  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
353     1    NaN  ...  0.331449  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
354     1    NaN  ...  0.183491  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[355 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[-0.048553466796875, 0.9013671875]
[0.30810546875, 0.49462890625]
[0.591796875, 0.47216796875]
[0.08984375, 0.82421875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.048553466796875, 0.9013671875]
[0.30810546875, 0.49462890625]
[0.591796875, 0.47216796875]
[0.08984375, 0.82421875]
This is the real loss :  tensor(0.1288, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
351     1    NaN  ...  0.198714  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
352     1    NaN  ...  0.351472  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
353     1    NaN  ...  0.331449  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
354     1    NaN  ...  0.183491  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
355     1    NaN  ...  0.128777  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[356 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.02777099609375, 0.93212890625]
[0.051971435546875, 0.96484375]
[0.57373046875, 0.439697265625]
[0.5, 0.529296875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.02777099609375, 0.93212890625]
[0.051971435546875, 0.96484375]
[0.57373046875, 0.439697265625]
[0.5, 0.529296875]
This is the real loss :  tensor(0.1070, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
352     1    NaN  ...  0.351472  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
353     1    NaN  ...  0.331449  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
354     1    NaN  ...  0.183491  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
355     1    NaN  ...  0.128777  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
356     1    NaN  ...  0.106989  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[357 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.26220703125, 0.701171875]
[0.1795654296875, 0.78662109375]
[0.35693359375, 0.73974609375]
[0.1834716796875, 0.79345703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.26220703125, 0.701171875]
[0.1795654296875, 0.78662109375]
[0.35693359375, 0.73974609375]
[0.1834716796875, 0.79345703125]
This is the real loss :  tensor(0.0634, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
353     1    NaN  ...  0.331449  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
354     1    NaN  ...  0.183491  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
355     1    NaN  ...  0.128777  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
356     1    NaN  ...  0.106989  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
357     1    NaN  ...  0.063410  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[358 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.12213134765625, 0.8857421875]
[0.139892578125, 0.91650390625]
[0.56982421875, 0.384765625]
[0.140869140625, 0.88525390625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.12213134765625, 0.8857421875]
[0.139892578125, 0.91650390625]
[0.56982421875, 0.384765625]
[0.140869140625, 0.88525390625]
This is the real loss :  tensor(0.0526, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
354     1    NaN  ...  0.183491  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
355     1    NaN  ...  0.128777  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
356     1    NaN  ...  0.106989  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
357     1    NaN  ...  0.063410  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
358     1    NaN  ...  0.052577  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[359 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.56494140625, 0.45654296875]
[0.4072265625, 0.5546875]
[0.0298614501953125, 0.98583984375]
[0.06866455078125, 0.91455078125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.56494140625, 0.45654296875]
[0.4072265625, 0.5546875]
[0.0298614501953125, 0.98583984375]
[0.06866455078125, 0.91455078125]
This is the real loss :  tensor(0.1337, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
355     1    NaN  ...  0.128777  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
356     1    NaN  ...  0.106989  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
357     1    NaN  ...  0.063410  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
358     1    NaN  ...  0.052577  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
359     1    NaN  ...  0.133734  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[360 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.06854248046875, 0.9384765625]
[0.28076171875, 0.74267578125]
[0.54248046875, 0.4345703125]
[0.10540771484375, 0.87548828125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.06854248046875, 0.9384765625]
[0.28076171875, 0.74267578125]
[0.54248046875, 0.4345703125]
[0.10540771484375, 0.87548828125]
This is the real loss :  tensor(0.2147, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
356     1    NaN  ...  0.106989  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
357     1    NaN  ...  0.063410  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
358     1    NaN  ...  0.052577  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
359     1    NaN  ...  0.133734  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
360     1    NaN  ...  0.214745  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[361 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.297119140625, 0.78466796875]
[0.47412109375, 0.57470703125]
[0.0045623779296875, 1.009765625]
[0.4912109375, 0.5673828125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.297119140625, 0.78466796875]
[0.47412109375, 0.57470703125]
[0.0045623779296875, 1.009765625]
[0.4912109375, 0.5673828125]
This is the real loss :  tensor(0.1653, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
357     1    NaN  ...  0.063410  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
358     1    NaN  ...  0.052577  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
359     1    NaN  ...  0.133734  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
360     1    NaN  ...  0.214745  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
361     1    NaN  ...  0.165299  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[362 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.11700439453125, 0.87109375]
[0.273681640625, 0.748046875]
[0.0443115234375, 0.92041015625]
[0.5927734375, 0.435302734375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.11700439453125, 0.87109375]
[0.273681640625, 0.748046875]
[0.0443115234375, 0.92041015625]
[0.5927734375, 0.435302734375]
This is the real loss :  tensor(0.0665, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
358     1    NaN  ...  0.052577  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
359     1    NaN  ...  0.133734  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
360     1    NaN  ...  0.214745  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
361     1    NaN  ...  0.165299  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
362     1    NaN  ...  0.066539  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[363 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.31884765625, 0.70458984375]
[0.2841796875, 0.66015625]
[-0.033966064453125, 1.0244140625]
[0.52587890625, 0.47900390625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.31884765625, 0.70458984375]
[0.2841796875, 0.66015625]
[-0.033966064453125, 1.0244140625]
[0.52587890625, 0.47900390625]
This is the real loss :  tensor(0.2109, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.582353  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
1       0    NaN  ...  0.548506  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
2       0    NaN  ...  0.426827  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.462099  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
4       0    NaN  ...  0.260899  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
..    ...    ...  ...       ...                                               ...
359     1    NaN  ...  0.133734  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
360     1    NaN  ...  0.214745  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
361     1    NaN  ...  0.165299  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
362     1    NaN  ...  0.066539  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
363     1    NaN  ...  0.210859  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[364 rows x 5 columns]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.40586644411087036, 0.5123195648193359]
[0.04140918329358101, 0.9123368859291077]
[0.3899787366390228, 0.6835499405860901]
[0.5092953443527222, 0.4866027235984802]
This is the real loss :  0.14839273691177368
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.10440552234649658, 0.9943879246711731]
[0.03758726641535759, 0.9650813937187195]
[0.4606722593307495, 0.47674083709716797]
[-0.028274979442358017, 0.9936532974243164]
This is the real loss :  0.44281911849975586
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.3461940586566925, 0.6664182543754578]
[0.3921132981777191, 0.6198898553848267]
[0.02195419743657112, 1.0387287139892578]
[0.33762872219085693, 0.6531800031661987]
This is the real loss :  0.6755221784114838
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.4950065612792969, 0.49095427989959717]
[0.4473028779029846, 0.5428802371025085]
[0.31823045015335083, 0.6963589787483215]
[0.014962766319513321, 0.9739495515823364]
This is the real loss :  0.9074872881174088
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.5422244071960449, 0.42665863037109375]
[-0.022865768522024155, 0.9733161926269531]
[0.42252904176712036, 0.6370730400085449]
[0.00437641516327858, 0.996898353099823]
This is the real loss :  1.0242670476436615
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.013913195580244064, 0.9638315439224243]
[0.38169461488723755, 0.7103350758552551]
[-0.07842278480529785, 1.0784436464309692]
[0.27044570446014404, 0.7032794952392578]
This is the real loss :  1.0748402699828148
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.3968953490257263, 0.6340117454528809]
[0.555304765701294, 0.43608781695365906]
[0.49592578411102295, 0.49791449308395386]
[-0.008446376770734787, 1.0834908485412598]
This is the real loss :  1.2826757952570915
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.33502307534217834, 0.7172778248786926]
[0.5428730845451355, 0.44281941652297974]
[0.36164435744285583, 0.6882901787757874]
[0.5451249480247498, 0.4154152274131775]
This is the real loss :  1.5473468229174614
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.03976884111762047, 1.050862431526184]
[0.4996393322944641, 0.4879053235054016]
[0.017489034682512283, 0.9434548020362854]
[-0.03866925463080406, 1.1393344402313232]
This is the real loss :  1.614904522895813
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.3034689426422119, 0.7516642212867737]
[0.45378124713897705, 0.5335593223571777]
[0.49603450298309326, 0.4660959243774414]
[0.00034250691533088684, 0.9883924126625061]
This is the real loss :  1.7734099179506302
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.5010023713111877, 0.47526997327804565]
[-0.020124536007642746, 1.117872714996338]
[0.5368326306343079, 0.46753644943237305]
[0.5270849466323853, 0.4637899398803711]
This is the real loss :  1.9593640714883804
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.01789924129843712, 1.009013056755066]
[-0.07085062563419342, 0.9776298403739929]
[0.08395549654960632, 0.9032033085823059]
[0.014725621789693832, 0.9618142247200012]
This is the real loss :  2.1671778857707977
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.010541331022977829, 1.027101755142212]
[0.27839386463165283, 0.6909443736076355]
[0.272367000579834, 0.7058614492416382]
[0.00028115883469581604, 1.0182385444641113]
This is the real loss :  2.3121777772903442
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012], [-0.010541331022977829, 1.027101755142212, 0.27839386463165283, 0.6909443736076355], [0.272367000579834, 0.7058614492416382, 0.00028115883469581604, 1.0182385444641113]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.28895533084869385, 0.6804113388061523]
[0.39941537380218506, 0.6184879541397095]
[0.011353697627782822, 0.9631422162055969]
[-0.06577624380588531, 1.0429608821868896]
This is the real loss :  2.429242894053459
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012], [-0.010541331022977829, 1.027101755142212, 0.27839386463165283, 0.6909443736076355], [0.272367000579834, 0.7058614492416382, 0.00028115883469581604, 1.0182385444641113], [0.28895533084869385, 0.6804113388061523, 0.39941537380218506, 0.6184879541397095], [0.011353697627782822, 0.9631422162055969, -0.06577624380588531, 1.0429608821868896]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.0018919818103313446, 0.9945912957191467]
[0.42653173208236694, 0.6086744070053101]
[0.01611100509762764, 1.0032296180725098]
[0.023631710559129715, 0.9491965770721436]
This is the real loss :  2.748483195900917
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012], [-0.010541331022977829, 1.027101755142212, 0.27839386463165283, 0.6909443736076355], [0.272367000579834, 0.7058614492416382, 0.00028115883469581604, 1.0182385444641113], [0.28895533084869385, 0.6804113388061523, 0.39941537380218506, 0.6184879541397095], [0.011353697627782822, 0.9631422162055969, -0.06577624380588531, 1.0429608821868896], [-0.0018919818103313446, 0.9945912957191467, 0.42653173208236694, 0.6086744070053101], [0.01611100509762764, 1.0032296180725098, 0.023631710559129715, 0.9491965770721436]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.03927203640341759, 1.001900553703308]
[0.3045749366283417, 0.7350011467933655]
[-0.05471464619040489, 1.0141152143478394]
[-0.021505441516637802, 0.9882152080535889]
This is the real loss :  3.0367319732904434
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012], [-0.010541331022977829, 1.027101755142212, 0.27839386463165283, 0.6909443736076355], [0.272367000579834, 0.7058614492416382, 0.00028115883469581604, 1.0182385444641113], [0.28895533084869385, 0.6804113388061523, 0.39941537380218506, 0.6184879541397095], [0.011353697627782822, 0.9631422162055969, -0.06577624380588531, 1.0429608821868896], [-0.0018919818103313446, 0.9945912957191467, 0.42653173208236694, 0.6086744070053101], [0.01611100509762764, 1.0032296180725098, 0.023631710559129715, 0.9491965770721436], [0.03927203640341759, 1.001900553703308, 0.3045749366283417, 0.7350011467933655], [-0.05471464619040489, 1.0141152143478394, -0.021505441516637802, 0.9882152080535889]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.5345359444618225, 0.4880406856536865]
[-0.019061248749494553, 0.9891769289970398]
[-0.03129149600863457, 1.1256051063537598]
[0.4196738004684448, 0.5263700485229492]
This is the real loss :  3.172472059726715
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012], [-0.010541331022977829, 1.027101755142212, 0.27839386463165283, 0.6909443736076355], [0.272367000579834, 0.7058614492416382, 0.00028115883469581604, 1.0182385444641113], [0.28895533084869385, 0.6804113388061523, 0.39941537380218506, 0.6184879541397095], [0.011353697627782822, 0.9631422162055969, -0.06577624380588531, 1.0429608821868896], [-0.0018919818103313446, 0.9945912957191467, 0.42653173208236694, 0.6086744070053101], [0.01611100509762764, 1.0032296180725098, 0.023631710559129715, 0.9491965770721436], [0.03927203640341759, 1.001900553703308, 0.3045749366283417, 0.7350011467933655], [-0.05471464619040489, 1.0141152143478394, -0.021505441516637802, 0.9882152080535889], [0.5345359444618225, 0.4880406856536865, -0.019061248749494553, 0.9891769289970398], [-0.03129149600863457, 1.1256051063537598, 0.4196738004684448, 0.5263700485229492]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.4761011004447937, 0.47544240951538086]
[0.5494738221168518, 0.41425636410713196]
[0.5106468200683594, 0.47100865840911865]
[0.20519176125526428, 0.7133517265319824]
This is the real loss :  3.3989361822605133
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012], [-0.010541331022977829, 1.027101755142212, 0.27839386463165283, 0.6909443736076355], [0.272367000579834, 0.7058614492416382, 0.00028115883469581604, 1.0182385444641113], [0.28895533084869385, 0.6804113388061523, 0.39941537380218506, 0.6184879541397095], [0.011353697627782822, 0.9631422162055969, -0.06577624380588531, 1.0429608821868896], [-0.0018919818103313446, 0.9945912957191467, 0.42653173208236694, 0.6086744070053101], [0.01611100509762764, 1.0032296180725098, 0.023631710559129715, 0.9491965770721436], [0.03927203640341759, 1.001900553703308, 0.3045749366283417, 0.7350011467933655], [-0.05471464619040489, 1.0141152143478394, -0.021505441516637802, 0.9882152080535889], [0.5345359444618225, 0.4880406856536865, -0.019061248749494553, 0.9891769289970398], [-0.03129149600863457, 1.1256051063537598, 0.4196738004684448, 0.5263700485229492], [0.4761011004447937, 0.47544240951538086, 0.5494738221168518, 0.41425636410713196], [0.5106468200683594, 0.47100865840911865, 0.20519176125526428, 0.7133517265319824]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.016757089644670486, 0.9700817465782166]
[0.36863061785697937, 0.6217348575592041]
[0.01527344062924385, 0.9615796208381653]
[0.5034911632537842, 0.49691423773765564]
This is the real loss :  3.495849072933197
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012], [-0.010541331022977829, 1.027101755142212, 0.27839386463165283, 0.6909443736076355], [0.272367000579834, 0.7058614492416382, 0.00028115883469581604, 1.0182385444641113], [0.28895533084869385, 0.6804113388061523, 0.39941537380218506, 0.6184879541397095], [0.011353697627782822, 0.9631422162055969, -0.06577624380588531, 1.0429608821868896], [-0.0018919818103313446, 0.9945912957191467, 0.42653173208236694, 0.6086744070053101], [0.01611100509762764, 1.0032296180725098, 0.023631710559129715, 0.9491965770721436], [0.03927203640341759, 1.001900553703308, 0.3045749366283417, 0.7350011467933655], [-0.05471464619040489, 1.0141152143478394, -0.021505441516637802, 0.9882152080535889], [0.5345359444618225, 0.4880406856536865, -0.019061248749494553, 0.9891769289970398], [-0.03129149600863457, 1.1256051063537598, 0.4196738004684448, 0.5263700485229492], [0.4761011004447937, 0.47544240951538086, 0.5494738221168518, 0.41425636410713196], [0.5106468200683594, 0.47100865840911865, 0.20519176125526428, 0.7133517265319824], [0.016757089644670486, 0.9700817465782166, 0.36863061785697937, 0.6217348575592041], [0.01527344062924385, 0.9615796208381653, 0.5034911632537842, 0.49691423773765564]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.002861347049474716, 0.9952285885810852]
[0.08026503026485443, 0.8754879832267761]
[0.3138233423233032, 0.724989116191864]
[-0.0068575553596019745, 1.0656505823135376]
This is the real loss :  3.623696729540825
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012], [-0.010541331022977829, 1.027101755142212, 0.27839386463165283, 0.6909443736076355], [0.272367000579834, 0.7058614492416382, 0.00028115883469581604, 1.0182385444641113], [0.28895533084869385, 0.6804113388061523, 0.39941537380218506, 0.6184879541397095], [0.011353697627782822, 0.9631422162055969, -0.06577624380588531, 1.0429608821868896], [-0.0018919818103313446, 0.9945912957191467, 0.42653173208236694, 0.6086744070053101], [0.01611100509762764, 1.0032296180725098, 0.023631710559129715, 0.9491965770721436], [0.03927203640341759, 1.001900553703308, 0.3045749366283417, 0.7350011467933655], [-0.05471464619040489, 1.0141152143478394, -0.021505441516637802, 0.9882152080535889], [0.5345359444618225, 0.4880406856536865, -0.019061248749494553, 0.9891769289970398], [-0.03129149600863457, 1.1256051063537598, 0.4196738004684448, 0.5263700485229492], [0.4761011004447937, 0.47544240951538086, 0.5494738221168518, 0.41425636410713196], [0.5106468200683594, 0.47100865840911865, 0.20519176125526428, 0.7133517265319824], [0.016757089644670486, 0.9700817465782166, 0.36863061785697937, 0.6217348575592041], [0.01527344062924385, 0.9615796208381653, 0.5034911632537842, 0.49691423773765564], [0.002861347049474716, 0.9952285885810852, 0.08026503026485443, 0.8754879832267761], [0.3138233423233032, 0.724989116191864, -0.0068575553596019745, 1.0656505823135376]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.4927557110786438, 0.4665831923484802]
[0.5457000732421875, 0.44373881816864014]
[0.4286043047904968, 0.5650157928466797]
[-0.03438287600874901, 0.914689838886261]
This is the real loss :  4.0439129918813705
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012], [-0.010541331022977829, 1.027101755142212, 0.27839386463165283, 0.6909443736076355], [0.272367000579834, 0.7058614492416382, 0.00028115883469581604, 1.0182385444641113], [0.28895533084869385, 0.6804113388061523, 0.39941537380218506, 0.6184879541397095], [0.011353697627782822, 0.9631422162055969, -0.06577624380588531, 1.0429608821868896], [-0.0018919818103313446, 0.9945912957191467, 0.42653173208236694, 0.6086744070053101], [0.01611100509762764, 1.0032296180725098, 0.023631710559129715, 0.9491965770721436], [0.03927203640341759, 1.001900553703308, 0.3045749366283417, 0.7350011467933655], [-0.05471464619040489, 1.0141152143478394, -0.021505441516637802, 0.9882152080535889], [0.5345359444618225, 0.4880406856536865, -0.019061248749494553, 0.9891769289970398], [-0.03129149600863457, 1.1256051063537598, 0.4196738004684448, 0.5263700485229492], [0.4761011004447937, 0.47544240951538086, 0.5494738221168518, 0.41425636410713196], [0.5106468200683594, 0.47100865840911865, 0.20519176125526428, 0.7133517265319824], [0.016757089644670486, 0.9700817465782166, 0.36863061785697937, 0.6217348575592041], [0.01527344062924385, 0.9615796208381653, 0.5034911632537842, 0.49691423773765564], [0.002861347049474716, 0.9952285885810852, 0.08026503026485443, 0.8754879832267761], [0.3138233423233032, 0.724989116191864, -0.0068575553596019745, 1.0656505823135376], [0.4927557110786438, 0.4665831923484802, 0.5457000732421875, 0.44373881816864014], [0.4286043047904968, 0.5650157928466797, -0.03438287600874901, 0.914689838886261]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.04023156687617302, 1.0274161100387573]
[0.020412985235452652, 1.0125770568847656]
[0.4871100187301636, 0.48980385065078735]
[-0.024354305118322372, 0.9926795363426208]
This is the real loss :  4.1065590009093285
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012], [-0.010541331022977829, 1.027101755142212, 0.27839386463165283, 0.6909443736076355], [0.272367000579834, 0.7058614492416382, 0.00028115883469581604, 1.0182385444641113], [0.28895533084869385, 0.6804113388061523, 0.39941537380218506, 0.6184879541397095], [0.011353697627782822, 0.9631422162055969, -0.06577624380588531, 1.0429608821868896], [-0.0018919818103313446, 0.9945912957191467, 0.42653173208236694, 0.6086744070053101], [0.01611100509762764, 1.0032296180725098, 0.023631710559129715, 0.9491965770721436], [0.03927203640341759, 1.001900553703308, 0.3045749366283417, 0.7350011467933655], [-0.05471464619040489, 1.0141152143478394, -0.021505441516637802, 0.9882152080535889], [0.5345359444618225, 0.4880406856536865, -0.019061248749494553, 0.9891769289970398], [-0.03129149600863457, 1.1256051063537598, 0.4196738004684448, 0.5263700485229492], [0.4761011004447937, 0.47544240951538086, 0.5494738221168518, 0.41425636410713196], [0.5106468200683594, 0.47100865840911865, 0.20519176125526428, 0.7133517265319824], [0.016757089644670486, 0.9700817465782166, 0.36863061785697937, 0.6217348575592041], [0.01527344062924385, 0.9615796208381653, 0.5034911632537842, 0.49691423773765564], [0.002861347049474716, 0.9952285885810852, 0.08026503026485443, 0.8754879832267761], [0.3138233423233032, 0.724989116191864, -0.0068575553596019745, 1.0656505823135376], [0.4927557110786438, 0.4665831923484802, 0.5457000732421875, 0.44373881816864014], [0.4286043047904968, 0.5650157928466797, -0.03438287600874901, 0.914689838886261], [-0.04023156687617302, 1.0274161100387573, 0.020412985235452652, 1.0125770568847656], [0.4871100187301636, 0.48980385065078735, -0.024354305118322372, 0.9926795363426208]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.4393797516822815, 0.6092626452445984]
[-0.0503314845263958, 1.040976643562317]
[0.39634567499160767, 0.6731142401695251]
[0.30240386724472046, 0.7293391823768616]
This is the real loss :  4.20388301461935
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012], [-0.010541331022977829, 1.027101755142212, 0.27839386463165283, 0.6909443736076355], [0.272367000579834, 0.7058614492416382, 0.00028115883469581604, 1.0182385444641113], [0.28895533084869385, 0.6804113388061523, 0.39941537380218506, 0.6184879541397095], [0.011353697627782822, 0.9631422162055969, -0.06577624380588531, 1.0429608821868896], [-0.0018919818103313446, 0.9945912957191467, 0.42653173208236694, 0.6086744070053101], [0.01611100509762764, 1.0032296180725098, 0.023631710559129715, 0.9491965770721436], [0.03927203640341759, 1.001900553703308, 0.3045749366283417, 0.7350011467933655], [-0.05471464619040489, 1.0141152143478394, -0.021505441516637802, 0.9882152080535889], [0.5345359444618225, 0.4880406856536865, -0.019061248749494553, 0.9891769289970398], [-0.03129149600863457, 1.1256051063537598, 0.4196738004684448, 0.5263700485229492], [0.4761011004447937, 0.47544240951538086, 0.5494738221168518, 0.41425636410713196], [0.5106468200683594, 0.47100865840911865, 0.20519176125526428, 0.7133517265319824], [0.016757089644670486, 0.9700817465782166, 0.36863061785697937, 0.6217348575592041], [0.01527344062924385, 0.9615796208381653, 0.5034911632537842, 0.49691423773765564], [0.002861347049474716, 0.9952285885810852, 0.08026503026485443, 0.8754879832267761], [0.3138233423233032, 0.724989116191864, -0.0068575553596019745, 1.0656505823135376], [0.4927557110786438, 0.4665831923484802, 0.5457000732421875, 0.44373881816864014], [0.4286043047904968, 0.5650157928466797, -0.03438287600874901, 0.914689838886261], [-0.04023156687617302, 1.0274161100387573, 0.020412985235452652, 1.0125770568847656], [0.4871100187301636, 0.48980385065078735, -0.024354305118322372, 0.9926795363426208], [0.4393797516822815, 0.6092626452445984, -0.0503314845263958, 1.040976643562317], [0.39634567499160767, 0.6731142401695251, 0.30240386724472046, 0.7293391823768616]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.0793527215719223, 1.0272775888442993]
[0.40042316913604736, 0.6546253561973572]
[-0.040432628244161606, 0.985545814037323]
[0.3108283281326294, 0.6712157726287842]
This is the real loss :  4.265535555779934
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012], [-0.010541331022977829, 1.027101755142212, 0.27839386463165283, 0.6909443736076355], [0.272367000579834, 0.7058614492416382, 0.00028115883469581604, 1.0182385444641113], [0.28895533084869385, 0.6804113388061523, 0.39941537380218506, 0.6184879541397095], [0.011353697627782822, 0.9631422162055969, -0.06577624380588531, 1.0429608821868896], [-0.0018919818103313446, 0.9945912957191467, 0.42653173208236694, 0.6086744070053101], [0.01611100509762764, 1.0032296180725098, 0.023631710559129715, 0.9491965770721436], [0.03927203640341759, 1.001900553703308, 0.3045749366283417, 0.7350011467933655], [-0.05471464619040489, 1.0141152143478394, -0.021505441516637802, 0.9882152080535889], [0.5345359444618225, 0.4880406856536865, -0.019061248749494553, 0.9891769289970398], [-0.03129149600863457, 1.1256051063537598, 0.4196738004684448, 0.5263700485229492], [0.4761011004447937, 0.47544240951538086, 0.5494738221168518, 0.41425636410713196], [0.5106468200683594, 0.47100865840911865, 0.20519176125526428, 0.7133517265319824], [0.016757089644670486, 0.9700817465782166, 0.36863061785697937, 0.6217348575592041], [0.01527344062924385, 0.9615796208381653, 0.5034911632537842, 0.49691423773765564], [0.002861347049474716, 0.9952285885810852, 0.08026503026485443, 0.8754879832267761], [0.3138233423233032, 0.724989116191864, -0.0068575553596019745, 1.0656505823135376], [0.4927557110786438, 0.4665831923484802, 0.5457000732421875, 0.44373881816864014], [0.4286043047904968, 0.5650157928466797, -0.03438287600874901, 0.914689838886261], [-0.04023156687617302, 1.0274161100387573, 0.020412985235452652, 1.0125770568847656], [0.4871100187301636, 0.48980385065078735, -0.024354305118322372, 0.9926795363426208], [0.4393797516822815, 0.6092626452445984, -0.0503314845263958, 1.040976643562317], [0.39634567499160767, 0.6731142401695251, 0.30240386724472046, 0.7293391823768616], [-0.0793527215719223, 1.0272775888442993, 0.40042316913604736, 0.6546253561973572], [-0.040432628244161606, 0.985545814037323, 0.3108283281326294, 0.6712157726287842]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.23698365688323975, 0.6146253943443298]
[0.01617550477385521, 0.9788640141487122]
[-0.017991792410612106, 1.106022834777832]
[0.509413480758667, 0.4739859104156494]
This is the real loss :  4.445231683552265
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012], [-0.010541331022977829, 1.027101755142212, 0.27839386463165283, 0.6909443736076355], [0.272367000579834, 0.7058614492416382, 0.00028115883469581604, 1.0182385444641113], [0.28895533084869385, 0.6804113388061523, 0.39941537380218506, 0.6184879541397095], [0.011353697627782822, 0.9631422162055969, -0.06577624380588531, 1.0429608821868896], [-0.0018919818103313446, 0.9945912957191467, 0.42653173208236694, 0.6086744070053101], [0.01611100509762764, 1.0032296180725098, 0.023631710559129715, 0.9491965770721436], [0.03927203640341759, 1.001900553703308, 0.3045749366283417, 0.7350011467933655], [-0.05471464619040489, 1.0141152143478394, -0.021505441516637802, 0.9882152080535889], [0.5345359444618225, 0.4880406856536865, -0.019061248749494553, 0.9891769289970398], [-0.03129149600863457, 1.1256051063537598, 0.4196738004684448, 0.5263700485229492], [0.4761011004447937, 0.47544240951538086, 0.5494738221168518, 0.41425636410713196], [0.5106468200683594, 0.47100865840911865, 0.20519176125526428, 0.7133517265319824], [0.016757089644670486, 0.9700817465782166, 0.36863061785697937, 0.6217348575592041], [0.01527344062924385, 0.9615796208381653, 0.5034911632537842, 0.49691423773765564], [0.002861347049474716, 0.9952285885810852, 0.08026503026485443, 0.8754879832267761], [0.3138233423233032, 0.724989116191864, -0.0068575553596019745, 1.0656505823135376], [0.4927557110786438, 0.4665831923484802, 0.5457000732421875, 0.44373881816864014], [0.4286043047904968, 0.5650157928466797, -0.03438287600874901, 0.914689838886261], [-0.04023156687617302, 1.0274161100387573, 0.020412985235452652, 1.0125770568847656], [0.4871100187301636, 0.48980385065078735, -0.024354305118322372, 0.9926795363426208], [0.4393797516822815, 0.6092626452445984, -0.0503314845263958, 1.040976643562317], [0.39634567499160767, 0.6731142401695251, 0.30240386724472046, 0.7293391823768616], [-0.0793527215719223, 1.0272775888442993, 0.40042316913604736, 0.6546253561973572], [-0.040432628244161606, 0.985545814037323, 0.3108283281326294, 0.6712157726287842], [0.23698365688323975, 0.6146253943443298, 0.01617550477385521, 0.9788640141487122], [-0.017991792410612106, 1.106022834777832, 0.509413480758667, 0.4739859104156494]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.506645143032074, 0.49443966150283813]
[0.2657829821109772, 0.7075903415679932]
[0.3331540822982788, 0.7690363526344299]
[0.0009912140667438507, 1.012966275215149]
This is the real loss :  4.546296499669552
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012], [-0.010541331022977829, 1.027101755142212, 0.27839386463165283, 0.6909443736076355], [0.272367000579834, 0.7058614492416382, 0.00028115883469581604, 1.0182385444641113], [0.28895533084869385, 0.6804113388061523, 0.39941537380218506, 0.6184879541397095], [0.011353697627782822, 0.9631422162055969, -0.06577624380588531, 1.0429608821868896], [-0.0018919818103313446, 0.9945912957191467, 0.42653173208236694, 0.6086744070053101], [0.01611100509762764, 1.0032296180725098, 0.023631710559129715, 0.9491965770721436], [0.03927203640341759, 1.001900553703308, 0.3045749366283417, 0.7350011467933655], [-0.05471464619040489, 1.0141152143478394, -0.021505441516637802, 0.9882152080535889], [0.5345359444618225, 0.4880406856536865, -0.019061248749494553, 0.9891769289970398], [-0.03129149600863457, 1.1256051063537598, 0.4196738004684448, 0.5263700485229492], [0.4761011004447937, 0.47544240951538086, 0.5494738221168518, 0.41425636410713196], [0.5106468200683594, 0.47100865840911865, 0.20519176125526428, 0.7133517265319824], [0.016757089644670486, 0.9700817465782166, 0.36863061785697937, 0.6217348575592041], [0.01527344062924385, 0.9615796208381653, 0.5034911632537842, 0.49691423773765564], [0.002861347049474716, 0.9952285885810852, 0.08026503026485443, 0.8754879832267761], [0.3138233423233032, 0.724989116191864, -0.0068575553596019745, 1.0656505823135376], [0.4927557110786438, 0.4665831923484802, 0.5457000732421875, 0.44373881816864014], [0.4286043047904968, 0.5650157928466797, -0.03438287600874901, 0.914689838886261], [-0.04023156687617302, 1.0274161100387573, 0.020412985235452652, 1.0125770568847656], [0.4871100187301636, 0.48980385065078735, -0.024354305118322372, 0.9926795363426208], [0.4393797516822815, 0.6092626452445984, -0.0503314845263958, 1.040976643562317], [0.39634567499160767, 0.6731142401695251, 0.30240386724472046, 0.7293391823768616], [-0.0793527215719223, 1.0272775888442993, 0.40042316913604736, 0.6546253561973572], [-0.040432628244161606, 0.985545814037323, 0.3108283281326294, 0.6712157726287842], [0.23698365688323975, 0.6146253943443298, 0.01617550477385521, 0.9788640141487122], [-0.017991792410612106, 1.106022834777832, 0.509413480758667, 0.4739859104156494], [0.506645143032074, 0.49443966150283813, 0.2657829821109772, 0.7075903415679932], [0.3331540822982788, 0.7690363526344299, 0.0009912140667438507, 1.012966275215149]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.0035591013729572296, 0.9972575306892395]
[-0.006878342479467392, 1.093258261680603]
[0.49123644828796387, 0.4907799959182739]
[0.5769304037094116, 0.450234591960907]
This is the real loss :  4.657567635178566
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012], [-0.010541331022977829, 1.027101755142212, 0.27839386463165283, 0.6909443736076355], [0.272367000579834, 0.7058614492416382, 0.00028115883469581604, 1.0182385444641113], [0.28895533084869385, 0.6804113388061523, 0.39941537380218506, 0.6184879541397095], [0.011353697627782822, 0.9631422162055969, -0.06577624380588531, 1.0429608821868896], [-0.0018919818103313446, 0.9945912957191467, 0.42653173208236694, 0.6086744070053101], [0.01611100509762764, 1.0032296180725098, 0.023631710559129715, 0.9491965770721436], [0.03927203640341759, 1.001900553703308, 0.3045749366283417, 0.7350011467933655], [-0.05471464619040489, 1.0141152143478394, -0.021505441516637802, 0.9882152080535889], [0.5345359444618225, 0.4880406856536865, -0.019061248749494553, 0.9891769289970398], [-0.03129149600863457, 1.1256051063537598, 0.4196738004684448, 0.5263700485229492], [0.4761011004447937, 0.47544240951538086, 0.5494738221168518, 0.41425636410713196], [0.5106468200683594, 0.47100865840911865, 0.20519176125526428, 0.7133517265319824], [0.016757089644670486, 0.9700817465782166, 0.36863061785697937, 0.6217348575592041], [0.01527344062924385, 0.9615796208381653, 0.5034911632537842, 0.49691423773765564], [0.002861347049474716, 0.9952285885810852, 0.08026503026485443, 0.8754879832267761], [0.3138233423233032, 0.724989116191864, -0.0068575553596019745, 1.0656505823135376], [0.4927557110786438, 0.4665831923484802, 0.5457000732421875, 0.44373881816864014], [0.4286043047904968, 0.5650157928466797, -0.03438287600874901, 0.914689838886261], [-0.04023156687617302, 1.0274161100387573, 0.020412985235452652, 1.0125770568847656], [0.4871100187301636, 0.48980385065078735, -0.024354305118322372, 0.9926795363426208], [0.4393797516822815, 0.6092626452445984, -0.0503314845263958, 1.040976643562317], [0.39634567499160767, 0.6731142401695251, 0.30240386724472046, 0.7293391823768616], [-0.0793527215719223, 1.0272775888442993, 0.40042316913604736, 0.6546253561973572], [-0.040432628244161606, 0.985545814037323, 0.3108283281326294, 0.6712157726287842], [0.23698365688323975, 0.6146253943443298, 0.01617550477385521, 0.9788640141487122], [-0.017991792410612106, 1.106022834777832, 0.509413480758667, 0.4739859104156494], [0.506645143032074, 0.49443966150283813, 0.2657829821109772, 0.7075903415679932], [0.3331540822982788, 0.7690363526344299, 0.0009912140667438507, 1.012966275215149], [0.0035591013729572296, 0.9972575306892395, -0.006878342479467392, 1.093258261680603], [0.49123644828796387, 0.4907799959182739, 0.5769304037094116, 0.450234591960907]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.03705750033259392, 1.0233534574508667]
[0.24717363715171814, 0.7397957444190979]
[0.3074831962585449, 0.633054792881012]
[0.0480886735022068, 0.9311363101005554]
This is the real loss :  4.703438777476549
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012], [-0.010541331022977829, 1.027101755142212, 0.27839386463165283, 0.6909443736076355], [0.272367000579834, 0.7058614492416382, 0.00028115883469581604, 1.0182385444641113], [0.28895533084869385, 0.6804113388061523, 0.39941537380218506, 0.6184879541397095], [0.011353697627782822, 0.9631422162055969, -0.06577624380588531, 1.0429608821868896], [-0.0018919818103313446, 0.9945912957191467, 0.42653173208236694, 0.6086744070053101], [0.01611100509762764, 1.0032296180725098, 0.023631710559129715, 0.9491965770721436], [0.03927203640341759, 1.001900553703308, 0.3045749366283417, 0.7350011467933655], [-0.05471464619040489, 1.0141152143478394, -0.021505441516637802, 0.9882152080535889], [0.5345359444618225, 0.4880406856536865, -0.019061248749494553, 0.9891769289970398], [-0.03129149600863457, 1.1256051063537598, 0.4196738004684448, 0.5263700485229492], [0.4761011004447937, 0.47544240951538086, 0.5494738221168518, 0.41425636410713196], [0.5106468200683594, 0.47100865840911865, 0.20519176125526428, 0.7133517265319824], [0.016757089644670486, 0.9700817465782166, 0.36863061785697937, 0.6217348575592041], [0.01527344062924385, 0.9615796208381653, 0.5034911632537842, 0.49691423773765564], [0.002861347049474716, 0.9952285885810852, 0.08026503026485443, 0.8754879832267761], [0.3138233423233032, 0.724989116191864, -0.0068575553596019745, 1.0656505823135376], [0.4927557110786438, 0.4665831923484802, 0.5457000732421875, 0.44373881816864014], [0.4286043047904968, 0.5650157928466797, -0.03438287600874901, 0.914689838886261], [-0.04023156687617302, 1.0274161100387573, 0.020412985235452652, 1.0125770568847656], [0.4871100187301636, 0.48980385065078735, -0.024354305118322372, 0.9926795363426208], [0.4393797516822815, 0.6092626452445984, -0.0503314845263958, 1.040976643562317], [0.39634567499160767, 0.6731142401695251, 0.30240386724472046, 0.7293391823768616], [-0.0793527215719223, 1.0272775888442993, 0.40042316913604736, 0.6546253561973572], [-0.040432628244161606, 0.985545814037323, 0.3108283281326294, 0.6712157726287842], [0.23698365688323975, 0.6146253943443298, 0.01617550477385521, 0.9788640141487122], [-0.017991792410612106, 1.106022834777832, 0.509413480758667, 0.4739859104156494], [0.506645143032074, 0.49443966150283813, 0.2657829821109772, 0.7075903415679932], [0.3331540822982788, 0.7690363526344299, 0.0009912140667438507, 1.012966275215149], [0.0035591013729572296, 0.9972575306892395, -0.006878342479467392, 1.093258261680603], [0.49123644828796387, 0.4907799959182739, 0.5769304037094116, 0.450234591960907], [-0.03705750033259392, 1.0233534574508667, 0.24717363715171814, 0.7397957444190979], [0.3074831962585449, 0.633054792881012, 0.0480886735022068, 0.9311363101005554]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.3923269510269165, 0.5683017373085022]
[0.007097501307725906, 0.9994974732398987]
[0.40013259649276733, 0.6164824962615967]
[0.30940932035446167, 0.761271059513092]
This is the real loss :  4.916435707360506
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012], [-0.010541331022977829, 1.027101755142212, 0.27839386463165283, 0.6909443736076355], [0.272367000579834, 0.7058614492416382, 0.00028115883469581604, 1.0182385444641113], [0.28895533084869385, 0.6804113388061523, 0.39941537380218506, 0.6184879541397095], [0.011353697627782822, 0.9631422162055969, -0.06577624380588531, 1.0429608821868896], [-0.0018919818103313446, 0.9945912957191467, 0.42653173208236694, 0.6086744070053101], [0.01611100509762764, 1.0032296180725098, 0.023631710559129715, 0.9491965770721436], [0.03927203640341759, 1.001900553703308, 0.3045749366283417, 0.7350011467933655], [-0.05471464619040489, 1.0141152143478394, -0.021505441516637802, 0.9882152080535889], [0.5345359444618225, 0.4880406856536865, -0.019061248749494553, 0.9891769289970398], [-0.03129149600863457, 1.1256051063537598, 0.4196738004684448, 0.5263700485229492], [0.4761011004447937, 0.47544240951538086, 0.5494738221168518, 0.41425636410713196], [0.5106468200683594, 0.47100865840911865, 0.20519176125526428, 0.7133517265319824], [0.016757089644670486, 0.9700817465782166, 0.36863061785697937, 0.6217348575592041], [0.01527344062924385, 0.9615796208381653, 0.5034911632537842, 0.49691423773765564], [0.002861347049474716, 0.9952285885810852, 0.08026503026485443, 0.8754879832267761], [0.3138233423233032, 0.724989116191864, -0.0068575553596019745, 1.0656505823135376], [0.4927557110786438, 0.4665831923484802, 0.5457000732421875, 0.44373881816864014], [0.4286043047904968, 0.5650157928466797, -0.03438287600874901, 0.914689838886261], [-0.04023156687617302, 1.0274161100387573, 0.020412985235452652, 1.0125770568847656], [0.4871100187301636, 0.48980385065078735, -0.024354305118322372, 0.9926795363426208], [0.4393797516822815, 0.6092626452445984, -0.0503314845263958, 1.040976643562317], [0.39634567499160767, 0.6731142401695251, 0.30240386724472046, 0.7293391823768616], [-0.0793527215719223, 1.0272775888442993, 0.40042316913604736, 0.6546253561973572], [-0.040432628244161606, 0.985545814037323, 0.3108283281326294, 0.6712157726287842], [0.23698365688323975, 0.6146253943443298, 0.01617550477385521, 0.9788640141487122], [-0.017991792410612106, 1.106022834777832, 0.509413480758667, 0.4739859104156494], [0.506645143032074, 0.49443966150283813, 0.2657829821109772, 0.7075903415679932], [0.3331540822982788, 0.7690363526344299, 0.0009912140667438507, 1.012966275215149], [0.0035591013729572296, 0.9972575306892395, -0.006878342479467392, 1.093258261680603], [0.49123644828796387, 0.4907799959182739, 0.5769304037094116, 0.450234591960907], [-0.03705750033259392, 1.0233534574508667, 0.24717363715171814, 0.7397957444190979], [0.3074831962585449, 0.633054792881012, 0.0480886735022068, 0.9311363101005554], [0.3923269510269165, 0.5683017373085022, 0.007097501307725906, 0.9994974732398987], [0.40013259649276733, 0.6164824962615967, 0.30940932035446167, 0.761271059513092]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.44519591331481934, 0.5651217699050903]
[0.5453522205352783, 0.45746633410453796]
[0.39753979444503784, 0.5841442942619324]
[0.5160083770751953, 0.46118786931037903]
This is the real loss :  5.204425234347582
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012], [-0.010541331022977829, 1.027101755142212, 0.27839386463165283, 0.6909443736076355], [0.272367000579834, 0.7058614492416382, 0.00028115883469581604, 1.0182385444641113], [0.28895533084869385, 0.6804113388061523, 0.39941537380218506, 0.6184879541397095], [0.011353697627782822, 0.9631422162055969, -0.06577624380588531, 1.0429608821868896], [-0.0018919818103313446, 0.9945912957191467, 0.42653173208236694, 0.6086744070053101], [0.01611100509762764, 1.0032296180725098, 0.023631710559129715, 0.9491965770721436], [0.03927203640341759, 1.001900553703308, 0.3045749366283417, 0.7350011467933655], [-0.05471464619040489, 1.0141152143478394, -0.021505441516637802, 0.9882152080535889], [0.5345359444618225, 0.4880406856536865, -0.019061248749494553, 0.9891769289970398], [-0.03129149600863457, 1.1256051063537598, 0.4196738004684448, 0.5263700485229492], [0.4761011004447937, 0.47544240951538086, 0.5494738221168518, 0.41425636410713196], [0.5106468200683594, 0.47100865840911865, 0.20519176125526428, 0.7133517265319824], [0.016757089644670486, 0.9700817465782166, 0.36863061785697937, 0.6217348575592041], [0.01527344062924385, 0.9615796208381653, 0.5034911632537842, 0.49691423773765564], [0.002861347049474716, 0.9952285885810852, 0.08026503026485443, 0.8754879832267761], [0.3138233423233032, 0.724989116191864, -0.0068575553596019745, 1.0656505823135376], [0.4927557110786438, 0.4665831923484802, 0.5457000732421875, 0.44373881816864014], [0.4286043047904968, 0.5650157928466797, -0.03438287600874901, 0.914689838886261], [-0.04023156687617302, 1.0274161100387573, 0.020412985235452652, 1.0125770568847656], [0.4871100187301636, 0.48980385065078735, -0.024354305118322372, 0.9926795363426208], [0.4393797516822815, 0.6092626452445984, -0.0503314845263958, 1.040976643562317], [0.39634567499160767, 0.6731142401695251, 0.30240386724472046, 0.7293391823768616], [-0.0793527215719223, 1.0272775888442993, 0.40042316913604736, 0.6546253561973572], [-0.040432628244161606, 0.985545814037323, 0.3108283281326294, 0.6712157726287842], [0.23698365688323975, 0.6146253943443298, 0.01617550477385521, 0.9788640141487122], [-0.017991792410612106, 1.106022834777832, 0.509413480758667, 0.4739859104156494], [0.506645143032074, 0.49443966150283813, 0.2657829821109772, 0.7075903415679932], [0.3331540822982788, 0.7690363526344299, 0.0009912140667438507, 1.012966275215149], [0.0035591013729572296, 0.9972575306892395, -0.006878342479467392, 1.093258261680603], [0.49123644828796387, 0.4907799959182739, 0.5769304037094116, 0.450234591960907], [-0.03705750033259392, 1.0233534574508667, 0.24717363715171814, 0.7397957444190979], [0.3074831962585449, 0.633054792881012, 0.0480886735022068, 0.9311363101005554], [0.3923269510269165, 0.5683017373085022, 0.007097501307725906, 0.9994974732398987], [0.40013259649276733, 0.6164824962615967, 0.30940932035446167, 0.761271059513092], [0.44519591331481934, 0.5651217699050903, 0.5453522205352783, 0.45746633410453796], [0.39753979444503784, 0.5841442942619324, 0.5160083770751953, 0.46118786931037903]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.467087984085083, 0.44436103105545044]
[-0.06790859997272491, 1.0497682094573975]
[0.44856369495391846, 0.5039730668067932]
[0.4258047938346863, 0.4810478687286377]
This is the real loss :  5.383408669382334
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012], [-0.010541331022977829, 1.027101755142212, 0.27839386463165283, 0.6909443736076355], [0.272367000579834, 0.7058614492416382, 0.00028115883469581604, 1.0182385444641113], [0.28895533084869385, 0.6804113388061523, 0.39941537380218506, 0.6184879541397095], [0.011353697627782822, 0.9631422162055969, -0.06577624380588531, 1.0429608821868896], [-0.0018919818103313446, 0.9945912957191467, 0.42653173208236694, 0.6086744070053101], [0.01611100509762764, 1.0032296180725098, 0.023631710559129715, 0.9491965770721436], [0.03927203640341759, 1.001900553703308, 0.3045749366283417, 0.7350011467933655], [-0.05471464619040489, 1.0141152143478394, -0.021505441516637802, 0.9882152080535889], [0.5345359444618225, 0.4880406856536865, -0.019061248749494553, 0.9891769289970398], [-0.03129149600863457, 1.1256051063537598, 0.4196738004684448, 0.5263700485229492], [0.4761011004447937, 0.47544240951538086, 0.5494738221168518, 0.41425636410713196], [0.5106468200683594, 0.47100865840911865, 0.20519176125526428, 0.7133517265319824], [0.016757089644670486, 0.9700817465782166, 0.36863061785697937, 0.6217348575592041], [0.01527344062924385, 0.9615796208381653, 0.5034911632537842, 0.49691423773765564], [0.002861347049474716, 0.9952285885810852, 0.08026503026485443, 0.8754879832267761], [0.3138233423233032, 0.724989116191864, -0.0068575553596019745, 1.0656505823135376], [0.4927557110786438, 0.4665831923484802, 0.5457000732421875, 0.44373881816864014], [0.4286043047904968, 0.5650157928466797, -0.03438287600874901, 0.914689838886261], [-0.04023156687617302, 1.0274161100387573, 0.020412985235452652, 1.0125770568847656], [0.4871100187301636, 0.48980385065078735, -0.024354305118322372, 0.9926795363426208], [0.4393797516822815, 0.6092626452445984, -0.0503314845263958, 1.040976643562317], [0.39634567499160767, 0.6731142401695251, 0.30240386724472046, 0.7293391823768616], [-0.0793527215719223, 1.0272775888442993, 0.40042316913604736, 0.6546253561973572], [-0.040432628244161606, 0.985545814037323, 0.3108283281326294, 0.6712157726287842], [0.23698365688323975, 0.6146253943443298, 0.01617550477385521, 0.9788640141487122], [-0.017991792410612106, 1.106022834777832, 0.509413480758667, 0.4739859104156494], [0.506645143032074, 0.49443966150283813, 0.2657829821109772, 0.7075903415679932], [0.3331540822982788, 0.7690363526344299, 0.0009912140667438507, 1.012966275215149], [0.0035591013729572296, 0.9972575306892395, -0.006878342479467392, 1.093258261680603], [0.49123644828796387, 0.4907799959182739, 0.5769304037094116, 0.450234591960907], [-0.03705750033259392, 1.0233534574508667, 0.24717363715171814, 0.7397957444190979], [0.3074831962585449, 0.633054792881012, 0.0480886735022068, 0.9311363101005554], [0.3923269510269165, 0.5683017373085022, 0.007097501307725906, 0.9994974732398987], [0.40013259649276733, 0.6164824962615967, 0.30940932035446167, 0.761271059513092], [0.44519591331481934, 0.5651217699050903, 0.5453522205352783, 0.45746633410453796], [0.39753979444503784, 0.5841442942619324, 0.5160083770751953, 0.46118786931037903], [0.467087984085083, 0.44436103105545044, -0.06790859997272491, 1.0497682094573975], [0.44856369495391846, 0.5039730668067932, 0.4258047938346863, 0.4810478687286377]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.2612181305885315, 0.6843036413192749]
[-0.03724139556288719, 1.0313953161239624]
[0.23633438348770142, 0.7641127705574036]
[0.0019404329359531403, 0.9860630631446838]
This is the real loss :  5.4186544716358185
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012], [-0.010541331022977829, 1.027101755142212, 0.27839386463165283, 0.6909443736076355], [0.272367000579834, 0.7058614492416382, 0.00028115883469581604, 1.0182385444641113], [0.28895533084869385, 0.6804113388061523, 0.39941537380218506, 0.6184879541397095], [0.011353697627782822, 0.9631422162055969, -0.06577624380588531, 1.0429608821868896], [-0.0018919818103313446, 0.9945912957191467, 0.42653173208236694, 0.6086744070053101], [0.01611100509762764, 1.0032296180725098, 0.023631710559129715, 0.9491965770721436], [0.03927203640341759, 1.001900553703308, 0.3045749366283417, 0.7350011467933655], [-0.05471464619040489, 1.0141152143478394, -0.021505441516637802, 0.9882152080535889], [0.5345359444618225, 0.4880406856536865, -0.019061248749494553, 0.9891769289970398], [-0.03129149600863457, 1.1256051063537598, 0.4196738004684448, 0.5263700485229492], [0.4761011004447937, 0.47544240951538086, 0.5494738221168518, 0.41425636410713196], [0.5106468200683594, 0.47100865840911865, 0.20519176125526428, 0.7133517265319824], [0.016757089644670486, 0.9700817465782166, 0.36863061785697937, 0.6217348575592041], [0.01527344062924385, 0.9615796208381653, 0.5034911632537842, 0.49691423773765564], [0.002861347049474716, 0.9952285885810852, 0.08026503026485443, 0.8754879832267761], [0.3138233423233032, 0.724989116191864, -0.0068575553596019745, 1.0656505823135376], [0.4927557110786438, 0.4665831923484802, 0.5457000732421875, 0.44373881816864014], [0.4286043047904968, 0.5650157928466797, -0.03438287600874901, 0.914689838886261], [-0.04023156687617302, 1.0274161100387573, 0.020412985235452652, 1.0125770568847656], [0.4871100187301636, 0.48980385065078735, -0.024354305118322372, 0.9926795363426208], [0.4393797516822815, 0.6092626452445984, -0.0503314845263958, 1.040976643562317], [0.39634567499160767, 0.6731142401695251, 0.30240386724472046, 0.7293391823768616], [-0.0793527215719223, 1.0272775888442993, 0.40042316913604736, 0.6546253561973572], [-0.040432628244161606, 0.985545814037323, 0.3108283281326294, 0.6712157726287842], [0.23698365688323975, 0.6146253943443298, 0.01617550477385521, 0.9788640141487122], [-0.017991792410612106, 1.106022834777832, 0.509413480758667, 0.4739859104156494], [0.506645143032074, 0.49443966150283813, 0.2657829821109772, 0.7075903415679932], [0.3331540822982788, 0.7690363526344299, 0.0009912140667438507, 1.012966275215149], [0.0035591013729572296, 0.9972575306892395, -0.006878342479467392, 1.093258261680603], [0.49123644828796387, 0.4907799959182739, 0.5769304037094116, 0.450234591960907], [-0.03705750033259392, 1.0233534574508667, 0.24717363715171814, 0.7397957444190979], [0.3074831962585449, 0.633054792881012, 0.0480886735022068, 0.9311363101005554], [0.3923269510269165, 0.5683017373085022, 0.007097501307725906, 0.9994974732398987], [0.40013259649276733, 0.6164824962615967, 0.30940932035446167, 0.761271059513092], [0.44519591331481934, 0.5651217699050903, 0.5453522205352783, 0.45746633410453796], [0.39753979444503784, 0.5841442942619324, 0.5160083770751953, 0.46118786931037903], [0.467087984085083, 0.44436103105545044, -0.06790859997272491, 1.0497682094573975], [0.44856369495391846, 0.5039730668067932, 0.4258047938346863, 0.4810478687286377], [0.2612181305885315, 0.6843036413192749, -0.03724139556288719, 1.0313953161239624], [0.23633438348770142, 0.7641127705574036, 0.0019404329359531403, 0.9860630631446838]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.5616891384124756, 0.45800068974494934]
[0.07230783998966217, 0.9294964671134949]
[-0.012544628232717514, 0.9945358633995056]
[0.15663409233093262, 0.8227774500846863]
This is the real loss :  5.503102771937847
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012], [-0.010541331022977829, 1.027101755142212, 0.27839386463165283, 0.6909443736076355], [0.272367000579834, 0.7058614492416382, 0.00028115883469581604, 1.0182385444641113], [0.28895533084869385, 0.6804113388061523, 0.39941537380218506, 0.6184879541397095], [0.011353697627782822, 0.9631422162055969, -0.06577624380588531, 1.0429608821868896], [-0.0018919818103313446, 0.9945912957191467, 0.42653173208236694, 0.6086744070053101], [0.01611100509762764, 1.0032296180725098, 0.023631710559129715, 0.9491965770721436], [0.03927203640341759, 1.001900553703308, 0.3045749366283417, 0.7350011467933655], [-0.05471464619040489, 1.0141152143478394, -0.021505441516637802, 0.9882152080535889], [0.5345359444618225, 0.4880406856536865, -0.019061248749494553, 0.9891769289970398], [-0.03129149600863457, 1.1256051063537598, 0.4196738004684448, 0.5263700485229492], [0.4761011004447937, 0.47544240951538086, 0.5494738221168518, 0.41425636410713196], [0.5106468200683594, 0.47100865840911865, 0.20519176125526428, 0.7133517265319824], [0.016757089644670486, 0.9700817465782166, 0.36863061785697937, 0.6217348575592041], [0.01527344062924385, 0.9615796208381653, 0.5034911632537842, 0.49691423773765564], [0.002861347049474716, 0.9952285885810852, 0.08026503026485443, 0.8754879832267761], [0.3138233423233032, 0.724989116191864, -0.0068575553596019745, 1.0656505823135376], [0.4927557110786438, 0.4665831923484802, 0.5457000732421875, 0.44373881816864014], [0.4286043047904968, 0.5650157928466797, -0.03438287600874901, 0.914689838886261], [-0.04023156687617302, 1.0274161100387573, 0.020412985235452652, 1.0125770568847656], [0.4871100187301636, 0.48980385065078735, -0.024354305118322372, 0.9926795363426208], [0.4393797516822815, 0.6092626452445984, -0.0503314845263958, 1.040976643562317], [0.39634567499160767, 0.6731142401695251, 0.30240386724472046, 0.7293391823768616], [-0.0793527215719223, 1.0272775888442993, 0.40042316913604736, 0.6546253561973572], [-0.040432628244161606, 0.985545814037323, 0.3108283281326294, 0.6712157726287842], [0.23698365688323975, 0.6146253943443298, 0.01617550477385521, 0.9788640141487122], [-0.017991792410612106, 1.106022834777832, 0.509413480758667, 0.4739859104156494], [0.506645143032074, 0.49443966150283813, 0.2657829821109772, 0.7075903415679932], [0.3331540822982788, 0.7690363526344299, 0.0009912140667438507, 1.012966275215149], [0.0035591013729572296, 0.9972575306892395, -0.006878342479467392, 1.093258261680603], [0.49123644828796387, 0.4907799959182739, 0.5769304037094116, 0.450234591960907], [-0.03705750033259392, 1.0233534574508667, 0.24717363715171814, 0.7397957444190979], [0.3074831962585449, 0.633054792881012, 0.0480886735022068, 0.9311363101005554], [0.3923269510269165, 0.5683017373085022, 0.007097501307725906, 0.9994974732398987], [0.40013259649276733, 0.6164824962615967, 0.30940932035446167, 0.761271059513092], [0.44519591331481934, 0.5651217699050903, 0.5453522205352783, 0.45746633410453796], [0.39753979444503784, 0.5841442942619324, 0.5160083770751953, 0.46118786931037903], [0.467087984085083, 0.44436103105545044, -0.06790859997272491, 1.0497682094573975], [0.44856369495391846, 0.5039730668067932, 0.4258047938346863, 0.4810478687286377], [0.2612181305885315, 0.6843036413192749, -0.03724139556288719, 1.0313953161239624], [0.23633438348770142, 0.7641127705574036, 0.0019404329359531403, 0.9860630631446838], [0.5616891384124756, 0.45800068974494934, 0.07230783998966217, 0.9294964671134949], [-0.012544628232717514, 0.9945358633995056, 0.15663409233093262, 0.8227774500846863]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.05776740983128548, 0.8698195815086365]
[0.4657704830169678, 0.5362539291381836]
[0.2573600113391876, 0.7085058689117432]
[0.47870194911956787, 0.5151973962783813]
This is the real loss :  5.654183469712734
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012], [-0.010541331022977829, 1.027101755142212, 0.27839386463165283, 0.6909443736076355], [0.272367000579834, 0.7058614492416382, 0.00028115883469581604, 1.0182385444641113], [0.28895533084869385, 0.6804113388061523, 0.39941537380218506, 0.6184879541397095], [0.011353697627782822, 0.9631422162055969, -0.06577624380588531, 1.0429608821868896], [-0.0018919818103313446, 0.9945912957191467, 0.42653173208236694, 0.6086744070053101], [0.01611100509762764, 1.0032296180725098, 0.023631710559129715, 0.9491965770721436], [0.03927203640341759, 1.001900553703308, 0.3045749366283417, 0.7350011467933655], [-0.05471464619040489, 1.0141152143478394, -0.021505441516637802, 0.9882152080535889], [0.5345359444618225, 0.4880406856536865, -0.019061248749494553, 0.9891769289970398], [-0.03129149600863457, 1.1256051063537598, 0.4196738004684448, 0.5263700485229492], [0.4761011004447937, 0.47544240951538086, 0.5494738221168518, 0.41425636410713196], [0.5106468200683594, 0.47100865840911865, 0.20519176125526428, 0.7133517265319824], [0.016757089644670486, 0.9700817465782166, 0.36863061785697937, 0.6217348575592041], [0.01527344062924385, 0.9615796208381653, 0.5034911632537842, 0.49691423773765564], [0.002861347049474716, 0.9952285885810852, 0.08026503026485443, 0.8754879832267761], [0.3138233423233032, 0.724989116191864, -0.0068575553596019745, 1.0656505823135376], [0.4927557110786438, 0.4665831923484802, 0.5457000732421875, 0.44373881816864014], [0.4286043047904968, 0.5650157928466797, -0.03438287600874901, 0.914689838886261], [-0.04023156687617302, 1.0274161100387573, 0.020412985235452652, 1.0125770568847656], [0.4871100187301636, 0.48980385065078735, -0.024354305118322372, 0.9926795363426208], [0.4393797516822815, 0.6092626452445984, -0.0503314845263958, 1.040976643562317], [0.39634567499160767, 0.6731142401695251, 0.30240386724472046, 0.7293391823768616], [-0.0793527215719223, 1.0272775888442993, 0.40042316913604736, 0.6546253561973572], [-0.040432628244161606, 0.985545814037323, 0.3108283281326294, 0.6712157726287842], [0.23698365688323975, 0.6146253943443298, 0.01617550477385521, 0.9788640141487122], [-0.017991792410612106, 1.106022834777832, 0.509413480758667, 0.4739859104156494], [0.506645143032074, 0.49443966150283813, 0.2657829821109772, 0.7075903415679932], [0.3331540822982788, 0.7690363526344299, 0.0009912140667438507, 1.012966275215149], [0.0035591013729572296, 0.9972575306892395, -0.006878342479467392, 1.093258261680603], [0.49123644828796387, 0.4907799959182739, 0.5769304037094116, 0.450234591960907], [-0.03705750033259392, 1.0233534574508667, 0.24717363715171814, 0.7397957444190979], [0.3074831962585449, 0.633054792881012, 0.0480886735022068, 0.9311363101005554], [0.3923269510269165, 0.5683017373085022, 0.007097501307725906, 0.9994974732398987], [0.40013259649276733, 0.6164824962615967, 0.30940932035446167, 0.761271059513092], [0.44519591331481934, 0.5651217699050903, 0.5453522205352783, 0.45746633410453796], [0.39753979444503784, 0.5841442942619324, 0.5160083770751953, 0.46118786931037903], [0.467087984085083, 0.44436103105545044, -0.06790859997272491, 1.0497682094573975], [0.44856369495391846, 0.5039730668067932, 0.4258047938346863, 0.4810478687286377], [0.2612181305885315, 0.6843036413192749, -0.03724139556288719, 1.0313953161239624], [0.23633438348770142, 0.7641127705574036, 0.0019404329359531403, 0.9860630631446838], [0.5616891384124756, 0.45800068974494934, 0.07230783998966217, 0.9294964671134949], [-0.012544628232717514, 0.9945358633995056, 0.15663409233093262, 0.8227774500846863], [0.05776740983128548, 0.8698195815086365, 0.4657704830169678, 0.5362539291381836], [0.2573600113391876, 0.7085058689117432, 0.47870194911956787, 0.5151973962783813]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.015174712985754013, 1.0285004377365112]
[0.5360817909240723, 0.5033567547798157]
[0.012863870710134506, 0.9657784700393677]
[0.5068503618240356, 0.4983740448951721]
This is the real loss :  6.020910941064358
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012], [-0.010541331022977829, 1.027101755142212, 0.27839386463165283, 0.6909443736076355], [0.272367000579834, 0.7058614492416382, 0.00028115883469581604, 1.0182385444641113], [0.28895533084869385, 0.6804113388061523, 0.39941537380218506, 0.6184879541397095], [0.011353697627782822, 0.9631422162055969, -0.06577624380588531, 1.0429608821868896], [-0.0018919818103313446, 0.9945912957191467, 0.42653173208236694, 0.6086744070053101], [0.01611100509762764, 1.0032296180725098, 0.023631710559129715, 0.9491965770721436], [0.03927203640341759, 1.001900553703308, 0.3045749366283417, 0.7350011467933655], [-0.05471464619040489, 1.0141152143478394, -0.021505441516637802, 0.9882152080535889], [0.5345359444618225, 0.4880406856536865, -0.019061248749494553, 0.9891769289970398], [-0.03129149600863457, 1.1256051063537598, 0.4196738004684448, 0.5263700485229492], [0.4761011004447937, 0.47544240951538086, 0.5494738221168518, 0.41425636410713196], [0.5106468200683594, 0.47100865840911865, 0.20519176125526428, 0.7133517265319824], [0.016757089644670486, 0.9700817465782166, 0.36863061785697937, 0.6217348575592041], [0.01527344062924385, 0.9615796208381653, 0.5034911632537842, 0.49691423773765564], [0.002861347049474716, 0.9952285885810852, 0.08026503026485443, 0.8754879832267761], [0.3138233423233032, 0.724989116191864, -0.0068575553596019745, 1.0656505823135376], [0.4927557110786438, 0.4665831923484802, 0.5457000732421875, 0.44373881816864014], [0.4286043047904968, 0.5650157928466797, -0.03438287600874901, 0.914689838886261], [-0.04023156687617302, 1.0274161100387573, 0.020412985235452652, 1.0125770568847656], [0.4871100187301636, 0.48980385065078735, -0.024354305118322372, 0.9926795363426208], [0.4393797516822815, 0.6092626452445984, -0.0503314845263958, 1.040976643562317], [0.39634567499160767, 0.6731142401695251, 0.30240386724472046, 0.7293391823768616], [-0.0793527215719223, 1.0272775888442993, 0.40042316913604736, 0.6546253561973572], [-0.040432628244161606, 0.985545814037323, 0.3108283281326294, 0.6712157726287842], [0.23698365688323975, 0.6146253943443298, 0.01617550477385521, 0.9788640141487122], [-0.017991792410612106, 1.106022834777832, 0.509413480758667, 0.4739859104156494], [0.506645143032074, 0.49443966150283813, 0.2657829821109772, 0.7075903415679932], [0.3331540822982788, 0.7690363526344299, 0.0009912140667438507, 1.012966275215149], [0.0035591013729572296, 0.9972575306892395, -0.006878342479467392, 1.093258261680603], [0.49123644828796387, 0.4907799959182739, 0.5769304037094116, 0.450234591960907], [-0.03705750033259392, 1.0233534574508667, 0.24717363715171814, 0.7397957444190979], [0.3074831962585449, 0.633054792881012, 0.0480886735022068, 0.9311363101005554], [0.3923269510269165, 0.5683017373085022, 0.007097501307725906, 0.9994974732398987], [0.40013259649276733, 0.6164824962615967, 0.30940932035446167, 0.761271059513092], [0.44519591331481934, 0.5651217699050903, 0.5453522205352783, 0.45746633410453796], [0.39753979444503784, 0.5841442942619324, 0.5160083770751953, 0.46118786931037903], [0.467087984085083, 0.44436103105545044, -0.06790859997272491, 1.0497682094573975], [0.44856369495391846, 0.5039730668067932, 0.4258047938346863, 0.4810478687286377], [0.2612181305885315, 0.6843036413192749, -0.03724139556288719, 1.0313953161239624], [0.23633438348770142, 0.7641127705574036, 0.0019404329359531403, 0.9860630631446838], [0.5616891384124756, 0.45800068974494934, 0.07230783998966217, 0.9294964671134949], [-0.012544628232717514, 0.9945358633995056, 0.15663409233093262, 0.8227774500846863], [0.05776740983128548, 0.8698195815086365, 0.4657704830169678, 0.5362539291381836], [0.2573600113391876, 0.7085058689117432, 0.47870194911956787, 0.5151973962783813], [-0.015174712985754013, 1.0285004377365112, 0.5360817909240723, 0.5033567547798157], [0.012863870710134506, 0.9657784700393677, 0.5068503618240356, 0.4983740448951721]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.3272557258605957, 0.7253463864326477]
[-0.06722380220890045, 1.0791971683502197]
[0.534986138343811, 0.47704941034317017]
[0.5005849003791809, 0.4649272561073303]
This is the real loss :  6.258272223174572
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012], [-0.010541331022977829, 1.027101755142212, 0.27839386463165283, 0.6909443736076355], [0.272367000579834, 0.7058614492416382, 0.00028115883469581604, 1.0182385444641113], [0.28895533084869385, 0.6804113388061523, 0.39941537380218506, 0.6184879541397095], [0.011353697627782822, 0.9631422162055969, -0.06577624380588531, 1.0429608821868896], [-0.0018919818103313446, 0.9945912957191467, 0.42653173208236694, 0.6086744070053101], [0.01611100509762764, 1.0032296180725098, 0.023631710559129715, 0.9491965770721436], [0.03927203640341759, 1.001900553703308, 0.3045749366283417, 0.7350011467933655], [-0.05471464619040489, 1.0141152143478394, -0.021505441516637802, 0.9882152080535889], [0.5345359444618225, 0.4880406856536865, -0.019061248749494553, 0.9891769289970398], [-0.03129149600863457, 1.1256051063537598, 0.4196738004684448, 0.5263700485229492], [0.4761011004447937, 0.47544240951538086, 0.5494738221168518, 0.41425636410713196], [0.5106468200683594, 0.47100865840911865, 0.20519176125526428, 0.7133517265319824], [0.016757089644670486, 0.9700817465782166, 0.36863061785697937, 0.6217348575592041], [0.01527344062924385, 0.9615796208381653, 0.5034911632537842, 0.49691423773765564], [0.002861347049474716, 0.9952285885810852, 0.08026503026485443, 0.8754879832267761], [0.3138233423233032, 0.724989116191864, -0.0068575553596019745, 1.0656505823135376], [0.4927557110786438, 0.4665831923484802, 0.5457000732421875, 0.44373881816864014], [0.4286043047904968, 0.5650157928466797, -0.03438287600874901, 0.914689838886261], [-0.04023156687617302, 1.0274161100387573, 0.020412985235452652, 1.0125770568847656], [0.4871100187301636, 0.48980385065078735, -0.024354305118322372, 0.9926795363426208], [0.4393797516822815, 0.6092626452445984, -0.0503314845263958, 1.040976643562317], [0.39634567499160767, 0.6731142401695251, 0.30240386724472046, 0.7293391823768616], [-0.0793527215719223, 1.0272775888442993, 0.40042316913604736, 0.6546253561973572], [-0.040432628244161606, 0.985545814037323, 0.3108283281326294, 0.6712157726287842], [0.23698365688323975, 0.6146253943443298, 0.01617550477385521, 0.9788640141487122], [-0.017991792410612106, 1.106022834777832, 0.509413480758667, 0.4739859104156494], [0.506645143032074, 0.49443966150283813, 0.2657829821109772, 0.7075903415679932], [0.3331540822982788, 0.7690363526344299, 0.0009912140667438507, 1.012966275215149], [0.0035591013729572296, 0.9972575306892395, -0.006878342479467392, 1.093258261680603], [0.49123644828796387, 0.4907799959182739, 0.5769304037094116, 0.450234591960907], [-0.03705750033259392, 1.0233534574508667, 0.24717363715171814, 0.7397957444190979], [0.3074831962585449, 0.633054792881012, 0.0480886735022068, 0.9311363101005554], [0.3923269510269165, 0.5683017373085022, 0.007097501307725906, 0.9994974732398987], [0.40013259649276733, 0.6164824962615967, 0.30940932035446167, 0.761271059513092], [0.44519591331481934, 0.5651217699050903, 0.5453522205352783, 0.45746633410453796], [0.39753979444503784, 0.5841442942619324, 0.5160083770751953, 0.46118786931037903], [0.467087984085083, 0.44436103105545044, -0.06790859997272491, 1.0497682094573975], [0.44856369495391846, 0.5039730668067932, 0.4258047938346863, 0.4810478687286377], [0.2612181305885315, 0.6843036413192749, -0.03724139556288719, 1.0313953161239624], [0.23633438348770142, 0.7641127705574036, 0.0019404329359531403, 0.9860630631446838], [0.5616891384124756, 0.45800068974494934, 0.07230783998966217, 0.9294964671134949], [-0.012544628232717514, 0.9945358633995056, 0.15663409233093262, 0.8227774500846863], [0.05776740983128548, 0.8698195815086365, 0.4657704830169678, 0.5362539291381836], [0.2573600113391876, 0.7085058689117432, 0.47870194911956787, 0.5151973962783813], [-0.015174712985754013, 1.0285004377365112, 0.5360817909240723, 0.5033567547798157], [0.012863870710134506, 0.9657784700393677, 0.5068503618240356, 0.4983740448951721], [0.3272557258605957, 0.7253463864326477, -0.06722380220890045, 1.0791971683502197], [0.534986138343811, 0.47704941034317017, 0.5005849003791809, 0.4649272561073303]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.4875568747520447, 0.4938989579677582]
[0.5809800624847412, 0.4206404983997345]
[-0.015788819640874863, 0.9971292614936829]
[0.042021218687295914, 0.9797156453132629]
This is the real loss :  6.406042747199535
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012], [-0.010541331022977829, 1.027101755142212, 0.27839386463165283, 0.6909443736076355], [0.272367000579834, 0.7058614492416382, 0.00028115883469581604, 1.0182385444641113], [0.28895533084869385, 0.6804113388061523, 0.39941537380218506, 0.6184879541397095], [0.011353697627782822, 0.9631422162055969, -0.06577624380588531, 1.0429608821868896], [-0.0018919818103313446, 0.9945912957191467, 0.42653173208236694, 0.6086744070053101], [0.01611100509762764, 1.0032296180725098, 0.023631710559129715, 0.9491965770721436], [0.03927203640341759, 1.001900553703308, 0.3045749366283417, 0.7350011467933655], [-0.05471464619040489, 1.0141152143478394, -0.021505441516637802, 0.9882152080535889], [0.5345359444618225, 0.4880406856536865, -0.019061248749494553, 0.9891769289970398], [-0.03129149600863457, 1.1256051063537598, 0.4196738004684448, 0.5263700485229492], [0.4761011004447937, 0.47544240951538086, 0.5494738221168518, 0.41425636410713196], [0.5106468200683594, 0.47100865840911865, 0.20519176125526428, 0.7133517265319824], [0.016757089644670486, 0.9700817465782166, 0.36863061785697937, 0.6217348575592041], [0.01527344062924385, 0.9615796208381653, 0.5034911632537842, 0.49691423773765564], [0.002861347049474716, 0.9952285885810852, 0.08026503026485443, 0.8754879832267761], [0.3138233423233032, 0.724989116191864, -0.0068575553596019745, 1.0656505823135376], [0.4927557110786438, 0.4665831923484802, 0.5457000732421875, 0.44373881816864014], [0.4286043047904968, 0.5650157928466797, -0.03438287600874901, 0.914689838886261], [-0.04023156687617302, 1.0274161100387573, 0.020412985235452652, 1.0125770568847656], [0.4871100187301636, 0.48980385065078735, -0.024354305118322372, 0.9926795363426208], [0.4393797516822815, 0.6092626452445984, -0.0503314845263958, 1.040976643562317], [0.39634567499160767, 0.6731142401695251, 0.30240386724472046, 0.7293391823768616], [-0.0793527215719223, 1.0272775888442993, 0.40042316913604736, 0.6546253561973572], [-0.040432628244161606, 0.985545814037323, 0.3108283281326294, 0.6712157726287842], [0.23698365688323975, 0.6146253943443298, 0.01617550477385521, 0.9788640141487122], [-0.017991792410612106, 1.106022834777832, 0.509413480758667, 0.4739859104156494], [0.506645143032074, 0.49443966150283813, 0.2657829821109772, 0.7075903415679932], [0.3331540822982788, 0.7690363526344299, 0.0009912140667438507, 1.012966275215149], [0.0035591013729572296, 0.9972575306892395, -0.006878342479467392, 1.093258261680603], [0.49123644828796387, 0.4907799959182739, 0.5769304037094116, 0.450234591960907], [-0.03705750033259392, 1.0233534574508667, 0.24717363715171814, 0.7397957444190979], [0.3074831962585449, 0.633054792881012, 0.0480886735022068, 0.9311363101005554], [0.3923269510269165, 0.5683017373085022, 0.007097501307725906, 0.9994974732398987], [0.40013259649276733, 0.6164824962615967, 0.30940932035446167, 0.761271059513092], [0.44519591331481934, 0.5651217699050903, 0.5453522205352783, 0.45746633410453796], [0.39753979444503784, 0.5841442942619324, 0.5160083770751953, 0.46118786931037903], [0.467087984085083, 0.44436103105545044, -0.06790859997272491, 1.0497682094573975], [0.44856369495391846, 0.5039730668067932, 0.4258047938346863, 0.4810478687286377], [0.2612181305885315, 0.6843036413192749, -0.03724139556288719, 1.0313953161239624], [0.23633438348770142, 0.7641127705574036, 0.0019404329359531403, 0.9860630631446838], [0.5616891384124756, 0.45800068974494934, 0.07230783998966217, 0.9294964671134949], [-0.012544628232717514, 0.9945358633995056, 0.15663409233093262, 0.8227774500846863], [0.05776740983128548, 0.8698195815086365, 0.4657704830169678, 0.5362539291381836], [0.2573600113391876, 0.7085058689117432, 0.47870194911956787, 0.5151973962783813], [-0.015174712985754013, 1.0285004377365112, 0.5360817909240723, 0.5033567547798157], [0.012863870710134506, 0.9657784700393677, 0.5068503618240356, 0.4983740448951721], [0.3272557258605957, 0.7253463864326477, -0.06722380220890045, 1.0791971683502197], [0.534986138343811, 0.47704941034317017, 0.5005849003791809, 0.4649272561073303], [0.4875568747520447, 0.4938989579677582, 0.5809800624847412, 0.4206404983997345], [-0.015788819640874863, 0.9971292614936829, 0.042021218687295914, 0.9797156453132629]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.05771462246775627, 1.043553352355957]
[0.0037750937044620514, 1.035496711730957]
[0.5715093612670898, 0.4238245487213135]
[0.022643957287073135, 0.9846598505973816]
This is the real loss :  6.48927416652441
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012], [-0.010541331022977829, 1.027101755142212, 0.27839386463165283, 0.6909443736076355], [0.272367000579834, 0.7058614492416382, 0.00028115883469581604, 1.0182385444641113], [0.28895533084869385, 0.6804113388061523, 0.39941537380218506, 0.6184879541397095], [0.011353697627782822, 0.9631422162055969, -0.06577624380588531, 1.0429608821868896], [-0.0018919818103313446, 0.9945912957191467, 0.42653173208236694, 0.6086744070053101], [0.01611100509762764, 1.0032296180725098, 0.023631710559129715, 0.9491965770721436], [0.03927203640341759, 1.001900553703308, 0.3045749366283417, 0.7350011467933655], [-0.05471464619040489, 1.0141152143478394, -0.021505441516637802, 0.9882152080535889], [0.5345359444618225, 0.4880406856536865, -0.019061248749494553, 0.9891769289970398], [-0.03129149600863457, 1.1256051063537598, 0.4196738004684448, 0.5263700485229492], [0.4761011004447937, 0.47544240951538086, 0.5494738221168518, 0.41425636410713196], [0.5106468200683594, 0.47100865840911865, 0.20519176125526428, 0.7133517265319824], [0.016757089644670486, 0.9700817465782166, 0.36863061785697937, 0.6217348575592041], [0.01527344062924385, 0.9615796208381653, 0.5034911632537842, 0.49691423773765564], [0.002861347049474716, 0.9952285885810852, 0.08026503026485443, 0.8754879832267761], [0.3138233423233032, 0.724989116191864, -0.0068575553596019745, 1.0656505823135376], [0.4927557110786438, 0.4665831923484802, 0.5457000732421875, 0.44373881816864014], [0.4286043047904968, 0.5650157928466797, -0.03438287600874901, 0.914689838886261], [-0.04023156687617302, 1.0274161100387573, 0.020412985235452652, 1.0125770568847656], [0.4871100187301636, 0.48980385065078735, -0.024354305118322372, 0.9926795363426208], [0.4393797516822815, 0.6092626452445984, -0.0503314845263958, 1.040976643562317], [0.39634567499160767, 0.6731142401695251, 0.30240386724472046, 0.7293391823768616], [-0.0793527215719223, 1.0272775888442993, 0.40042316913604736, 0.6546253561973572], [-0.040432628244161606, 0.985545814037323, 0.3108283281326294, 0.6712157726287842], [0.23698365688323975, 0.6146253943443298, 0.01617550477385521, 0.9788640141487122], [-0.017991792410612106, 1.106022834777832, 0.509413480758667, 0.4739859104156494], [0.506645143032074, 0.49443966150283813, 0.2657829821109772, 0.7075903415679932], [0.3331540822982788, 0.7690363526344299, 0.0009912140667438507, 1.012966275215149], [0.0035591013729572296, 0.9972575306892395, -0.006878342479467392, 1.093258261680603], [0.49123644828796387, 0.4907799959182739, 0.5769304037094116, 0.450234591960907], [-0.03705750033259392, 1.0233534574508667, 0.24717363715171814, 0.7397957444190979], [0.3074831962585449, 0.633054792881012, 0.0480886735022068, 0.9311363101005554], [0.3923269510269165, 0.5683017373085022, 0.007097501307725906, 0.9994974732398987], [0.40013259649276733, 0.6164824962615967, 0.30940932035446167, 0.761271059513092], [0.44519591331481934, 0.5651217699050903, 0.5453522205352783, 0.45746633410453796], [0.39753979444503784, 0.5841442942619324, 0.5160083770751953, 0.46118786931037903], [0.467087984085083, 0.44436103105545044, -0.06790859997272491, 1.0497682094573975], [0.44856369495391846, 0.5039730668067932, 0.4258047938346863, 0.4810478687286377], [0.2612181305885315, 0.6843036413192749, -0.03724139556288719, 1.0313953161239624], [0.23633438348770142, 0.7641127705574036, 0.0019404329359531403, 0.9860630631446838], [0.5616891384124756, 0.45800068974494934, 0.07230783998966217, 0.9294964671134949], [-0.012544628232717514, 0.9945358633995056, 0.15663409233093262, 0.8227774500846863], [0.05776740983128548, 0.8698195815086365, 0.4657704830169678, 0.5362539291381836], [0.2573600113391876, 0.7085058689117432, 0.47870194911956787, 0.5151973962783813], [-0.015174712985754013, 1.0285004377365112, 0.5360817909240723, 0.5033567547798157], [0.012863870710134506, 0.9657784700393677, 0.5068503618240356, 0.4983740448951721], [0.3272557258605957, 0.7253463864326477, -0.06722380220890045, 1.0791971683502197], [0.534986138343811, 0.47704941034317017, 0.5005849003791809, 0.4649272561073303], [0.4875568747520447, 0.4938989579677582, 0.5809800624847412, 0.4206404983997345], [-0.015788819640874863, 0.9971292614936829, 0.042021218687295914, 0.9797156453132629], [-0.05771462246775627, 1.043553352355957, 0.0037750937044620514, 1.035496711730957], [0.5715093612670898, 0.4238245487213135, 0.022643957287073135, 0.9846598505973816]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.006053198128938675, 1.0155829191207886]
[0.5531402826309204, 0.47858691215515137]
[0.022380132228136063, 0.9865034222602844]
[0.4616469144821167, 0.4835014343261719]
This is the real loss :  6.627073667943478
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012], [-0.010541331022977829, 1.027101755142212, 0.27839386463165283, 0.6909443736076355], [0.272367000579834, 0.7058614492416382, 0.00028115883469581604, 1.0182385444641113], [0.28895533084869385, 0.6804113388061523, 0.39941537380218506, 0.6184879541397095], [0.011353697627782822, 0.9631422162055969, -0.06577624380588531, 1.0429608821868896], [-0.0018919818103313446, 0.9945912957191467, 0.42653173208236694, 0.6086744070053101], [0.01611100509762764, 1.0032296180725098, 0.023631710559129715, 0.9491965770721436], [0.03927203640341759, 1.001900553703308, 0.3045749366283417, 0.7350011467933655], [-0.05471464619040489, 1.0141152143478394, -0.021505441516637802, 0.9882152080535889], [0.5345359444618225, 0.4880406856536865, -0.019061248749494553, 0.9891769289970398], [-0.03129149600863457, 1.1256051063537598, 0.4196738004684448, 0.5263700485229492], [0.4761011004447937, 0.47544240951538086, 0.5494738221168518, 0.41425636410713196], [0.5106468200683594, 0.47100865840911865, 0.20519176125526428, 0.7133517265319824], [0.016757089644670486, 0.9700817465782166, 0.36863061785697937, 0.6217348575592041], [0.01527344062924385, 0.9615796208381653, 0.5034911632537842, 0.49691423773765564], [0.002861347049474716, 0.9952285885810852, 0.08026503026485443, 0.8754879832267761], [0.3138233423233032, 0.724989116191864, -0.0068575553596019745, 1.0656505823135376], [0.4927557110786438, 0.4665831923484802, 0.5457000732421875, 0.44373881816864014], [0.4286043047904968, 0.5650157928466797, -0.03438287600874901, 0.914689838886261], [-0.04023156687617302, 1.0274161100387573, 0.020412985235452652, 1.0125770568847656], [0.4871100187301636, 0.48980385065078735, -0.024354305118322372, 0.9926795363426208], [0.4393797516822815, 0.6092626452445984, -0.0503314845263958, 1.040976643562317], [0.39634567499160767, 0.6731142401695251, 0.30240386724472046, 0.7293391823768616], [-0.0793527215719223, 1.0272775888442993, 0.40042316913604736, 0.6546253561973572], [-0.040432628244161606, 0.985545814037323, 0.3108283281326294, 0.6712157726287842], [0.23698365688323975, 0.6146253943443298, 0.01617550477385521, 0.9788640141487122], [-0.017991792410612106, 1.106022834777832, 0.509413480758667, 0.4739859104156494], [0.506645143032074, 0.49443966150283813, 0.2657829821109772, 0.7075903415679932], [0.3331540822982788, 0.7690363526344299, 0.0009912140667438507, 1.012966275215149], [0.0035591013729572296, 0.9972575306892395, -0.006878342479467392, 1.093258261680603], [0.49123644828796387, 0.4907799959182739, 0.5769304037094116, 0.450234591960907], [-0.03705750033259392, 1.0233534574508667, 0.24717363715171814, 0.7397957444190979], [0.3074831962585449, 0.633054792881012, 0.0480886735022068, 0.9311363101005554], [0.3923269510269165, 0.5683017373085022, 0.007097501307725906, 0.9994974732398987], [0.40013259649276733, 0.6164824962615967, 0.30940932035446167, 0.761271059513092], [0.44519591331481934, 0.5651217699050903, 0.5453522205352783, 0.45746633410453796], [0.39753979444503784, 0.5841442942619324, 0.5160083770751953, 0.46118786931037903], [0.467087984085083, 0.44436103105545044, -0.06790859997272491, 1.0497682094573975], [0.44856369495391846, 0.5039730668067932, 0.4258047938346863, 0.4810478687286377], [0.2612181305885315, 0.6843036413192749, -0.03724139556288719, 1.0313953161239624], [0.23633438348770142, 0.7641127705574036, 0.0019404329359531403, 0.9860630631446838], [0.5616891384124756, 0.45800068974494934, 0.07230783998966217, 0.9294964671134949], [-0.012544628232717514, 0.9945358633995056, 0.15663409233093262, 0.8227774500846863], [0.05776740983128548, 0.8698195815086365, 0.4657704830169678, 0.5362539291381836], [0.2573600113391876, 0.7085058689117432, 0.47870194911956787, 0.5151973962783813], [-0.015174712985754013, 1.0285004377365112, 0.5360817909240723, 0.5033567547798157], [0.012863870710134506, 0.9657784700393677, 0.5068503618240356, 0.4983740448951721], [0.3272557258605957, 0.7253463864326477, -0.06722380220890045, 1.0791971683502197], [0.534986138343811, 0.47704941034317017, 0.5005849003791809, 0.4649272561073303], [0.4875568747520447, 0.4938989579677582, 0.5809800624847412, 0.4206404983997345], [-0.015788819640874863, 0.9971292614936829, 0.042021218687295914, 0.9797156453132629], [-0.05771462246775627, 1.043553352355957, 0.0037750937044620514, 1.035496711730957], [0.5715093612670898, 0.4238245487213135, 0.022643957287073135, 0.9846598505973816], [0.006053198128938675, 1.0155829191207886, 0.5531402826309204, 0.47858691215515137], [0.022380132228136063, 0.9865034222602844, 0.4616469144821167, 0.4835014343261719]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.021657999604940414, 1.0445481538772583]
[0.01180225983262062, 1.022977590560913]
[0.07475757598876953, 0.8564907908439636]
[-0.009597290307283401, 0.9714941382408142]
This is the real loss :  6.826283119618893
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012], [-0.010541331022977829, 1.027101755142212, 0.27839386463165283, 0.6909443736076355], [0.272367000579834, 0.7058614492416382, 0.00028115883469581604, 1.0182385444641113], [0.28895533084869385, 0.6804113388061523, 0.39941537380218506, 0.6184879541397095], [0.011353697627782822, 0.9631422162055969, -0.06577624380588531, 1.0429608821868896], [-0.0018919818103313446, 0.9945912957191467, 0.42653173208236694, 0.6086744070053101], [0.01611100509762764, 1.0032296180725098, 0.023631710559129715, 0.9491965770721436], [0.03927203640341759, 1.001900553703308, 0.3045749366283417, 0.7350011467933655], [-0.05471464619040489, 1.0141152143478394, -0.021505441516637802, 0.9882152080535889], [0.5345359444618225, 0.4880406856536865, -0.019061248749494553, 0.9891769289970398], [-0.03129149600863457, 1.1256051063537598, 0.4196738004684448, 0.5263700485229492], [0.4761011004447937, 0.47544240951538086, 0.5494738221168518, 0.41425636410713196], [0.5106468200683594, 0.47100865840911865, 0.20519176125526428, 0.7133517265319824], [0.016757089644670486, 0.9700817465782166, 0.36863061785697937, 0.6217348575592041], [0.01527344062924385, 0.9615796208381653, 0.5034911632537842, 0.49691423773765564], [0.002861347049474716, 0.9952285885810852, 0.08026503026485443, 0.8754879832267761], [0.3138233423233032, 0.724989116191864, -0.0068575553596019745, 1.0656505823135376], [0.4927557110786438, 0.4665831923484802, 0.5457000732421875, 0.44373881816864014], [0.4286043047904968, 0.5650157928466797, -0.03438287600874901, 0.914689838886261], [-0.04023156687617302, 1.0274161100387573, 0.020412985235452652, 1.0125770568847656], [0.4871100187301636, 0.48980385065078735, -0.024354305118322372, 0.9926795363426208], [0.4393797516822815, 0.6092626452445984, -0.0503314845263958, 1.040976643562317], [0.39634567499160767, 0.6731142401695251, 0.30240386724472046, 0.7293391823768616], [-0.0793527215719223, 1.0272775888442993, 0.40042316913604736, 0.6546253561973572], [-0.040432628244161606, 0.985545814037323, 0.3108283281326294, 0.6712157726287842], [0.23698365688323975, 0.6146253943443298, 0.01617550477385521, 0.9788640141487122], [-0.017991792410612106, 1.106022834777832, 0.509413480758667, 0.4739859104156494], [0.506645143032074, 0.49443966150283813, 0.2657829821109772, 0.7075903415679932], [0.3331540822982788, 0.7690363526344299, 0.0009912140667438507, 1.012966275215149], [0.0035591013729572296, 0.9972575306892395, -0.006878342479467392, 1.093258261680603], [0.49123644828796387, 0.4907799959182739, 0.5769304037094116, 0.450234591960907], [-0.03705750033259392, 1.0233534574508667, 0.24717363715171814, 0.7397957444190979], [0.3074831962585449, 0.633054792881012, 0.0480886735022068, 0.9311363101005554], [0.3923269510269165, 0.5683017373085022, 0.007097501307725906, 0.9994974732398987], [0.40013259649276733, 0.6164824962615967, 0.30940932035446167, 0.761271059513092], [0.44519591331481934, 0.5651217699050903, 0.5453522205352783, 0.45746633410453796], [0.39753979444503784, 0.5841442942619324, 0.5160083770751953, 0.46118786931037903], [0.467087984085083, 0.44436103105545044, -0.06790859997272491, 1.0497682094573975], [0.44856369495391846, 0.5039730668067932, 0.4258047938346863, 0.4810478687286377], [0.2612181305885315, 0.6843036413192749, -0.03724139556288719, 1.0313953161239624], [0.23633438348770142, 0.7641127705574036, 0.0019404329359531403, 0.9860630631446838], [0.5616891384124756, 0.45800068974494934, 0.07230783998966217, 0.9294964671134949], [-0.012544628232717514, 0.9945358633995056, 0.15663409233093262, 0.8227774500846863], [0.05776740983128548, 0.8698195815086365, 0.4657704830169678, 0.5362539291381836], [0.2573600113391876, 0.7085058689117432, 0.47870194911956787, 0.5151973962783813], [-0.015174712985754013, 1.0285004377365112, 0.5360817909240723, 0.5033567547798157], [0.012863870710134506, 0.9657784700393677, 0.5068503618240356, 0.4983740448951721], [0.3272557258605957, 0.7253463864326477, -0.06722380220890045, 1.0791971683502197], [0.534986138343811, 0.47704941034317017, 0.5005849003791809, 0.4649272561073303], [0.4875568747520447, 0.4938989579677582, 0.5809800624847412, 0.4206404983997345], [-0.015788819640874863, 0.9971292614936829, 0.042021218687295914, 0.9797156453132629], [-0.05771462246775627, 1.043553352355957, 0.0037750937044620514, 1.035496711730957], [0.5715093612670898, 0.4238245487213135, 0.022643957287073135, 0.9846598505973816], [0.006053198128938675, 1.0155829191207886, 0.5531402826309204, 0.47858691215515137], [0.022380132228136063, 0.9865034222602844, 0.4616469144821167, 0.4835014343261719], [-0.021657999604940414, 1.0445481538772583, 0.01180225983262062, 1.022977590560913], [0.07475757598876953, 0.8564907908439636, -0.009597290307283401, 0.9714941382408142]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.02613074705004692, 1.161534070968628]
[0.3728724718093872, 0.6328967213630676]
[0.31629616022109985, 0.6707234382629395]
[0.37125736474990845, 0.6814730763435364]
This is the real loss :  7.085985444486141
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012], [-0.010541331022977829, 1.027101755142212, 0.27839386463165283, 0.6909443736076355], [0.272367000579834, 0.7058614492416382, 0.00028115883469581604, 1.0182385444641113], [0.28895533084869385, 0.6804113388061523, 0.39941537380218506, 0.6184879541397095], [0.011353697627782822, 0.9631422162055969, -0.06577624380588531, 1.0429608821868896], [-0.0018919818103313446, 0.9945912957191467, 0.42653173208236694, 0.6086744070053101], [0.01611100509762764, 1.0032296180725098, 0.023631710559129715, 0.9491965770721436], [0.03927203640341759, 1.001900553703308, 0.3045749366283417, 0.7350011467933655], [-0.05471464619040489, 1.0141152143478394, -0.021505441516637802, 0.9882152080535889], [0.5345359444618225, 0.4880406856536865, -0.019061248749494553, 0.9891769289970398], [-0.03129149600863457, 1.1256051063537598, 0.4196738004684448, 0.5263700485229492], [0.4761011004447937, 0.47544240951538086, 0.5494738221168518, 0.41425636410713196], [0.5106468200683594, 0.47100865840911865, 0.20519176125526428, 0.7133517265319824], [0.016757089644670486, 0.9700817465782166, 0.36863061785697937, 0.6217348575592041], [0.01527344062924385, 0.9615796208381653, 0.5034911632537842, 0.49691423773765564], [0.002861347049474716, 0.9952285885810852, 0.08026503026485443, 0.8754879832267761], [0.3138233423233032, 0.724989116191864, -0.0068575553596019745, 1.0656505823135376], [0.4927557110786438, 0.4665831923484802, 0.5457000732421875, 0.44373881816864014], [0.4286043047904968, 0.5650157928466797, -0.03438287600874901, 0.914689838886261], [-0.04023156687617302, 1.0274161100387573, 0.020412985235452652, 1.0125770568847656], [0.4871100187301636, 0.48980385065078735, -0.024354305118322372, 0.9926795363426208], [0.4393797516822815, 0.6092626452445984, -0.0503314845263958, 1.040976643562317], [0.39634567499160767, 0.6731142401695251, 0.30240386724472046, 0.7293391823768616], [-0.0793527215719223, 1.0272775888442993, 0.40042316913604736, 0.6546253561973572], [-0.040432628244161606, 0.985545814037323, 0.3108283281326294, 0.6712157726287842], [0.23698365688323975, 0.6146253943443298, 0.01617550477385521, 0.9788640141487122], [-0.017991792410612106, 1.106022834777832, 0.509413480758667, 0.4739859104156494], [0.506645143032074, 0.49443966150283813, 0.2657829821109772, 0.7075903415679932], [0.3331540822982788, 0.7690363526344299, 0.0009912140667438507, 1.012966275215149], [0.0035591013729572296, 0.9972575306892395, -0.006878342479467392, 1.093258261680603], [0.49123644828796387, 0.4907799959182739, 0.5769304037094116, 0.450234591960907], [-0.03705750033259392, 1.0233534574508667, 0.24717363715171814, 0.7397957444190979], [0.3074831962585449, 0.633054792881012, 0.0480886735022068, 0.9311363101005554], [0.3923269510269165, 0.5683017373085022, 0.007097501307725906, 0.9994974732398987], [0.40013259649276733, 0.6164824962615967, 0.30940932035446167, 0.761271059513092], [0.44519591331481934, 0.5651217699050903, 0.5453522205352783, 0.45746633410453796], [0.39753979444503784, 0.5841442942619324, 0.5160083770751953, 0.46118786931037903], [0.467087984085083, 0.44436103105545044, -0.06790859997272491, 1.0497682094573975], [0.44856369495391846, 0.5039730668067932, 0.4258047938346863, 0.4810478687286377], [0.2612181305885315, 0.6843036413192749, -0.03724139556288719, 1.0313953161239624], [0.23633438348770142, 0.7641127705574036, 0.0019404329359531403, 0.9860630631446838], [0.5616891384124756, 0.45800068974494934, 0.07230783998966217, 0.9294964671134949], [-0.012544628232717514, 0.9945358633995056, 0.15663409233093262, 0.8227774500846863], [0.05776740983128548, 0.8698195815086365, 0.4657704830169678, 0.5362539291381836], [0.2573600113391876, 0.7085058689117432, 0.47870194911956787, 0.5151973962783813], [-0.015174712985754013, 1.0285004377365112, 0.5360817909240723, 0.5033567547798157], [0.012863870710134506, 0.9657784700393677, 0.5068503618240356, 0.4983740448951721], [0.3272557258605957, 0.7253463864326477, -0.06722380220890045, 1.0791971683502197], [0.534986138343811, 0.47704941034317017, 0.5005849003791809, 0.4649272561073303], [0.4875568747520447, 0.4938989579677582, 0.5809800624847412, 0.4206404983997345], [-0.015788819640874863, 0.9971292614936829, 0.042021218687295914, 0.9797156453132629], [-0.05771462246775627, 1.043553352355957, 0.0037750937044620514, 1.035496711730957], [0.5715093612670898, 0.4238245487213135, 0.022643957287073135, 0.9846598505973816], [0.006053198128938675, 1.0155829191207886, 0.5531402826309204, 0.47858691215515137], [0.022380132228136063, 0.9865034222602844, 0.4616469144821167, 0.4835014343261719], [-0.021657999604940414, 1.0445481538772583, 0.01180225983262062, 1.022977590560913], [0.07475757598876953, 0.8564907908439636, -0.009597290307283401, 0.9714941382408142], [-0.02613074705004692, 1.161534070968628, 0.3728724718093872, 0.6328967213630676], [0.31629616022109985, 0.6707234382629395, 0.37125736474990845, 0.6814730763435364]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.3212659955024719, 0.7064064145088196]
[0.08746406435966492, 0.7983171343803406]
[-0.025685545057058334, 0.9569680094718933]
[0.03536762669682503, 0.959369957447052]
This is the real loss :  7.2940922155976295
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012], [-0.010541331022977829, 1.027101755142212, 0.27839386463165283, 0.6909443736076355], [0.272367000579834, 0.7058614492416382, 0.00028115883469581604, 1.0182385444641113], [0.28895533084869385, 0.6804113388061523, 0.39941537380218506, 0.6184879541397095], [0.011353697627782822, 0.9631422162055969, -0.06577624380588531, 1.0429608821868896], [-0.0018919818103313446, 0.9945912957191467, 0.42653173208236694, 0.6086744070053101], [0.01611100509762764, 1.0032296180725098, 0.023631710559129715, 0.9491965770721436], [0.03927203640341759, 1.001900553703308, 0.3045749366283417, 0.7350011467933655], [-0.05471464619040489, 1.0141152143478394, -0.021505441516637802, 0.9882152080535889], [0.5345359444618225, 0.4880406856536865, -0.019061248749494553, 0.9891769289970398], [-0.03129149600863457, 1.1256051063537598, 0.4196738004684448, 0.5263700485229492], [0.4761011004447937, 0.47544240951538086, 0.5494738221168518, 0.41425636410713196], [0.5106468200683594, 0.47100865840911865, 0.20519176125526428, 0.7133517265319824], [0.016757089644670486, 0.9700817465782166, 0.36863061785697937, 0.6217348575592041], [0.01527344062924385, 0.9615796208381653, 0.5034911632537842, 0.49691423773765564], [0.002861347049474716, 0.9952285885810852, 0.08026503026485443, 0.8754879832267761], [0.3138233423233032, 0.724989116191864, -0.0068575553596019745, 1.0656505823135376], [0.4927557110786438, 0.4665831923484802, 0.5457000732421875, 0.44373881816864014], [0.4286043047904968, 0.5650157928466797, -0.03438287600874901, 0.914689838886261], [-0.04023156687617302, 1.0274161100387573, 0.020412985235452652, 1.0125770568847656], [0.4871100187301636, 0.48980385065078735, -0.024354305118322372, 0.9926795363426208], [0.4393797516822815, 0.6092626452445984, -0.0503314845263958, 1.040976643562317], [0.39634567499160767, 0.6731142401695251, 0.30240386724472046, 0.7293391823768616], [-0.0793527215719223, 1.0272775888442993, 0.40042316913604736, 0.6546253561973572], [-0.040432628244161606, 0.985545814037323, 0.3108283281326294, 0.6712157726287842], [0.23698365688323975, 0.6146253943443298, 0.01617550477385521, 0.9788640141487122], [-0.017991792410612106, 1.106022834777832, 0.509413480758667, 0.4739859104156494], [0.506645143032074, 0.49443966150283813, 0.2657829821109772, 0.7075903415679932], [0.3331540822982788, 0.7690363526344299, 0.0009912140667438507, 1.012966275215149], [0.0035591013729572296, 0.9972575306892395, -0.006878342479467392, 1.093258261680603], [0.49123644828796387, 0.4907799959182739, 0.5769304037094116, 0.450234591960907], [-0.03705750033259392, 1.0233534574508667, 0.24717363715171814, 0.7397957444190979], [0.3074831962585449, 0.633054792881012, 0.0480886735022068, 0.9311363101005554], [0.3923269510269165, 0.5683017373085022, 0.007097501307725906, 0.9994974732398987], [0.40013259649276733, 0.6164824962615967, 0.30940932035446167, 0.761271059513092], [0.44519591331481934, 0.5651217699050903, 0.5453522205352783, 0.45746633410453796], [0.39753979444503784, 0.5841442942619324, 0.5160083770751953, 0.46118786931037903], [0.467087984085083, 0.44436103105545044, -0.06790859997272491, 1.0497682094573975], [0.44856369495391846, 0.5039730668067932, 0.4258047938346863, 0.4810478687286377], [0.2612181305885315, 0.6843036413192749, -0.03724139556288719, 1.0313953161239624], [0.23633438348770142, 0.7641127705574036, 0.0019404329359531403, 0.9860630631446838], [0.5616891384124756, 0.45800068974494934, 0.07230783998966217, 0.9294964671134949], [-0.012544628232717514, 0.9945358633995056, 0.15663409233093262, 0.8227774500846863], [0.05776740983128548, 0.8698195815086365, 0.4657704830169678, 0.5362539291381836], [0.2573600113391876, 0.7085058689117432, 0.47870194911956787, 0.5151973962783813], [-0.015174712985754013, 1.0285004377365112, 0.5360817909240723, 0.5033567547798157], [0.012863870710134506, 0.9657784700393677, 0.5068503618240356, 0.4983740448951721], [0.3272557258605957, 0.7253463864326477, -0.06722380220890045, 1.0791971683502197], [0.534986138343811, 0.47704941034317017, 0.5005849003791809, 0.4649272561073303], [0.4875568747520447, 0.4938989579677582, 0.5809800624847412, 0.4206404983997345], [-0.015788819640874863, 0.9971292614936829, 0.042021218687295914, 0.9797156453132629], [-0.05771462246775627, 1.043553352355957, 0.0037750937044620514, 1.035496711730957], [0.5715093612670898, 0.4238245487213135, 0.022643957287073135, 0.9846598505973816], [0.006053198128938675, 1.0155829191207886, 0.5531402826309204, 0.47858691215515137], [0.022380132228136063, 0.9865034222602844, 0.4616469144821167, 0.4835014343261719], [-0.021657999604940414, 1.0445481538772583, 0.01180225983262062, 1.022977590560913], [0.07475757598876953, 0.8564907908439636, -0.009597290307283401, 0.9714941382408142], [-0.02613074705004692, 1.161534070968628, 0.3728724718093872, 0.6328967213630676], [0.31629616022109985, 0.6707234382629395, 0.37125736474990845, 0.6814730763435364], [0.3212659955024719, 0.7064064145088196, 0.08746406435966492, 0.7983171343803406], [-0.025685545057058334, 0.9569680094718933, 0.03536762669682503, 0.959369957447052]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.0007572658360004425, 1.0075920820236206]
[0.5509716272354126, 0.43671494722366333]
[0.020638976246118546, 0.9834943413734436]
[-0.0027267225086688995, 0.9780183434486389]
This is the real loss :  7.595000125467777
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012], [-0.010541331022977829, 1.027101755142212, 0.27839386463165283, 0.6909443736076355], [0.272367000579834, 0.7058614492416382, 0.00028115883469581604, 1.0182385444641113], [0.28895533084869385, 0.6804113388061523, 0.39941537380218506, 0.6184879541397095], [0.011353697627782822, 0.9631422162055969, -0.06577624380588531, 1.0429608821868896], [-0.0018919818103313446, 0.9945912957191467, 0.42653173208236694, 0.6086744070053101], [0.01611100509762764, 1.0032296180725098, 0.023631710559129715, 0.9491965770721436], [0.03927203640341759, 1.001900553703308, 0.3045749366283417, 0.7350011467933655], [-0.05471464619040489, 1.0141152143478394, -0.021505441516637802, 0.9882152080535889], [0.5345359444618225, 0.4880406856536865, -0.019061248749494553, 0.9891769289970398], [-0.03129149600863457, 1.1256051063537598, 0.4196738004684448, 0.5263700485229492], [0.4761011004447937, 0.47544240951538086, 0.5494738221168518, 0.41425636410713196], [0.5106468200683594, 0.47100865840911865, 0.20519176125526428, 0.7133517265319824], [0.016757089644670486, 0.9700817465782166, 0.36863061785697937, 0.6217348575592041], [0.01527344062924385, 0.9615796208381653, 0.5034911632537842, 0.49691423773765564], [0.002861347049474716, 0.9952285885810852, 0.08026503026485443, 0.8754879832267761], [0.3138233423233032, 0.724989116191864, -0.0068575553596019745, 1.0656505823135376], [0.4927557110786438, 0.4665831923484802, 0.5457000732421875, 0.44373881816864014], [0.4286043047904968, 0.5650157928466797, -0.03438287600874901, 0.914689838886261], [-0.04023156687617302, 1.0274161100387573, 0.020412985235452652, 1.0125770568847656], [0.4871100187301636, 0.48980385065078735, -0.024354305118322372, 0.9926795363426208], [0.4393797516822815, 0.6092626452445984, -0.0503314845263958, 1.040976643562317], [0.39634567499160767, 0.6731142401695251, 0.30240386724472046, 0.7293391823768616], [-0.0793527215719223, 1.0272775888442993, 0.40042316913604736, 0.6546253561973572], [-0.040432628244161606, 0.985545814037323, 0.3108283281326294, 0.6712157726287842], [0.23698365688323975, 0.6146253943443298, 0.01617550477385521, 0.9788640141487122], [-0.017991792410612106, 1.106022834777832, 0.509413480758667, 0.4739859104156494], [0.506645143032074, 0.49443966150283813, 0.2657829821109772, 0.7075903415679932], [0.3331540822982788, 0.7690363526344299, 0.0009912140667438507, 1.012966275215149], [0.0035591013729572296, 0.9972575306892395, -0.006878342479467392, 1.093258261680603], [0.49123644828796387, 0.4907799959182739, 0.5769304037094116, 0.450234591960907], [-0.03705750033259392, 1.0233534574508667, 0.24717363715171814, 0.7397957444190979], [0.3074831962585449, 0.633054792881012, 0.0480886735022068, 0.9311363101005554], [0.3923269510269165, 0.5683017373085022, 0.007097501307725906, 0.9994974732398987], [0.40013259649276733, 0.6164824962615967, 0.30940932035446167, 0.761271059513092], [0.44519591331481934, 0.5651217699050903, 0.5453522205352783, 0.45746633410453796], [0.39753979444503784, 0.5841442942619324, 0.5160083770751953, 0.46118786931037903], [0.467087984085083, 0.44436103105545044, -0.06790859997272491, 1.0497682094573975], [0.44856369495391846, 0.5039730668067932, 0.4258047938346863, 0.4810478687286377], [0.2612181305885315, 0.6843036413192749, -0.03724139556288719, 1.0313953161239624], [0.23633438348770142, 0.7641127705574036, 0.0019404329359531403, 0.9860630631446838], [0.5616891384124756, 0.45800068974494934, 0.07230783998966217, 0.9294964671134949], [-0.012544628232717514, 0.9945358633995056, 0.15663409233093262, 0.8227774500846863], [0.05776740983128548, 0.8698195815086365, 0.4657704830169678, 0.5362539291381836], [0.2573600113391876, 0.7085058689117432, 0.47870194911956787, 0.5151973962783813], [-0.015174712985754013, 1.0285004377365112, 0.5360817909240723, 0.5033567547798157], [0.012863870710134506, 0.9657784700393677, 0.5068503618240356, 0.4983740448951721], [0.3272557258605957, 0.7253463864326477, -0.06722380220890045, 1.0791971683502197], [0.534986138343811, 0.47704941034317017, 0.5005849003791809, 0.4649272561073303], [0.4875568747520447, 0.4938989579677582, 0.5809800624847412, 0.4206404983997345], [-0.015788819640874863, 0.9971292614936829, 0.042021218687295914, 0.9797156453132629], [-0.05771462246775627, 1.043553352355957, 0.0037750937044620514, 1.035496711730957], [0.5715093612670898, 0.4238245487213135, 0.022643957287073135, 0.9846598505973816], [0.006053198128938675, 1.0155829191207886, 0.5531402826309204, 0.47858691215515137], [0.022380132228136063, 0.9865034222602844, 0.4616469144821167, 0.4835014343261719], [-0.021657999604940414, 1.0445481538772583, 0.01180225983262062, 1.022977590560913], [0.07475757598876953, 0.8564907908439636, -0.009597290307283401, 0.9714941382408142], [-0.02613074705004692, 1.161534070968628, 0.3728724718093872, 0.6328967213630676], [0.31629616022109985, 0.6707234382629395, 0.37125736474990845, 0.6814730763435364], [0.3212659955024719, 0.7064064145088196, 0.08746406435966492, 0.7983171343803406], [-0.025685545057058334, 0.9569680094718933, 0.03536762669682503, 0.959369957447052], [0.0007572658360004425, 1.0075920820236206, 0.5509716272354126, 0.43671494722366333], [0.020638976246118546, 0.9834943413734436, -0.0027267225086688995, 0.9780183434486389]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.5046491622924805, 0.519620954990387]
[0.4695517420768738, 0.5282024145126343]
[0.42038843035697937, 0.643025815486908]
[0.4514685273170471, 0.5349951982498169]
This is the real loss :  7.837134070694447
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012], [-0.010541331022977829, 1.027101755142212, 0.27839386463165283, 0.6909443736076355], [0.272367000579834, 0.7058614492416382, 0.00028115883469581604, 1.0182385444641113], [0.28895533084869385, 0.6804113388061523, 0.39941537380218506, 0.6184879541397095], [0.011353697627782822, 0.9631422162055969, -0.06577624380588531, 1.0429608821868896], [-0.0018919818103313446, 0.9945912957191467, 0.42653173208236694, 0.6086744070053101], [0.01611100509762764, 1.0032296180725098, 0.023631710559129715, 0.9491965770721436], [0.03927203640341759, 1.001900553703308, 0.3045749366283417, 0.7350011467933655], [-0.05471464619040489, 1.0141152143478394, -0.021505441516637802, 0.9882152080535889], [0.5345359444618225, 0.4880406856536865, -0.019061248749494553, 0.9891769289970398], [-0.03129149600863457, 1.1256051063537598, 0.4196738004684448, 0.5263700485229492], [0.4761011004447937, 0.47544240951538086, 0.5494738221168518, 0.41425636410713196], [0.5106468200683594, 0.47100865840911865, 0.20519176125526428, 0.7133517265319824], [0.016757089644670486, 0.9700817465782166, 0.36863061785697937, 0.6217348575592041], [0.01527344062924385, 0.9615796208381653, 0.5034911632537842, 0.49691423773765564], [0.002861347049474716, 0.9952285885810852, 0.08026503026485443, 0.8754879832267761], [0.3138233423233032, 0.724989116191864, -0.0068575553596019745, 1.0656505823135376], [0.4927557110786438, 0.4665831923484802, 0.5457000732421875, 0.44373881816864014], [0.4286043047904968, 0.5650157928466797, -0.03438287600874901, 0.914689838886261], [-0.04023156687617302, 1.0274161100387573, 0.020412985235452652, 1.0125770568847656], [0.4871100187301636, 0.48980385065078735, -0.024354305118322372, 0.9926795363426208], [0.4393797516822815, 0.6092626452445984, -0.0503314845263958, 1.040976643562317], [0.39634567499160767, 0.6731142401695251, 0.30240386724472046, 0.7293391823768616], [-0.0793527215719223, 1.0272775888442993, 0.40042316913604736, 0.6546253561973572], [-0.040432628244161606, 0.985545814037323, 0.3108283281326294, 0.6712157726287842], [0.23698365688323975, 0.6146253943443298, 0.01617550477385521, 0.9788640141487122], [-0.017991792410612106, 1.106022834777832, 0.509413480758667, 0.4739859104156494], [0.506645143032074, 0.49443966150283813, 0.2657829821109772, 0.7075903415679932], [0.3331540822982788, 0.7690363526344299, 0.0009912140667438507, 1.012966275215149], [0.0035591013729572296, 0.9972575306892395, -0.006878342479467392, 1.093258261680603], [0.49123644828796387, 0.4907799959182739, 0.5769304037094116, 0.450234591960907], [-0.03705750033259392, 1.0233534574508667, 0.24717363715171814, 0.7397957444190979], [0.3074831962585449, 0.633054792881012, 0.0480886735022068, 0.9311363101005554], [0.3923269510269165, 0.5683017373085022, 0.007097501307725906, 0.9994974732398987], [0.40013259649276733, 0.6164824962615967, 0.30940932035446167, 0.761271059513092], [0.44519591331481934, 0.5651217699050903, 0.5453522205352783, 0.45746633410453796], [0.39753979444503784, 0.5841442942619324, 0.5160083770751953, 0.46118786931037903], [0.467087984085083, 0.44436103105545044, -0.06790859997272491, 1.0497682094573975], [0.44856369495391846, 0.5039730668067932, 0.4258047938346863, 0.4810478687286377], [0.2612181305885315, 0.6843036413192749, -0.03724139556288719, 1.0313953161239624], [0.23633438348770142, 0.7641127705574036, 0.0019404329359531403, 0.9860630631446838], [0.5616891384124756, 0.45800068974494934, 0.07230783998966217, 0.9294964671134949], [-0.012544628232717514, 0.9945358633995056, 0.15663409233093262, 0.8227774500846863], [0.05776740983128548, 0.8698195815086365, 0.4657704830169678, 0.5362539291381836], [0.2573600113391876, 0.7085058689117432, 0.47870194911956787, 0.5151973962783813], [-0.015174712985754013, 1.0285004377365112, 0.5360817909240723, 0.5033567547798157], [0.012863870710134506, 0.9657784700393677, 0.5068503618240356, 0.4983740448951721], [0.3272557258605957, 0.7253463864326477, -0.06722380220890045, 1.0791971683502197], [0.534986138343811, 0.47704941034317017, 0.5005849003791809, 0.4649272561073303], [0.4875568747520447, 0.4938989579677582, 0.5809800624847412, 0.4206404983997345], [-0.015788819640874863, 0.9971292614936829, 0.042021218687295914, 0.9797156453132629], [-0.05771462246775627, 1.043553352355957, 0.0037750937044620514, 1.035496711730957], [0.5715093612670898, 0.4238245487213135, 0.022643957287073135, 0.9846598505973816], [0.006053198128938675, 1.0155829191207886, 0.5531402826309204, 0.47858691215515137], [0.022380132228136063, 0.9865034222602844, 0.4616469144821167, 0.4835014343261719], [-0.021657999604940414, 1.0445481538772583, 0.01180225983262062, 1.022977590560913], [0.07475757598876953, 0.8564907908439636, -0.009597290307283401, 0.9714941382408142], [-0.02613074705004692, 1.161534070968628, 0.3728724718093872, 0.6328967213630676], [0.31629616022109985, 0.6707234382629395, 0.37125736474990845, 0.6814730763435364], [0.3212659955024719, 0.7064064145088196, 0.08746406435966492, 0.7983171343803406], [-0.025685545057058334, 0.9569680094718933, 0.03536762669682503, 0.959369957447052], [0.0007572658360004425, 1.0075920820236206, 0.5509716272354126, 0.43671494722366333], [0.020638976246118546, 0.9834943413734436, -0.0027267225086688995, 0.9780183434486389], [0.5046491622924805, 0.519620954990387, 0.4695517420768738, 0.5282024145126343], [0.42038843035697937, 0.643025815486908, 0.4514685273170471, 0.5349951982498169]]checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3

Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.5568240880966187, 0.45697152614593506]
[0.43472015857696533, 0.5904071927070618]
[-0.01316763088107109, 1.0405367612838745]
[0.061522189527750015, 0.9188206791877747]
This is the real loss :  7.972826756536961
val_targets: [array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.40586644411087036, 0.5123195648193359, 0.04140918329358101, 0.9123368859291077], [0.3899787366390228, 0.6835499405860901, 0.5092953443527222, 0.4866027235984802], [-0.10440552234649658, 0.9943879246711731, 0.03758726641535759, 0.9650813937187195], [0.4606722593307495, 0.47674083709716797, -0.028274979442358017, 0.9936532974243164], [0.3461940586566925, 0.6664182543754578, 0.3921132981777191, 0.6198898553848267], [0.02195419743657112, 1.0387287139892578, 0.33762872219085693, 0.6531800031661987], [0.4950065612792969, 0.49095427989959717, 0.4473028779029846, 0.5428802371025085], [0.31823045015335083, 0.6963589787483215, 0.014962766319513321, 0.9739495515823364], [0.5422244071960449, 0.42665863037109375, -0.022865768522024155, 0.9733161926269531], [0.42252904176712036, 0.6370730400085449, 0.00437641516327858, 0.996898353099823], [-0.013913195580244064, 0.9638315439224243, 0.38169461488723755, 0.7103350758552551], [-0.07842278480529785, 1.0784436464309692, 0.27044570446014404, 0.7032794952392578], [0.3968953490257263, 0.6340117454528809, 0.555304765701294, 0.43608781695365906], [0.49592578411102295, 0.49791449308395386, -0.008446376770734787, 1.0834908485412598], [0.33502307534217834, 0.7172778248786926, 0.5428730845451355, 0.44281941652297974], [0.36164435744285583, 0.6882901787757874, 0.5451249480247498, 0.4154152274131775], [-0.03976884111762047, 1.050862431526184, 0.4996393322944641, 0.4879053235054016], [0.017489034682512283, 0.9434548020362854, -0.03866925463080406, 1.1393344402313232], [0.3034689426422119, 0.7516642212867737, 0.45378124713897705, 0.5335593223571777], [0.49603450298309326, 0.4660959243774414, 0.00034250691533088684, 0.9883924126625061], [0.5010023713111877, 0.47526997327804565, -0.020124536007642746, 1.117872714996338], [0.5368326306343079, 0.46753644943237305, 0.5270849466323853, 0.4637899398803711], [-0.01789924129843712, 1.009013056755066, -0.07085062563419342, 0.9776298403739929], [0.08395549654960632, 0.9032033085823059, 0.014725621789693832, 0.9618142247200012], [-0.010541331022977829, 1.027101755142212, 0.27839386463165283, 0.6909443736076355], [0.272367000579834, 0.7058614492416382, 0.00028115883469581604, 1.0182385444641113], [0.28895533084869385, 0.6804113388061523, 0.39941537380218506, 0.6184879541397095], [0.011353697627782822, 0.9631422162055969, -0.06577624380588531, 1.0429608821868896], [-0.0018919818103313446, 0.9945912957191467, 0.42653173208236694, 0.6086744070053101], [0.01611100509762764, 1.0032296180725098, 0.023631710559129715, 0.9491965770721436], [0.03927203640341759, 1.001900553703308, 0.3045749366283417, 0.7350011467933655], [-0.05471464619040489, 1.0141152143478394, -0.021505441516637802, 0.9882152080535889], [0.5345359444618225, 0.4880406856536865, -0.019061248749494553, 0.9891769289970398], [-0.03129149600863457, 1.1256051063537598, 0.4196738004684448, 0.5263700485229492], [0.4761011004447937, 0.47544240951538086, 0.5494738221168518, 0.41425636410713196], [0.5106468200683594, 0.47100865840911865, 0.20519176125526428, 0.7133517265319824], [0.016757089644670486, 0.9700817465782166, 0.36863061785697937, 0.6217348575592041], [0.01527344062924385, 0.9615796208381653, 0.5034911632537842, 0.49691423773765564], [0.002861347049474716, 0.9952285885810852, 0.08026503026485443, 0.8754879832267761], [0.3138233423233032, 0.724989116191864, -0.0068575553596019745, 1.0656505823135376], [0.4927557110786438, 0.4665831923484802, 0.5457000732421875, 0.44373881816864014], [0.4286043047904968, 0.5650157928466797, -0.03438287600874901, 0.914689838886261], [-0.04023156687617302, 1.0274161100387573, 0.020412985235452652, 1.0125770568847656], [0.4871100187301636, 0.48980385065078735, -0.024354305118322372, 0.9926795363426208], [0.4393797516822815, 0.6092626452445984, -0.0503314845263958, 1.040976643562317], [0.39634567499160767, 0.6731142401695251, 0.30240386724472046, 0.7293391823768616], [-0.0793527215719223, 1.0272775888442993, 0.40042316913604736, 0.6546253561973572], [-0.040432628244161606, 0.985545814037323, 0.3108283281326294, 0.6712157726287842], [0.23698365688323975, 0.6146253943443298, 0.01617550477385521, 0.9788640141487122], [-0.017991792410612106, 1.106022834777832, 0.509413480758667, 0.4739859104156494], [0.506645143032074, 0.49443966150283813, 0.2657829821109772, 0.7075903415679932], [0.3331540822982788, 0.7690363526344299, 0.0009912140667438507, 1.012966275215149], [0.0035591013729572296, 0.9972575306892395, -0.006878342479467392, 1.093258261680603], [0.49123644828796387, 0.4907799959182739, 0.5769304037094116, 0.450234591960907], [-0.03705750033259392, 1.0233534574508667, 0.24717363715171814, 0.7397957444190979], [0.3074831962585449, 0.633054792881012, 0.0480886735022068, 0.9311363101005554], [0.3923269510269165, 0.5683017373085022, 0.007097501307725906, 0.9994974732398987], [0.40013259649276733, 0.6164824962615967, 0.30940932035446167, 0.761271059513092], [0.44519591331481934, 0.5651217699050903, 0.5453522205352783, 0.45746633410453796], [0.39753979444503784, 0.5841442942619324, 0.5160083770751953, 0.46118786931037903], [0.467087984085083, 0.44436103105545044, -0.06790859997272491, 1.0497682094573975], [0.44856369495391846, 0.5039730668067932, 0.4258047938346863, 0.4810478687286377], [0.2612181305885315, 0.6843036413192749, -0.03724139556288719, 1.0313953161239624], [0.23633438348770142, 0.7641127705574036, 0.0019404329359531403, 0.9860630631446838], [0.5616891384124756, 0.45800068974494934, 0.07230783998966217, 0.9294964671134949], [-0.012544628232717514, 0.9945358633995056, 0.15663409233093262, 0.8227774500846863], [0.05776740983128548, 0.8698195815086365, 0.4657704830169678, 0.5362539291381836], [0.2573600113391876, 0.7085058689117432, 0.47870194911956787, 0.5151973962783813], [-0.015174712985754013, 1.0285004377365112, 0.5360817909240723, 0.5033567547798157], [0.012863870710134506, 0.9657784700393677, 0.5068503618240356, 0.4983740448951721], [0.3272557258605957, 0.7253463864326477, -0.06722380220890045, 1.0791971683502197], [0.534986138343811, 0.47704941034317017, 0.5005849003791809, 0.4649272561073303], [0.4875568747520447, 0.4938989579677582, 0.5809800624847412, 0.4206404983997345], [-0.015788819640874863, 0.9971292614936829, 0.042021218687295914, 0.9797156453132629], [-0.05771462246775627, 1.043553352355957, 0.0037750937044620514, 1.035496711730957], [0.5715093612670898, 0.4238245487213135, 0.022643957287073135, 0.9846598505973816], [0.006053198128938675, 1.0155829191207886, 0.5531402826309204, 0.47858691215515137], [0.022380132228136063, 0.9865034222602844, 0.4616469144821167, 0.4835014343261719], [-0.021657999604940414, 1.0445481538772583, 0.01180225983262062, 1.022977590560913], [0.07475757598876953, 0.8564907908439636, -0.009597290307283401, 0.9714941382408142], [-0.02613074705004692, 1.161534070968628, 0.3728724718093872, 0.6328967213630676], [0.31629616022109985, 0.6707234382629395, 0.37125736474990845, 0.6814730763435364], [0.3212659955024719, 0.7064064145088196, 0.08746406435966492, 0.7983171343803406], [-0.025685545057058334, 0.9569680094718933, 0.03536762669682503, 0.959369957447052], [0.0007572658360004425, 1.0075920820236206, 0.5509716272354126, 0.43671494722366333], [0.020638976246118546, 0.9834943413734436, -0.0027267225086688995, 0.9780183434486389], [0.5046491622924805, 0.519620954990387, 0.4695517420768738, 0.5282024145126343], [0.42038843035697937, 0.643025815486908, 0.4514685273170471, 0.5349951982498169], [0.5568240880966187, 0.45697152614593506, 0.43472015857696533, 0.5904071927070618], [-0.01316763088107109, 1.0405367612838745, 0.061522189527750015, 0.9188206791877747]]
Batch size (logits): 4, Batch size (labels): 4
Total Predictions: 90, Total Targets: 180
