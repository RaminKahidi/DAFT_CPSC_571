True
Training completed.
MRI Directory: ./ProcessedDataset/MRIs/
Tabular Directory: ./RawDataset/HER2_target.csv
tabularFileName: HER2_target
MONAI version: 1.1.0
Numpy version: 1.19.2
Pytorch version: 1.13.1+cu117
MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False
MONAI rev id: a2ec3752f54bfc3b40e7952234fbeb5452ed63e3
MONAI __file__: /home/ramin.kahidi/software/miniconda3/envs/pytorch/lib/python3.7/site-packages/monai/__init__.py

Optional dependencies:
Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.
Nibabel version: NOT INSTALLED or UNKNOWN VERSION.
scikit-image version: NOT INSTALLED or UNKNOWN VERSION.
Pillow version: 9.4.0
Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.
gdown version: NOT INSTALLED or UNKNOWN VERSION.
TorchVision version: 0.13.1a0
tqdm version: 4.66.2
lmdb version: NOT INSTALLED or UNKNOWN VERSION.
psutil version: NOT INSTALLED or UNKNOWN VERSION.
pandas version: 1.3.5
einops version: NOT INSTALLED or UNKNOWN VERSION.
transformers version: NOT INSTALLED or UNKNOWN VERSION.
mlflow version: NOT INSTALLED or UNKNOWN VERSION.
pynrrd version: NOT INSTALLED or UNKNOWN VERSION.

For details about installing the optional dependencies, please visit:
    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies

DAFT
workers: 20
dcm_patient_filenames:  ['Breast_MRI_001.pt', 'Breast_MRI_002.pt', 'Breast_MRI_003.pt', 'Breast_MRI_004.pt', 'Breast_MRI_005.pt', 'Breast_MRI_006.pt', 'Breast_MRI_007.pt', 'Breast_MRI_008.pt', 'Breast_MRI_009.pt', 'Breast_MRI_010.pt', 'Breast_MRI_011.pt', 'Breast_MRI_012.pt', 'Breast_MRI_013.pt', 'Breast_MRI_014.pt', 'Breast_MRI_015.pt', 'Breast_MRI_016.pt', 'Breast_MRI_017.pt', 'Breast_MRI_018.pt', 'Breast_MRI_019.pt', 'Breast_MRI_020.pt', 'Breast_MRI_021.pt', 'Breast_MRI_022.pt', 'Breast_MRI_023.pt', 'Breast_MRI_024.pt', 'Breast_MRI_025.pt', 'Breast_MRI_026.pt', 'Breast_MRI_027.pt', 'Breast_MRI_028.pt', 'Breast_MRI_029.pt', 'Breast_MRI_030.pt', 'Breast_MRI_031.pt', 'Breast_MRI_032.pt', 'Breast_MRI_033.pt', 'Breast_MRI_034.pt', 'Breast_MRI_035.pt', 'Breast_MRI_036.pt', 'Breast_MRI_037.pt', 'Breast_MRI_038.pt', 'Breast_MRI_039.pt', 'Breast_MRI_040.pt', 'Breast_MRI_041.pt', 'Breast_MRI_042.pt', 'Breast_MRI_043.pt', 'Breast_MRI_044.pt', 'Breast_MRI_045.pt', 'Breast_MRI_046.pt', 'Breast_MRI_047.pt', 'Breast_MRI_048.pt', 'Breast_MRI_049.pt', 'Breast_MRI_050.pt', 'Breast_MRI_051.pt', 'Breast_MRI_052.pt', 'Breast_MRI_053.pt', 'Breast_MRI_054.pt', 'Breast_MRI_055.pt', 'Breast_MRI_056.pt', 'Breast_MRI_057.pt', 'Breast_MRI_058.pt', 'Breast_MRI_059.pt', 'Breast_MRI_060.pt', 'Breast_MRI_061.pt', 'Breast_MRI_062.pt', 'Breast_MRI_063.pt', 'Breast_MRI_064.pt', 'Breast_MRI_065.pt', 'Breast_MRI_066.pt', 'Breast_MRI_067.pt', 'Breast_MRI_068.pt', 'Breast_MRI_069.pt', 'Breast_MRI_070.pt', 'Breast_MRI_071.pt', 'Breast_MRI_072.pt', 'Breast_MRI_073.pt', 'Breast_MRI_074.pt', 'Breast_MRI_075.pt', 'Breast_MRI_076.pt', 'Breast_MRI_077.pt', 'Breast_MRI_078.pt', 'Breast_MRI_079.pt', 'Breast_MRI_080.pt', 'Breast_MRI_081.pt', 'Breast_MRI_082.pt', 'Breast_MRI_083.pt', 'Breast_MRI_084.pt', 'Breast_MRI_085.pt', 'Breast_MRI_086.pt', 'Breast_MRI_087.pt', 'Breast_MRI_088.pt', 'Breast_MRI_089.pt', 'Breast_MRI_090.pt', 'Breast_MRI_091.pt', 'Breast_MRI_092.pt', 'Breast_MRI_093.pt', 'Breast_MRI_094.pt', 'Breast_MRI_095.pt', 'Breast_MRI_096.pt', 'Breast_MRI_097.pt', 'Breast_MRI_098.pt', 'Breast_MRI_099.pt', 'Breast_MRI_100.pt', 'Breast_MRI_101.pt', 'Breast_MRI_102.pt', 'Breast_MRI_103.pt', 'Breast_MRI_104.pt', 'Breast_MRI_105.pt', 'Breast_MRI_106.pt', 'Breast_MRI_107.pt', 'Breast_MRI_108.pt', 'Breast_MRI_109.pt', 'Breast_MRI_110.pt', 'Breast_MRI_111.pt', 'Breast_MRI_112.pt', 'Breast_MRI_113.pt', 'Breast_MRI_114.pt', 'Breast_MRI_115.pt', 'Breast_MRI_116.pt', 'Breast_MRI_117.pt', 'Breast_MRI_118.pt', 'Breast_MRI_119.pt', 'Breast_MRI_121.pt', 'Breast_MRI_122.pt', 'Breast_MRI_123.pt', 'Breast_MRI_124.pt', 'Breast_MRI_125.pt', 'Breast_MRI_126.pt', 'Breast_MRI_127.pt', 'Breast_MRI_128.pt', 'Breast_MRI_129.pt', 'Breast_MRI_131.pt', 'Breast_MRI_132.pt', 'Breast_MRI_133.pt', 'Breast_MRI_134.pt', 'Breast_MRI_135.pt', 'Breast_MRI_136.pt', 'Breast_MRI_137.pt', 'Breast_MRI_138.pt', 'Breast_MRI_139.pt', 'Breast_MRI_140.pt', 'Breast_MRI_141.pt', 'Breast_MRI_142.pt', 'Breast_MRI_143.pt', 'Breast_MRI_144.pt', 'Breast_MRI_145.pt', 'Breast_MRI_146.pt', 'Breast_MRI_147.pt', 'Breast_MRI_148.pt', 'Breast_MRI_149.pt', 'Breast_MRI_150.pt', 'Breast_MRI_151.pt', 'Breast_MRI_152.pt', 'Breast_MRI_153.pt', 'Breast_MRI_154.pt', 'Breast_MRI_155.pt', 'Breast_MRI_156.pt', 'Breast_MRI_157.pt', 'Breast_MRI_158.pt', 'Breast_MRI_159.pt', 'Breast_MRI_160.pt', 'Breast_MRI_161.pt', 'Breast_MRI_162.pt', 'Breast_MRI_163.pt', 'Breast_MRI_164.pt', 'Breast_MRI_165.pt', 'Breast_MRI_166.pt', 'Breast_MRI_168.pt', 'Breast_MRI_169.pt', 'Breast_MRI_170.pt', 'Breast_MRI_171.pt', 'Breast_MRI_172.pt', 'Breast_MRI_173.pt', 'Breast_MRI_174.pt', 'Breast_MRI_175.pt', 'Breast_MRI_176.pt', 'Breast_MRI_177.pt', 'Breast_MRI_178.pt', 'Breast_MRI_179.pt', 'Breast_MRI_180.pt', 'Breast_MRI_181.pt', 'Breast_MRI_182.pt', 'Breast_MRI_183.pt', 'Breast_MRI_184.pt', 'Breast_MRI_185.pt', 'Breast_MRI_186.pt', 'Breast_MRI_187.pt', 'Breast_MRI_188.pt', 'Breast_MRI_189.pt', 'Breast_MRI_190.pt', 'Breast_MRI_191.pt', 'Breast_MRI_192.pt', 'Breast_MRI_193.pt', 'Breast_MRI_194.pt', 'Breast_MRI_195.pt', 'Breast_MRI_196.pt', 'Breast_MRI_197.pt', 'Breast_MRI_198.pt', 'Breast_MRI_199.pt', 'Breast_MRI_200.pt', 'Breast_MRI_201.pt', 'Breast_MRI_202.pt', 'Breast_MRI_203.pt', 'Breast_MRI_204.pt', 'Breast_MRI_205.pt', 'Breast_MRI_206.pt', 'Breast_MRI_207.pt', 'Breast_MRI_208.pt', 'Breast_MRI_209.pt', 'Breast_MRI_210.pt', 'Breast_MRI_211.pt', 'Breast_MRI_212.pt', 'Breast_MRI_213.pt', 'Breast_MRI_214.pt', 'Breast_MRI_215.pt', 'Breast_MRI_216.pt', 'Breast_MRI_217.pt', 'Breast_MRI_218.pt', 'Breast_MRI_219.pt', 'Breast_MRI_220.pt', 'Breast_MRI_221.pt', 'Breast_MRI_222.pt', 'Breast_MRI_223.pt', 'Breast_MRI_224.pt', 'Breast_MRI_225.pt', 'Breast_MRI_226.pt', 'Breast_MRI_227.pt', 'Breast_MRI_228.pt', 'Breast_MRI_229.pt', 'Breast_MRI_230.pt', 'Breast_MRI_231.pt', 'Breast_MRI_232.pt', 'Breast_MRI_233.pt', 'Breast_MRI_234.pt', 'Breast_MRI_235.pt', 'Breast_MRI_236.pt', 'Breast_MRI_237.pt', 'Breast_MRI_238.pt', 'Breast_MRI_239.pt', 'Breast_MRI_240.pt', 'Breast_MRI_241.pt', 'Breast_MRI_242.pt', 'Breast_MRI_243.pt', 'Breast_MRI_244.pt', 'Breast_MRI_245.pt', 'Breast_MRI_246.pt', 'Breast_MRI_247.pt', 'Breast_MRI_248.pt', 'Breast_MRI_249.pt', 'Breast_MRI_251.pt', 'Breast_MRI_252.pt', 'Breast_MRI_253.pt', 'Breast_MRI_254.pt', 'Breast_MRI_255.pt', 'Breast_MRI_256.pt', 'Breast_MRI_257.pt', 'Breast_MRI_258.pt', 'Breast_MRI_259.pt', 'Breast_MRI_260.pt', 'Breast_MRI_261.pt', 'Breast_MRI_262.pt', 'Breast_MRI_263.pt', 'Breast_MRI_264.pt', 'Breast_MRI_265.pt', 'Breast_MRI_266.pt', 'Breast_MRI_267.pt', 'Breast_MRI_268.pt', 'Breast_MRI_269.pt', 'Breast_MRI_270.pt', 'Breast_MRI_271.pt', 'Breast_MRI_272.pt', 'Breast_MRI_273.pt', 'Breast_MRI_274.pt', 'Breast_MRI_275.pt', 'Breast_MRI_276.pt', 'Breast_MRI_277.pt', 'Breast_MRI_278.pt', 'Breast_MRI_279.pt', 'Breast_MRI_280.pt', 'Breast_MRI_281.pt', 'Breast_MRI_282.pt', 'Breast_MRI_283.pt', 'Breast_MRI_284.pt', 'Breast_MRI_285.pt', 'Breast_MRI_286.pt', 'Breast_MRI_287.pt', 'Breast_MRI_288.pt', 'Breast_MRI_289.pt', 'Breast_MRI_290.pt', 'Breast_MRI_291.pt', 'Breast_MRI_292.pt', 'Breast_MRI_293.pt', 'Breast_MRI_294.pt', 'Breast_MRI_295.pt', 'Breast_MRI_296.pt', 'Breast_MRI_297.pt', 'Breast_MRI_298.pt', 'Breast_MRI_299.pt', 'Breast_MRI_301.pt', 'Breast_MRI_302.pt', 'Breast_MRI_303.pt', 'Breast_MRI_304.pt', 'Breast_MRI_305.pt', 'Breast_MRI_306.pt', 'Breast_MRI_307.pt', 'Breast_MRI_308.pt', 'Breast_MRI_309.pt', 'Breast_MRI_310.pt', 'Breast_MRI_311.pt', 'Breast_MRI_312.pt', 'Breast_MRI_313.pt', 'Breast_MRI_314.pt', 'Breast_MRI_315.pt', 'Breast_MRI_316.pt', 'Breast_MRI_317.pt', 'Breast_MRI_318.pt', 'Breast_MRI_319.pt', 'Breast_MRI_320.pt', 'Breast_MRI_321.pt', 'Breast_MRI_322.pt', 'Breast_MRI_323.pt', 'Breast_MRI_324.pt', 'Breast_MRI_325.pt', 'Breast_MRI_326.pt', 'Breast_MRI_327.pt', 'Breast_MRI_328.pt', 'Breast_MRI_329.pt', 'Breast_MRI_330.pt', 'Breast_MRI_331.pt', 'Breast_MRI_332.pt', 'Breast_MRI_333.pt', 'Breast_MRI_334.pt', 'Breast_MRI_335.pt', 'Breast_MRI_336.pt', 'Breast_MRI_337.pt', 'Breast_MRI_338.pt', 'Breast_MRI_339.pt', 'Breast_MRI_340.pt', 'Breast_MRI_341.pt', 'Breast_MRI_342.pt', 'Breast_MRI_343.pt', 'Breast_MRI_344.pt', 'Breast_MRI_345.pt', 'Breast_MRI_346.pt', 'Breast_MRI_347.pt', 'Breast_MRI_348.pt', 'Breast_MRI_349.pt', 'Breast_MRI_350.pt', 'Breast_MRI_351.pt', 'Breast_MRI_352.pt', 'Breast_MRI_353.pt', 'Breast_MRI_354.pt', 'Breast_MRI_355.pt', 'Breast_MRI_356.pt', 'Breast_MRI_357.pt', 'Breast_MRI_358.pt', 'Breast_MRI_359.pt', 'Breast_MRI_360.pt', 'Breast_MRI_361.pt', 'Breast_MRI_362.pt', 'Breast_MRI_363.pt', 'Breast_MRI_364.pt', 'Breast_MRI_365.pt', 'Breast_MRI_366.pt', 'Breast_MRI_367.pt', 'Breast_MRI_368.pt', 'Breast_MRI_369.pt', 'Breast_MRI_370.pt', 'Breast_MRI_371.pt', 'Breast_MRI_372.pt', 'Breast_MRI_373.pt', 'Breast_MRI_374.pt', 'Breast_MRI_375.pt', 'Breast_MRI_376.pt', 'Breast_MRI_377.pt', 'Breast_MRI_378.pt', 'Breast_MRI_379.pt', 'Breast_MRI_380.pt', 'Breast_MRI_381.pt', 'Breast_MRI_382.pt', 'Breast_MRI_383.pt', 'Breast_MRI_384.pt', 'Breast_MRI_385.pt', 'Breast_MRI_386.pt', 'Breast_MRI_387.pt', 'Breast_MRI_388.pt', 'Breast_MRI_389.pt', 'Breast_MRI_390.pt', 'Breast_MRI_391.pt', 'Breast_MRI_392.pt', 'Breast_MRI_393.pt', 'Breast_MRI_394.pt', 'Breast_MRI_395.pt', 'Breast_MRI_396.pt', 'Breast_MRI_397.pt', 'Breast_MRI_398.pt', 'Breast_MRI_399.pt', 'Breast_MRI_400.pt', 'Breast_MRI_401.pt', 'Breast_MRI_402.pt', 'Breast_MRI_403.pt', 'Breast_MRI_404.pt', 'Breast_MRI_405.pt', 'Breast_MRI_406.pt', 'Breast_MRI_407.pt', 'Breast_MRI_408.pt', 'Breast_MRI_409.pt', 'Breast_MRI_410.pt', 'Breast_MRI_411.pt', 'Breast_MRI_412.pt', 'Breast_MRI_413.pt', 'Breast_MRI_414.pt', 'Breast_MRI_415.pt', 'Breast_MRI_416.pt', 'Breast_MRI_417.pt', 'Breast_MRI_418.pt', 'Breast_MRI_419.pt', 'Breast_MRI_420.pt', 'Breast_MRI_421.pt', 'Breast_MRI_422.pt', 'Breast_MRI_423.pt', 'Breast_MRI_424.pt', 'Breast_MRI_425.pt', 'Breast_MRI_426.pt', 'Breast_MRI_427.pt', 'Breast_MRI_428.pt', 'Breast_MRI_429.pt', 'Breast_MRI_430.pt', 'Breast_MRI_431.pt', 'Breast_MRI_432.pt', 'Breast_MRI_433.pt', 'Breast_MRI_434.pt', 'Breast_MRI_435.pt', 'Breast_MRI_436.pt', 'Breast_MRI_437.pt', 'Breast_MRI_438.pt', 'Breast_MRI_439.pt', 'Breast_MRI_440.pt', 'Breast_MRI_441.pt', 'Breast_MRI_442.pt', 'Breast_MRI_443.pt', 'Breast_MRI_444.pt', 'Breast_MRI_445.pt', 'Breast_MRI_446.pt', 'Breast_MRI_447.pt', 'Breast_MRI_448.pt', 'Breast_MRI_449.pt', 'Breast_MRI_450.pt', 'Breast_MRI_451.pt', 'Breast_MRI_452.pt', 'Breast_MRI_453.pt', 'Breast_MRI_454.pt', 'Breast_MRI_455.pt', 'Breast_MRI_456.pt', 'Breast_MRI_457.pt', 'Breast_MRI_458.pt', 'Breast_MRI_459.pt', 'Breast_MRI_460.pt', 'Breast_MRI_461.pt', 'Breast_MRI_462.pt', 'Breast_MRI_463.pt', 'Breast_MRI_464.pt', 'Breast_MRI_465.pt', 'Breast_MRI_466.pt', 'Breast_MRI_467.pt', 'Breast_MRI_468.pt', 'Breast_MRI_469.pt', 'Breast_MRI_470.pt', 'Breast_MRI_471.pt', 'Breast_MRI_472.pt', 'Breast_MRI_473.pt', 'Breast_MRI_474.pt', 'Breast_MRI_475.pt', 'Breast_MRI_476.pt', 'Breast_MRI_477.pt', 'Breast_MRI_478.pt', 'Breast_MRI_479.pt', 'Breast_MRI_480.pt', 'Breast_MRI_481.pt', 'Breast_MRI_482.pt', 'Breast_MRI_483.pt', 'Breast_MRI_484.pt', 'Breast_MRI_485.pt', 'Breast_MRI_486.pt', 'Breast_MRI_487.pt', 'Breast_MRI_488.pt', 'Breast_MRI_489.pt', 'Breast_MRI_490.pt', 'Breast_MRI_491.pt', 'Breast_MRI_492.pt', 'Breast_MRI_493.pt', 'Breast_MRI_494.pt', 'Breast_MRI_495.pt', 'Breast_MRI_496.pt', 'Breast_MRI_497.pt', 'Breast_MRI_498.pt', 'Breast_MRI_499.pt', 'Breast_MRI_500.pt', 'Breast_MRI_501.pt', 'Breast_MRI_502.pt', 'Breast_MRI_503.pt', 'Breast_MRI_504.pt', 'Breast_MRI_505.pt', 'Breast_MRI_506.pt', 'Breast_MRI_507.pt', 'Breast_MRI_508.pt', 'Breast_MRI_509.pt', 'Breast_MRI_510.pt', 'Breast_MRI_511.pt', 'Breast_MRI_512.pt', 'Breast_MRI_513.pt', 'Breast_MRI_514.pt', 'Breast_MRI_515.pt', 'Breast_MRI_516.pt', 'Breast_MRI_517.pt', 'Breast_MRI_518.pt', 'Breast_MRI_519.pt', 'Breast_MRI_520.pt', 'Breast_MRI_521.pt', 'Breast_MRI_522.pt', 'Breast_MRI_523.pt', 'Breast_MRI_524.pt', 'Breast_MRI_525.pt', 'Breast_MRI_526.pt', 'Breast_MRI_527.pt', 'Breast_MRI_528.pt', 'Breast_MRI_529.pt', 'Breast_MRI_530.pt', 'Breast_MRI_531.pt', 'Breast_MRI_532.pt', 'Breast_MRI_533.pt', 'Breast_MRI_534.pt', 'Breast_MRI_535.pt', 'Breast_MRI_536.pt', 'Breast_MRI_537.pt', 'Breast_MRI_538.pt', 'Breast_MRI_539.pt', 'Breast_MRI_540.pt', 'Breast_MRI_541.pt', 'Breast_MRI_542.pt', 'Breast_MRI_543.pt', 'Breast_MRI_544.pt', 'Breast_MRI_545.pt', 'Breast_MRI_546.pt', 'Breast_MRI_547.pt', 'Breast_MRI_548.pt', 'Breast_MRI_549.pt', 'Breast_MRI_550.pt', 'Breast_MRI_551.pt', 'Breast_MRI_552.pt', 'Breast_MRI_553.pt', 'Breast_MRI_554.pt', 'Breast_MRI_555.pt', 'Breast_MRI_556.pt', 'Breast_MRI_557.pt', 'Breast_MRI_558.pt', 'Breast_MRI_559.pt', 'Breast_MRI_560.pt', 'Breast_MRI_561.pt', 'Breast_MRI_562.pt', 'Breast_MRI_563.pt', 'Breast_MRI_564.pt', 'Breast_MRI_565.pt', 'Breast_MRI_566.pt', 'Breast_MRI_567.pt', 'Breast_MRI_568.pt', 'Breast_MRI_569.pt', 'Breast_MRI_570.pt', 'Breast_MRI_571.pt', 'Breast_MRI_572.pt', 'Breast_MRI_573.pt', 'Breast_MRI_574.pt', 'Breast_MRI_575.pt', 'Breast_MRI_576.pt', 'Breast_MRI_578.pt', 'Breast_MRI_579.pt', 'Breast_MRI_580.pt', 'Breast_MRI_581.pt', 'Breast_MRI_582.pt', 'Breast_MRI_583.pt', 'Breast_MRI_584.pt', 'Breast_MRI_585.pt', 'Breast_MRI_586.pt', 'Breast_MRI_587.pt', 'Breast_MRI_588.pt', 'Breast_MRI_589.pt', 'Breast_MRI_590.pt', 'Breast_MRI_591.pt', 'Breast_MRI_592.pt', 'Breast_MRI_593.pt', 'Breast_MRI_594.pt', 'Breast_MRI_595.pt', 'Breast_MRI_596.pt', 'Breast_MRI_597.pt', 'Breast_MRI_598.pt', 'Breast_MRI_599.pt', 'Breast_MRI_600.pt', 'Breast_MRI_601.pt', 'Breast_MRI_602.pt', 'Breast_MRI_603.pt', 'Breast_MRI_604.pt', 'Breast_MRI_605.pt', 'Breast_MRI_606.pt', 'Breast_MRI_607.pt', 'Breast_MRI_608.pt', 'Breast_MRI_609.pt', 'Breast_MRI_610.pt', 'Breast_MRI_611.pt', 'Breast_MRI_612.pt', 'Breast_MRI_613.pt', 'Breast_MRI_614.pt', 'Breast_MRI_615.pt', 'Breast_MRI_616.pt', 'Breast_MRI_617.pt', 'Breast_MRI_618.pt', 'Breast_MRI_619.pt', 'Breast_MRI_620.pt', 'Breast_MRI_621.pt', 'Breast_MRI_622.pt', 'Breast_MRI_623.pt', 'Breast_MRI_624.pt', 'Breast_MRI_625.pt', 'Breast_MRI_626.pt', 'Breast_MRI_627.pt', 'Breast_MRI_628.pt', 'Breast_MRI_629.pt', 'Breast_MRI_630.pt', 'Breast_MRI_631.pt', 'Breast_MRI_632.pt', 'Breast_MRI_633.pt', 'Breast_MRI_634.pt', 'Breast_MRI_635.pt', 'Breast_MRI_636.pt', 'Breast_MRI_637.pt', 'Breast_MRI_638.pt', 'Breast_MRI_639.pt', 'Breast_MRI_640.pt', 'Breast_MRI_641.pt', 'Breast_MRI_642.pt', 'Breast_MRI_643.pt', 'Breast_MRI_644.pt', 'Breast_MRI_645.pt', 'Breast_MRI_647.pt', 'Breast_MRI_648.pt', 'Breast_MRI_649.pt', 'Breast_MRI_650.pt', 'Breast_MRI_651.pt', 'Breast_MRI_652.pt', 'Breast_MRI_653.pt', 'Breast_MRI_654.pt', 'Breast_MRI_655.pt', 'Breast_MRI_656.pt', 'Breast_MRI_657.pt', 'Breast_MRI_658.pt', 'Breast_MRI_659.pt', 'Breast_MRI_660.pt', 'Breast_MRI_661.pt', 'Breast_MRI_662.pt', 'Breast_MRI_663.pt', 'Breast_MRI_664.pt', 'Breast_MRI_665.pt', 'Breast_MRI_666.pt', 'Breast_MRI_667.pt', 'Breast_MRI_668.pt', 'Breast_MRI_669.pt', 'Breast_MRI_670.pt', 'Breast_MRI_672.pt', 'Breast_MRI_673.pt', 'Breast_MRI_674.pt', 'Breast_MRI_675.pt', 'Breast_MRI_676.pt', 'Breast_MRI_677.pt', 'Breast_MRI_678.pt', 'Breast_MRI_679.pt', 'Breast_MRI_680.pt', 'Breast_MRI_681.pt', 'Breast_MRI_682.pt', 'Breast_MRI_683.pt', 'Breast_MRI_684.pt', 'Breast_MRI_685.pt', 'Breast_MRI_686.pt', 'Breast_MRI_687.pt', 'Breast_MRI_688.pt', 'Breast_MRI_689.pt', 'Breast_MRI_690.pt', 'Breast_MRI_691.pt', 'Breast_MRI_692.pt', 'Breast_MRI_693.pt', 'Breast_MRI_694.pt', 'Breast_MRI_695.pt', 'Breast_MRI_696.pt', 'Breast_MRI_697.pt', 'Breast_MRI_698.pt', 'Breast_MRI_699.pt', 'Breast_MRI_700.pt', 'Breast_MRI_701.pt', 'Breast_MRI_702.pt', 'Breast_MRI_703.pt', 'Breast_MRI_704.pt', 'Breast_MRI_705.pt', 'Breast_MRI_706.pt', 'Breast_MRI_707.pt', 'Breast_MRI_708.pt', 'Breast_MRI_709.pt', 'Breast_MRI_710.pt', 'Breast_MRI_711.pt', 'Breast_MRI_712.pt', 'Breast_MRI_713.pt', 'Breast_MRI_714.pt', 'Breast_MRI_715.pt', 'Breast_MRI_716.pt', 'Breast_MRI_717.pt', 'Breast_MRI_718.pt', 'Breast_MRI_719.pt', 'Breast_MRI_720.pt', 'Breast_MRI_721.pt', 'Breast_MRI_722.pt', 'Breast_MRI_723.pt', 'Breast_MRI_724.pt', 'Breast_MRI_725.pt', 'Breast_MRI_726.pt', 'Breast_MRI_727.pt', 'Breast_MRI_728.pt', 'Breast_MRI_729.pt', 'Breast_MRI_730.pt', 'Breast_MRI_731.pt', 'Breast_MRI_732.pt', 'Breast_MRI_733.pt', 'Breast_MRI_734.pt', 'Breast_MRI_735.pt', 'Breast_MRI_736.pt', 'Breast_MRI_737.pt', 'Breast_MRI_738.pt', 'Breast_MRI_739.pt', 'Breast_MRI_740.pt', 'Breast_MRI_741.pt', 'Breast_MRI_742.pt', 'Breast_MRI_743.pt', 'Breast_MRI_744.pt', 'Breast_MRI_745.pt', 'Breast_MRI_746.pt', 'Breast_MRI_747.pt', 'Breast_MRI_748.pt', 'Breast_MRI_749.pt', 'Breast_MRI_750.pt', 'Breast_MRI_751.pt', 'Breast_MRI_752.pt', 'Breast_MRI_753.pt', 'Breast_MRI_754.pt', 'Breast_MRI_755.pt', 'Breast_MRI_756.pt', 'Breast_MRI_757.pt', 'Breast_MRI_758.pt', 'Breast_MRI_759.pt', 'Breast_MRI_760.pt', 'Breast_MRI_761.pt', 'Breast_MRI_762.pt', 'Breast_MRI_763.pt', 'Breast_MRI_764.pt', 'Breast_MRI_765.pt', 'Breast_MRI_766.pt', 'Breast_MRI_767.pt', 'Breast_MRI_768.pt', 'Breast_MRI_769.pt', 'Breast_MRI_770.pt', 'Breast_MRI_771.pt', 'Breast_MRI_772.pt', 'Breast_MRI_773.pt', 'Breast_MRI_774.pt', 'Breast_MRI_775.pt', 'Breast_MRI_776.pt', 'Breast_MRI_777.pt', 'Breast_MRI_778.pt', 'Breast_MRI_779.pt', 'Breast_MRI_780.pt', 'Breast_MRI_781.pt', 'Breast_MRI_782.pt', 'Breast_MRI_783.pt', 'Breast_MRI_784.pt', 'Breast_MRI_785.pt', 'Breast_MRI_786.pt', 'Breast_MRI_787.pt', 'Breast_MRI_788.pt', 'Breast_MRI_789.pt', 'Breast_MRI_790.pt', 'Breast_MRI_791.pt', 'Breast_MRI_792.pt', 'Breast_MRI_793.pt', 'Breast_MRI_794.pt', 'Breast_MRI_795.pt', 'Breast_MRI_796.pt', 'Breast_MRI_797.pt', 'Breast_MRI_798.pt', 'Breast_MRI_799.pt', 'Breast_MRI_800.pt', 'Breast_MRI_801.pt', 'Breast_MRI_802.pt', 'Breast_MRI_803.pt', 'Breast_MRI_804.pt', 'Breast_MRI_805.pt', 'Breast_MRI_806.pt', 'Breast_MRI_807.pt', 'Breast_MRI_808.pt', 'Breast_MRI_809.pt', 'Breast_MRI_810.pt', 'Breast_MRI_811.pt', 'Breast_MRI_812.pt', 'Breast_MRI_813.pt', 'Breast_MRI_814.pt', 'Breast_MRI_815.pt', 'Breast_MRI_816.pt', 'Breast_MRI_817.pt', 'Breast_MRI_818.pt', 'Breast_MRI_819.pt', 'Breast_MRI_820.pt', 'Breast_MRI_821.pt', 'Breast_MRI_822.pt', 'Breast_MRI_823.pt', 'Breast_MRI_824.pt', 'Breast_MRI_825.pt', 'Breast_MRI_826.pt', 'Breast_MRI_827.pt', 'Breast_MRI_828.pt', 'Breast_MRI_829.pt', 'Breast_MRI_830.pt', 'Breast_MRI_831.pt', 'Breast_MRI_832.pt', 'Breast_MRI_833.pt', 'Breast_MRI_834.pt', 'Breast_MRI_835.pt', 'Breast_MRI_836.pt', 'Breast_MRI_837.pt', 'Breast_MRI_838.pt', 'Breast_MRI_839.pt', 'Breast_MRI_840.pt', 'Breast_MRI_841.pt', 'Breast_MRI_842.pt', 'Breast_MRI_843.pt', 'Breast_MRI_844.pt', 'Breast_MRI_845.pt', 'Breast_MRI_846.pt', 'Breast_MRI_847.pt', 'Breast_MRI_848.pt', 'Breast_MRI_849.pt', 'Breast_MRI_850.pt', 'Breast_MRI_851.pt', 'Breast_MRI_852.pt', 'Breast_MRI_853.pt', 'Breast_MRI_854.pt', 'Breast_MRI_855.pt', 'Breast_MRI_856.pt', 'Breast_MRI_857.pt', 'Breast_MRI_858.pt', 'Breast_MRI_859.pt', 'Breast_MRI_860.pt', 'Breast_MRI_861.pt', 'Breast_MRI_862.pt', 'Breast_MRI_863.pt', 'Breast_MRI_864.pt', 'Breast_MRI_865.pt', 'Breast_MRI_866.pt', 'Breast_MRI_867.pt', 'Breast_MRI_868.pt', 'Breast_MRI_869.pt', 'Breast_MRI_870.pt', 'Breast_MRI_871.pt', 'Breast_MRI_872.pt', 'Breast_MRI_873.pt', 'Breast_MRI_874.pt', 'Breast_MRI_875.pt', 'Breast_MRI_876.pt', 'Breast_MRI_877.pt', 'Breast_MRI_878.pt', 'Breast_MRI_879.pt', 'Breast_MRI_880.pt', 'Breast_MRI_881.pt', 'Breast_MRI_882.pt', 'Breast_MRI_883.pt', 'Breast_MRI_884.pt', 'Breast_MRI_885.pt', 'Breast_MRI_886.pt', 'Breast_MRI_887.pt', 'Breast_MRI_888.pt', 'Breast_MRI_889.pt', 'Breast_MRI_890.pt', 'Breast_MRI_891.pt', 'Breast_MRI_892.pt', 'Breast_MRI_893.pt', 'Breast_MRI_894.pt', 'Breast_MRI_895.pt', 'Breast_MRI_896.pt', 'Breast_MRI_897.pt', 'Breast_MRI_898.pt', 'Breast_MRI_899.pt', 'Breast_MRI_900.pt', 'Breast_MRI_901.pt', 'Breast_MRI_902.pt', 'Breast_MRI_903.pt', 'Breast_MRI_904.pt', 'Breast_MRI_905.pt', 'Breast_MRI_906.pt', 'Breast_MRI_907.pt', 'Breast_MRI_908.pt', 'Breast_MRI_909.pt', 'Breast_MRI_910.pt', 'Breast_MRI_911.pt', 'Breast_MRI_912.pt', 'Breast_MRI_913.pt', 'Breast_MRI_914.pt', 'Breast_MRI_915.pt', 'Breast_MRI_916.pt', 'Breast_MRI_917.pt', 'Breast_MRI_918.pt', 'Breast_MRI_919.pt', 'Breast_MRI_920.pt', 'Breast_MRI_921.pt', 'Breast_MRI_922.pt']
row 0: Patient ID                                  Breast_MRI_001
Date of Birth (Days)                              0.705925
Image Position of Patient (Z)                     0.746579
Image Position of Patient (Y)                     0.180863
Days to MRI (From the Date of Diagnosis)          0.255952
Image Position of Patient (X)                     0.068046
TR (Repetition Time)                              0.150454
TE (Echo Time)                                    0.073041
Oncotype score                                    0.241581
Tumor Grade(N)\n(Nuclear)_3.0                            0
Staging(Tumor Size)# [T]_2.0                             1
Tumor Progression_0                                      0
Tumor Progression_1                                      1
Name: 0, dtype: object
Epoch 1/50
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[-0.396728515625, -0.486083984375]
[0.1055908203125, -0.3544921875]
[-0.31494140625, -0.37548828125]
[0.04278564453125, -0.314208984375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.396728515625, -0.486083984375]
[0.1055908203125, -0.3544921875]
[-0.31494140625, -0.37548828125]
[0.04278564453125, -0.314208984375]
This is the real loss :  tensor(0.7497, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:   Epoch  Batch  ...      Loss                                        True Value
0     0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[1 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[-0.385986328125, -0.437255859375]
[-0.302978515625, -0.39208984375]
[0.04473876953125, -0.3212890625]
[0.1461181640625, -0.2861328125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[-0.385986328125, -0.437255859375]
[-0.302978515625, -0.39208984375]
[0.04473876953125, -0.3212890625]
[0.1461181640625, -0.2861328125]
This is the real loss :  tensor(0.7238, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:   Epoch  Batch  ...      Loss                                        True Value
0     0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1     0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[2 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.0830078125, -0.298095703125]
[0.3056640625, -0.4453125]
[-0.06585693359375, -0.303955078125]
[0.045501708984375, -0.301513671875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.0830078125, -0.298095703125]
[0.3056640625, -0.4453125]
[-0.06585693359375, -0.303955078125]
[0.045501708984375, -0.301513671875]
This is the real loss :  tensor(0.6349, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:   Epoch  Batch  ...      Loss                                        True Value
0     0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1     0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2     0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[3 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.1490478515625, -0.05865478515625]
[0.0141754150390625, -0.07525634765625]
[0.55517578125, -0.76123046875]
[-0.118896484375, -0.1387939453125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1490478515625, -0.05865478515625]
[0.0141754150390625, -0.07525634765625]
[0.55517578125, -0.76123046875]
[-0.118896484375, -0.1387939453125]
This is the real loss :  tensor(0.4692, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:   Epoch  Batch  ...      Loss                                        True Value
0     0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1     0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2     0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3     0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[4 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.1376953125, -0.364501953125]
[0.432861328125, 0.04168701171875]
[0.36181640625, -0.1260986328125]
[0.27978515625, -0.533203125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.1376953125, -0.364501953125]
[0.432861328125, 0.04168701171875]
[0.36181640625, -0.1260986328125]
[0.27978515625, -0.533203125]
This is the real loss :  tensor(0.4252, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:   Epoch  Batch  ...      Loss                                        True Value
0     0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1     0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2     0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3     0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4     0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[5 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.67041015625, -0.5986328125]
[0.1195068359375, 0.003467559814453125]
[0.22119140625, 0.0518798828125]
[0.348876953125, -0.055419921875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.67041015625, -0.5986328125]
[0.1195068359375, 0.003467559814453125]
[0.22119140625, 0.0518798828125]
[0.348876953125, -0.055419921875]
This is the real loss :  tensor(0.6021, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:   Epoch  Batch  ...      Loss                                        True Value
0     0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1     0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2     0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3     0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4     0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5     0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[6 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.260009765625, -0.146484375]
[0.83349609375, 0.0748291015625]
[0.2235107421875, -0.11163330078125]
[0.2086181640625, -0.1639404296875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.260009765625, -0.146484375]
[0.83349609375, 0.0748291015625]
[0.2235107421875, -0.11163330078125]
[0.2086181640625, -0.1639404296875]
This is the real loss :  tensor(0.4235, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:   Epoch  Batch  ...      Loss                                        True Value
0     0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1     0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2     0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3     0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4     0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5     0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6     0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[7 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.53271484375, -0.241943359375]
[0.32080078125, 0.05889892578125]
[0.416748046875, 0.0784912109375]
[0.326416015625, -0.032318115234375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.53271484375, -0.241943359375]
[0.32080078125, 0.05889892578125]
[0.416748046875, 0.0784912109375]
[0.326416015625, -0.032318115234375]
This is the real loss :  tensor(0.2825, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:   Epoch  Batch  ...      Loss                                        True Value
0     0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1     0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2     0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3     0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4     0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5     0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6     0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7     0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[8 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.447998046875, -0.1767578125]
[0.396484375, 0.1861572265625]
[0.354736328125, -0.1494140625]
[0.54443359375, 0.1588134765625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.447998046875, -0.1767578125]
[0.396484375, 0.1861572265625]
[0.354736328125, -0.1494140625]
[0.54443359375, 0.1588134765625]
This is the real loss :  tensor(0.1758, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:   Epoch  Batch  ...      Loss                                        True Value
0     0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1     0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2     0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3     0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4     0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5     0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6     0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7     0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8     0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[9 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.34765625, -0.01325225830078125]
[0.26708984375, -0.10369873046875]
[0.384521484375, 0.1224365234375]
[0.419677734375, 0.119140625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.34765625, -0.01325225830078125]
[0.26708984375, -0.10369873046875]
[0.384521484375, 0.1224365234375]
[0.419677734375, 0.119140625]
This is the real loss :  tensor(0.3730, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:   Epoch  Batch  ...      Loss                                        True Value
0     0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1     0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2     0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3     0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4     0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5     0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6     0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7     0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8     0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9     0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[10 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.41259765625, -0.0635986328125]
[0.66748046875, 0.2393798828125]
[0.363037109375, 0.0026683807373046875]
[0.2490234375, -0.016815185546875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.41259765625, -0.0635986328125]
[0.66748046875, 0.2393798828125]
[0.363037109375, 0.0026683807373046875]
[0.2490234375, -0.016815185546875]
This is the real loss :  tensor(0.1859, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[11 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.387939453125, 0.0009007453918457031]
[0.3876953125, 0.054656982421875]
[0.5771484375, 0.2496337890625]
[0.337646484375, -0.0836181640625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.387939453125, 0.0009007453918457031]
[0.3876953125, 0.054656982421875]
[0.5771484375, 0.2496337890625]
[0.337646484375, -0.0836181640625]
This is the real loss :  tensor(0.1799, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[12 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.51220703125, 0.16748046875]
[0.63671875, 0.1510009765625]
[0.452392578125, -0.05450439453125]
[0.285400390625, -0.054931640625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.51220703125, 0.16748046875]
[0.63671875, 0.1510009765625]
[0.452392578125, -0.05450439453125]
[0.285400390625, -0.054931640625]
This is the real loss :  tensor(0.2397, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[13 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.64453125, 0.09844970703125]
[0.58154296875, 0.1455078125]
[0.294921875, 0.0193328857421875]
[0.227294921875, -0.047882080078125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.64453125, 0.09844970703125]
[0.58154296875, 0.1455078125]
[0.294921875, 0.0193328857421875]
[0.227294921875, -0.047882080078125]
This is the real loss :  tensor(0.2475, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[14 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.60205078125, 0.067138671875]
[0.35546875, 0.0196990966796875]
[0.51123046875, 0.040008544921875]
[0.52734375, -0.002696990966796875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.60205078125, 0.067138671875]
[0.35546875, 0.0196990966796875]
[0.51123046875, 0.040008544921875]
[0.52734375, -0.002696990966796875]
This is the real loss :  tensor(0.2641, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[15 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.337158203125, 0.00437164306640625]
[0.420654296875, 0.0014829635620117188]
[0.76904296875, 0.13037109375]
[0.5048828125, 0.046356201171875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.337158203125, 0.00437164306640625]
[0.420654296875, 0.0014829635620117188]
[0.76904296875, 0.13037109375]
[0.5048828125, 0.046356201171875]
This is the real loss :  tensor(0.1366, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[16 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.60205078125, 0.0193939208984375]
[0.55224609375, 0.0095977783203125]
[0.43603515625, -0.003063201904296875]
[0.8798828125, 0.1104736328125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.60205078125, 0.0193939208984375]
[0.55224609375, 0.0095977783203125]
[0.43603515625, -0.003063201904296875]
[0.8798828125, 0.1104736328125]
This is the real loss :  tensor(0.2237, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[17 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.5341796875, 0.091796875]
[0.62890625, 0.00531768798828125]
[0.7958984375, 0.05267333984375]
[0.367919921875, -0.0297393798828125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.5341796875, 0.091796875]
[0.62890625, 0.00531768798828125]
[0.7958984375, 0.05267333984375]
[0.367919921875, -0.0297393798828125]
This is the real loss :  tensor(0.1010, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[18 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.454345703125, 0.0004253387451171875]
[0.89599609375, 0.025604248046875]
[0.348388671875, 0.013885498046875]
[0.415283203125, -0.005840301513671875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.454345703125, 0.0004253387451171875]
[0.89599609375, 0.025604248046875]
[0.348388671875, 0.013885498046875]
[0.415283203125, -0.005840301513671875]
This is the real loss :  tensor(0.4656, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[19 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.60791015625, 0.1680908203125]
[0.6005859375, 0.002040863037109375]
[0.304443359375, 0.04705810546875]
[1.015625, -0.0316162109375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.60791015625, 0.1680908203125]
[0.6005859375, 0.002040863037109375]
[0.304443359375, 0.04705810546875]
[1.015625, -0.0316162109375]
This is the real loss :  tensor(0.1036, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[20 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.476318359375, -0.0012989044189453125]
[1.240234375, -0.0595703125]
[0.232421875, 0.047393798828125]
[0.62353515625, 0.09002685546875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.476318359375, -0.0012989044189453125]
[1.240234375, -0.0595703125]
[0.232421875, 0.047393798828125]
[0.62353515625, 0.09002685546875]
This is the real loss :  tensor(0.1346, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[21 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.720703125, -0.050933837890625]
[0.445556640625, 0.1314697265625]
[0.67333984375, 0.03546142578125]
[0.8154296875, 0.03826904296875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.720703125, -0.050933837890625]
[0.445556640625, 0.1314697265625]
[0.67333984375, 0.03546142578125]
[0.8154296875, 0.03826904296875]
This is the real loss :  tensor(0.2281, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[22 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.52490234375, 0.11077880859375]
[0.5361328125, -0.003261566162109375]
[1.0615234375, 0.01800537109375]
[0.53564453125, 0.0159149169921875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.52490234375, 0.11077880859375]
[0.5361328125, -0.003261566162109375]
[1.0615234375, 0.01800537109375]
[0.53564453125, 0.0159149169921875]
This is the real loss :  tensor(0.3489, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[23 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.83251953125, 0.055816650390625]
[0.474609375, 0.031341552734375]
[0.720703125, 0.12127685546875]
[0.63818359375, 0.053863525390625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.83251953125, 0.055816650390625]
[0.474609375, 0.031341552734375]
[0.720703125, 0.12127685546875]
[0.63818359375, 0.053863525390625]
This is the real loss :  tensor(0.0668, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[24 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.6611328125, 0.058197021484375]
[0.66455078125, 0.035125732421875]
[0.60009765625, 0.041168212890625]
[0.80859375, 0.11566162109375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.6611328125, 0.058197021484375]
[0.66455078125, 0.035125732421875]
[0.60009765625, 0.041168212890625]
[0.80859375, 0.11566162109375]
This is the real loss :  tensor(0.2287, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[25 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.498291015625, 0.0322265625]
[0.4951171875, 0.0374755859375]
[0.904296875, 0.10736083984375]
[0.724609375, 0.2191162109375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.498291015625, 0.0322265625]
[0.4951171875, 0.0374755859375]
[0.904296875, 0.10736083984375]
[0.724609375, 0.2191162109375]
This is the real loss :  tensor(0.0817, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[26 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.60546875, 0.02801513671875]
[0.775390625, 0.1568603515625]
[1.01953125, 0.2227783203125]
[0.69287109375, 0.036102294921875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.60546875, 0.02801513671875]
[0.775390625, 0.1568603515625]
[1.01953125, 0.2227783203125]
[0.69287109375, 0.036102294921875]
This is the real loss :  tensor(0.0471, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[27 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.67822265625, 0.1376953125]
[0.7548828125, 0.044891357421875]
[0.69482421875, 0.015533447265625]
[0.69775390625, 0.26611328125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.67822265625, 0.1376953125]
[0.7548828125, 0.044891357421875]
[0.69482421875, 0.015533447265625]
[0.69775390625, 0.26611328125]
This is the real loss :  tensor(0.0550, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.055018  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[28 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.63037109375, 0.10675048828125]
[1.21484375, 0.07855224609375]
[0.712890625, 0.123046875]
[0.82861328125, 0.2022705078125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.63037109375, 0.10675048828125]
[1.21484375, 0.07855224609375]
[0.712890625, 0.123046875]
[0.82861328125, 0.2022705078125]
This is the real loss :  tensor(0.2026, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.055018  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
28     0    NaN  ...  0.202612  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[29 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.70166015625, 0.1768798828125]
[0.75927734375, 0.18212890625]
[0.6162109375, 0.08056640625]
[0.73974609375, 0.007045745849609375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.70166015625, 0.1768798828125]
[0.75927734375, 0.18212890625]
[0.6162109375, 0.08056640625]
[0.73974609375, 0.007045745849609375]
This is the real loss :  tensor(0.2373, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.055018  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
28     0    NaN  ...  0.202612  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.237297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[30 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.77197265625, 0.284912109375]
[1.06640625, 0.03826904296875]
[0.71240234375, 0.143310546875]
[0.56982421875, 0.059783935546875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.77197265625, 0.284912109375]
[1.06640625, 0.03826904296875]
[0.71240234375, 0.143310546875]
[0.56982421875, 0.059783935546875]
This is the real loss :  tensor(0.0539, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.055018  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
28     0    NaN  ...  0.202612  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.237297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
30     0    NaN  ...  0.053865  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[31 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.7822265625, -0.016998291015625]
[0.73095703125, 0.1455078125]
[0.74951171875, 0.28515625]
[0.935546875, 0.038116455078125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7822265625, -0.016998291015625]
[0.73095703125, 0.1455078125]
[0.74951171875, 0.28515625]
[0.935546875, 0.038116455078125]
This is the real loss :  tensor(0.0364, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.055018  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
28     0    NaN  ...  0.202612  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.237297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
30     0    NaN  ...  0.053865  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
31     0    NaN  ...  0.036367  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[32 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[1.0224609375, 0.251708984375]
[0.54296875, 0.10589599609375]
[0.654296875, 0.06170654296875]
[0.904296875, -0.0003032684326171875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[1.0224609375, 0.251708984375]
[0.54296875, 0.10589599609375]
[0.654296875, 0.06170654296875]
[0.904296875, -0.0003032684326171875]
This is the real loss :  tensor(0.0521, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.055018  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
28     0    NaN  ...  0.202612  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.237297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
30     0    NaN  ...  0.053865  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
31     0    NaN  ...  0.036367  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
32     0    NaN  ...  0.052054  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[33 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.9970703125, -0.0253448486328125]
[0.6513671875, 0.0699462890625]
[0.74951171875, 0.2607421875]
[0.8232421875, 0.07806396484375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.9970703125, -0.0253448486328125]
[0.6513671875, 0.0699462890625]
[0.74951171875, 0.2607421875]
[0.8232421875, 0.07806396484375]
This is the real loss :  tensor(0.0369, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.055018  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
28     0    NaN  ...  0.202612  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.237297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
30     0    NaN  ...  0.053865  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
31     0    NaN  ...  0.036367  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
32     0    NaN  ...  0.052054  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
33     0    NaN  ...  0.036895  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[34 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.90283203125, 0.1734619140625]
[0.6884765625, 0.06390380859375]
[0.80517578125, 0.16455078125]
[0.96435546875, -0.00775909423828125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.90283203125, 0.1734619140625]
[0.6884765625, 0.06390380859375]
[0.80517578125, 0.16455078125]
[0.96435546875, -0.00775909423828125]
This is the real loss :  tensor(0.2082, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.055018  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
28     0    NaN  ...  0.202612  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.237297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
30     0    NaN  ...  0.053865  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
31     0    NaN  ...  0.036367  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
32     0    NaN  ...  0.052054  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
33     0    NaN  ...  0.036895  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
34     0    NaN  ...  0.208221  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[35 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.876953125, 0.0028362274169921875]
[0.8203125, 0.1727294921875]
[0.83935546875, 0.09613037109375]
[0.81103515625, 0.1571044921875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.876953125, 0.0028362274169921875]
[0.8203125, 0.1727294921875]
[0.83935546875, 0.09613037109375]
[0.81103515625, 0.1571044921875]
This is the real loss :  tensor(0.3470, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.055018  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
28     0    NaN  ...  0.202612  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.237297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
30     0    NaN  ...  0.053865  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
31     0    NaN  ...  0.036367  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
32     0    NaN  ...  0.052054  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
33     0    NaN  ...  0.036895  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
34     0    NaN  ...  0.208221  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
35     0    NaN  ...  0.346967  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[36 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[1.025390625, -0.037689208984375]
[0.8056640625, 0.1387939453125]
[0.74755859375, 0.22509765625]
[0.79541015625, 0.160400390625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[1.025390625, -0.037689208984375]
[0.8056640625, 0.1387939453125]
[0.74755859375, 0.22509765625]
[0.79541015625, 0.160400390625]
This is the real loss :  tensor(0.0301, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.055018  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
28     0    NaN  ...  0.202612  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.237297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
30     0    NaN  ...  0.053865  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
31     0    NaN  ...  0.036367  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
32     0    NaN  ...  0.052054  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
33     0    NaN  ...  0.036895  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
34     0    NaN  ...  0.208221  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
35     0    NaN  ...  0.346967  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
36     0    NaN  ...  0.030135  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[37 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.77197265625, 0.0740966796875]
[0.7734375, -0.0091552734375]
[0.81982421875, 0.1197509765625]
[0.615234375, 0.2152099609375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.77197265625, 0.0740966796875]
[0.7734375, -0.0091552734375]
[0.81982421875, 0.1197509765625]
[0.615234375, 0.2152099609375]
This is the real loss :  tensor(0.0438, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.055018  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
28     0    NaN  ...  0.202612  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.237297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
30     0    NaN  ...  0.053865  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
31     0    NaN  ...  0.036367  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
32     0    NaN  ...  0.052054  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
33     0    NaN  ...  0.036895  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
34     0    NaN  ...  0.208221  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
35     0    NaN  ...  0.346967  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
36     0    NaN  ...  0.030135  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
37     0    NaN  ...  0.043758  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[38 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.90478515625, -0.0047454833984375]
[0.7265625, 0.2086181640625]
[0.76123046875, 0.15380859375]
[0.8720703125, 0.00937652587890625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.90478515625, -0.0047454833984375]
[0.7265625, 0.2086181640625]
[0.76123046875, 0.15380859375]
[0.8720703125, 0.00937652587890625]
This is the real loss :  tensor(0.0281, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.055018  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
28     0    NaN  ...  0.202612  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.237297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
30     0    NaN  ...  0.053865  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
31     0    NaN  ...  0.036367  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
32     0    NaN  ...  0.052054  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
33     0    NaN  ...  0.036895  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
34     0    NaN  ...  0.208221  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
35     0    NaN  ...  0.346967  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
36     0    NaN  ...  0.030135  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
37     0    NaN  ...  0.043758  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
38     0    NaN  ...  0.028062  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[39 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.89306640625, 0.1265869140625]
[0.75146484375, 0.1533203125]
[0.6904296875, 0.216796875]
[1.1337890625, -0.0042724609375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.89306640625, 0.1265869140625]
[0.75146484375, 0.1533203125]
[0.6904296875, 0.216796875]
[1.1337890625, -0.0042724609375]
This is the real loss :  tensor(0.3753, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.055018  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
28     0    NaN  ...  0.202612  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.237297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
30     0    NaN  ...  0.053865  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
31     0    NaN  ...  0.036367  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
32     0    NaN  ...  0.052054  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
33     0    NaN  ...  0.036895  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
34     0    NaN  ...  0.208221  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
35     0    NaN  ...  0.346967  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
36     0    NaN  ...  0.030135  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
37     0    NaN  ...  0.043758  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
38     0    NaN  ...  0.028062  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.375342  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[40 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.69970703125, 0.2078857421875]
[1.21484375, 0.009735107421875]
[0.634765625, 0.2135009765625]
[0.84228515625, 0.08477783203125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.69970703125, 0.2078857421875]
[1.21484375, 0.009735107421875]
[0.634765625, 0.2135009765625]
[0.84228515625, 0.08477783203125]
This is the real loss :  tensor(0.0488, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.055018  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
28     0    NaN  ...  0.202612  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.237297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
30     0    NaN  ...  0.053865  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
31     0    NaN  ...  0.036367  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
32     0    NaN  ...  0.052054  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
33     0    NaN  ...  0.036895  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
34     0    NaN  ...  0.208221  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
35     0    NaN  ...  0.346967  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
36     0    NaN  ...  0.030135  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
37     0    NaN  ...  0.043758  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
38     0    NaN  ...  0.028062  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.375342  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
40     0    NaN  ...  0.048836  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[41 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.783203125, 0.011871337890625]
[0.859375, 0.026519775390625]
[0.7470703125, 0.2271728515625]
[0.84912109375, -0.0128936767578125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.783203125, 0.011871337890625]
[0.859375, 0.026519775390625]
[0.7470703125, 0.2271728515625]
[0.84912109375, -0.0128936767578125]
This is the real loss :  tensor(0.0258, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.055018  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
28     0    NaN  ...  0.202612  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.237297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
30     0    NaN  ...  0.053865  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
31     0    NaN  ...  0.036367  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
32     0    NaN  ...  0.052054  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
33     0    NaN  ...  0.036895  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
34     0    NaN  ...  0.208221  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
35     0    NaN  ...  0.346967  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
36     0    NaN  ...  0.030135  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
37     0    NaN  ...  0.043758  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
38     0    NaN  ...  0.028062  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.375342  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
40     0    NaN  ...  0.048836  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
41     0    NaN  ...  0.025767  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[42 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.76220703125, 0.1719970703125]
[0.8798828125, 0.0222930908203125]
[0.84130859375, 0.07598876953125]
[0.75537109375, 0.09954833984375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.76220703125, 0.1719970703125]
[0.8798828125, 0.0222930908203125]
[0.84130859375, 0.07598876953125]
[0.75537109375, 0.09954833984375]
This is the real loss :  tensor(0.3805, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.055018  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
28     0    NaN  ...  0.202612  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.237297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
30     0    NaN  ...  0.053865  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
31     0    NaN  ...  0.036367  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
32     0    NaN  ...  0.052054  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
33     0    NaN  ...  0.036895  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
34     0    NaN  ...  0.208221  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
35     0    NaN  ...  0.346967  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
36     0    NaN  ...  0.030135  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
37     0    NaN  ...  0.043758  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
38     0    NaN  ...  0.028062  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.375342  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
40     0    NaN  ...  0.048836  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
41     0    NaN  ...  0.025767  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
42     0    NaN  ...  0.380506  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[43 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.81005859375, 0.1417236328125]
[0.7646484375, 0.11328125]
[0.7333984375, 0.16455078125]
[0.8515625, 0.11016845703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.81005859375, 0.1417236328125]
[0.7646484375, 0.11328125]
[0.7333984375, 0.16455078125]
[0.8515625, 0.11016845703125]
This is the real loss :  tensor(0.2174, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.055018  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
28     0    NaN  ...  0.202612  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.237297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
30     0    NaN  ...  0.053865  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
31     0    NaN  ...  0.036367  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
32     0    NaN  ...  0.052054  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
33     0    NaN  ...  0.036895  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
34     0    NaN  ...  0.208221  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
35     0    NaN  ...  0.346967  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
36     0    NaN  ...  0.030135  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
37     0    NaN  ...  0.043758  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
38     0    NaN  ...  0.028062  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.375342  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
40     0    NaN  ...  0.048836  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
41     0    NaN  ...  0.025767  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
42     0    NaN  ...  0.380506  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
43     0    NaN  ...  0.217437  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[44 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.81005859375, 0.1378173828125]
[1.08984375, 0.044708251953125]
[0.892578125, 0.114013671875]
[0.86572265625, 0.1829833984375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.81005859375, 0.1378173828125]
[1.08984375, 0.044708251953125]
[0.892578125, 0.114013671875]
[0.86572265625, 0.1829833984375]
This is the real loss :  tensor(0.1857, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.055018  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
28     0    NaN  ...  0.202612  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.237297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
30     0    NaN  ...  0.053865  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
31     0    NaN  ...  0.036367  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
32     0    NaN  ...  0.052054  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
33     0    NaN  ...  0.036895  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
34     0    NaN  ...  0.208221  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
35     0    NaN  ...  0.346967  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
36     0    NaN  ...  0.030135  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
37     0    NaN  ...  0.043758  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
38     0    NaN  ...  0.028062  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.375342  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
40     0    NaN  ...  0.048836  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
41     0    NaN  ...  0.025767  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
42     0    NaN  ...  0.380506  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
43     0    NaN  ...  0.217437  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
44     0    NaN  ...  0.185710  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[45 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[1.025390625, -0.015655517578125]
[0.73876953125, 0.230224609375]
[0.90087890625, 0.0997314453125]
[0.953125, 0.09393310546875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[1.025390625, -0.015655517578125]
[0.73876953125, 0.230224609375]
[0.90087890625, 0.0997314453125]
[0.953125, 0.09393310546875]
This is the real loss :  tensor(0.2339, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.055018  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
28     0    NaN  ...  0.202612  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.237297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
30     0    NaN  ...  0.053865  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
31     0    NaN  ...  0.036367  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
32     0    NaN  ...  0.052054  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
33     0    NaN  ...  0.036895  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
34     0    NaN  ...  0.208221  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
35     0    NaN  ...  0.346967  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
36     0    NaN  ...  0.030135  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
37     0    NaN  ...  0.043758  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
38     0    NaN  ...  0.028062  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.375342  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
40     0    NaN  ...  0.048836  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
41     0    NaN  ...  0.025767  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
42     0    NaN  ...  0.380506  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
43     0    NaN  ...  0.217437  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
44     0    NaN  ...  0.185710  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
45     0    NaN  ...  0.233914  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[46 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.7900390625, 0.1494140625]
[0.86279296875, 0.08758544921875]
[0.8359375, 0.057708740234375]
[0.8369140625, 0.12359619140625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7900390625, 0.1494140625]
[0.86279296875, 0.08758544921875]
[0.8359375, 0.057708740234375]
[0.8369140625, 0.12359619140625]
This is the real loss :  tensor(0.2152, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.055018  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
28     0    NaN  ...  0.202612  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.237297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
30     0    NaN  ...  0.053865  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
31     0    NaN  ...  0.036367  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
32     0    NaN  ...  0.052054  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
33     0    NaN  ...  0.036895  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
34     0    NaN  ...  0.208221  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
35     0    NaN  ...  0.346967  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
36     0    NaN  ...  0.030135  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
37     0    NaN  ...  0.043758  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
38     0    NaN  ...  0.028062  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.375342  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
40     0    NaN  ...  0.048836  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
41     0    NaN  ...  0.025767  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
42     0    NaN  ...  0.380506  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
43     0    NaN  ...  0.217437  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
44     0    NaN  ...  0.185710  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
45     0    NaN  ...  0.233914  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
46     0    NaN  ...  0.215185  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[47 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.89111328125, 0.1693115234375]
[0.76025390625, 0.225830078125]
[0.88671875, -0.0031719207763671875]
[0.77197265625, 0.07415771484375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.89111328125, 0.1693115234375]
[0.76025390625, 0.225830078125]
[0.88671875, -0.0031719207763671875]
[0.77197265625, 0.07415771484375]
This is the real loss :  tensor(0.0274, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.055018  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
28     0    NaN  ...  0.202612  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.237297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
30     0    NaN  ...  0.053865  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
31     0    NaN  ...  0.036367  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
32     0    NaN  ...  0.052054  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
33     0    NaN  ...  0.036895  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
34     0    NaN  ...  0.208221  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
35     0    NaN  ...  0.346967  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
36     0    NaN  ...  0.030135  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
37     0    NaN  ...  0.043758  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
38     0    NaN  ...  0.028062  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.375342  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
40     0    NaN  ...  0.048836  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
41     0    NaN  ...  0.025767  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
42     0    NaN  ...  0.380506  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
43     0    NaN  ...  0.217437  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
44     0    NaN  ...  0.185710  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
45     0    NaN  ...  0.233914  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
46     0    NaN  ...  0.215185  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
47     0    NaN  ...  0.027417  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[48 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.9990234375, 0.04302978515625]
[0.79296875, 0.240966796875]
[0.67333984375, 0.0244598388671875]
[0.93505859375, 0.2081298828125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.9990234375, 0.04302978515625]
[0.79296875, 0.240966796875]
[0.67333984375, 0.0244598388671875]
[0.93505859375, 0.2081298828125]
This is the real loss :  tensor(0.0322, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.055018  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
28     0    NaN  ...  0.202612  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.237297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
30     0    NaN  ...  0.053865  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
31     0    NaN  ...  0.036367  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
32     0    NaN  ...  0.052054  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
33     0    NaN  ...  0.036895  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
34     0    NaN  ...  0.208221  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
35     0    NaN  ...  0.346967  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
36     0    NaN  ...  0.030135  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
37     0    NaN  ...  0.043758  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
38     0    NaN  ...  0.028062  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.375342  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
40     0    NaN  ...  0.048836  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
41     0    NaN  ...  0.025767  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
42     0    NaN  ...  0.380506  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
43     0    NaN  ...  0.217437  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
44     0    NaN  ...  0.185710  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
45     0    NaN  ...  0.233914  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
46     0    NaN  ...  0.215185  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
47     0    NaN  ...  0.027417  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
48     0    NaN  ...  0.032203  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[49 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.64697265625, 0.1611328125]
[0.9453125, 0.032501220703125]
[0.734375, 0.2236328125]
[1.0615234375, 0.046630859375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.64697265625, 0.1611328125]
[0.9453125, 0.032501220703125]
[0.734375, 0.2236328125]
[1.0615234375, 0.046630859375]
This is the real loss :  tensor(0.0351, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.055018  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
28     0    NaN  ...  0.202612  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.237297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
30     0    NaN  ...  0.053865  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
31     0    NaN  ...  0.036367  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
32     0    NaN  ...  0.052054  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
33     0    NaN  ...  0.036895  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
34     0    NaN  ...  0.208221  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
35     0    NaN  ...  0.346967  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
36     0    NaN  ...  0.030135  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
37     0    NaN  ...  0.043758  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
38     0    NaN  ...  0.028062  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.375342  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
40     0    NaN  ...  0.048836  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
41     0    NaN  ...  0.025767  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
42     0    NaN  ...  0.380506  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
43     0    NaN  ...  0.217437  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
44     0    NaN  ...  0.185710  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
45     0    NaN  ...  0.233914  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
46     0    NaN  ...  0.215185  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
47     0    NaN  ...  0.027417  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
48     0    NaN  ...  0.032203  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
49     0    NaN  ...  0.035146  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[50 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[1.3251953125, 0.053741455078125]
[0.7802734375, 0.1668701171875]
[0.76513671875, 0.253662109375]
[0.76171875, 0.2041015625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[1.3251953125, 0.053741455078125]
[0.7802734375, 0.1668701171875]
[0.76513671875, 0.253662109375]
[0.76171875, 0.2041015625]
This is the real loss :  tensor(0.2037, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.055018  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
28     0    NaN  ...  0.202612  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.237297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
30     0    NaN  ...  0.053865  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
31     0    NaN  ...  0.036367  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
32     0    NaN  ...  0.052054  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
33     0    NaN  ...  0.036895  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
34     0    NaN  ...  0.208221  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
35     0    NaN  ...  0.346967  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
36     0    NaN  ...  0.030135  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
37     0    NaN  ...  0.043758  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
38     0    NaN  ...  0.028062  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.375342  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
40     0    NaN  ...  0.048836  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
41     0    NaN  ...  0.025767  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
42     0    NaN  ...  0.380506  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
43     0    NaN  ...  0.217437  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
44     0    NaN  ...  0.185710  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
45     0    NaN  ...  0.233914  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
46     0    NaN  ...  0.215185  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
47     0    NaN  ...  0.027417  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
48     0    NaN  ...  0.032203  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
49     0    NaN  ...  0.035146  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
50     0    NaN  ...  0.203689  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[51 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.78857421875, 0.1640625]
[0.73974609375, 0.1353759765625]
[0.82080078125, 0.041412353515625]
[0.87890625, 0.115966796875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.78857421875, 0.1640625]
[0.73974609375, 0.1353759765625]
[0.82080078125, 0.041412353515625]
[0.87890625, 0.115966796875]
This is the real loss :  tensor(0.1836, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.055018  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
28     0    NaN  ...  0.202612  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.237297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
30     0    NaN  ...  0.053865  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
31     0    NaN  ...  0.036367  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
32     0    NaN  ...  0.052054  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
33     0    NaN  ...  0.036895  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
34     0    NaN  ...  0.208221  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
35     0    NaN  ...  0.346967  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
36     0    NaN  ...  0.030135  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
37     0    NaN  ...  0.043758  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
38     0    NaN  ...  0.028062  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.375342  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
40     0    NaN  ...  0.048836  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
41     0    NaN  ...  0.025767  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
42     0    NaN  ...  0.380506  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
43     0    NaN  ...  0.217437  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
44     0    NaN  ...  0.185710  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
45     0    NaN  ...  0.233914  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
46     0    NaN  ...  0.215185  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
47     0    NaN  ...  0.027417  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
48     0    NaN  ...  0.032203  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
49     0    NaN  ...  0.035146  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
50     0    NaN  ...  0.203689  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
51     0    NaN  ...  0.183580  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[52 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.9404296875, 0.13671875]
[0.78759765625, 0.184326171875]
[1.0283203125, 0.1102294921875]
[0.87939453125, 0.0902099609375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.9404296875, 0.13671875]
[0.78759765625, 0.184326171875]
[1.0283203125, 0.1102294921875]
[0.87939453125, 0.0902099609375]
This is the real loss :  tensor(0.2466, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.055018  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
28     0    NaN  ...  0.202612  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.237297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
30     0    NaN  ...  0.053865  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
31     0    NaN  ...  0.036367  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
32     0    NaN  ...  0.052054  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
33     0    NaN  ...  0.036895  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
34     0    NaN  ...  0.208221  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
35     0    NaN  ...  0.346967  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
36     0    NaN  ...  0.030135  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
37     0    NaN  ...  0.043758  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
38     0    NaN  ...  0.028062  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.375342  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
40     0    NaN  ...  0.048836  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
41     0    NaN  ...  0.025767  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
42     0    NaN  ...  0.380506  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
43     0    NaN  ...  0.217437  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
44     0    NaN  ...  0.185710  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
45     0    NaN  ...  0.233914  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
46     0    NaN  ...  0.215185  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
47     0    NaN  ...  0.027417  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
48     0    NaN  ...  0.032203  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
49     0    NaN  ...  0.035146  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
50     0    NaN  ...  0.203689  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
51     0    NaN  ...  0.183580  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
52     0    NaN  ...  0.246644  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[53 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[1.0908203125, 0.06927490234375]
[0.74755859375, 0.2039794921875]
[0.80029296875, 0.2080078125]
[0.81494140625, 0.19091796875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[1.0908203125, 0.06927490234375]
[0.74755859375, 0.2039794921875]
[0.80029296875, 0.2080078125]
[0.81494140625, 0.19091796875]
This is the real loss :  tensor(0.0340, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.055018  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
28     0    NaN  ...  0.202612  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.237297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
30     0    NaN  ...  0.053865  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
31     0    NaN  ...  0.036367  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
32     0    NaN  ...  0.052054  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
33     0    NaN  ...  0.036895  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
34     0    NaN  ...  0.208221  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
35     0    NaN  ...  0.346967  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
36     0    NaN  ...  0.030135  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
37     0    NaN  ...  0.043758  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
38     0    NaN  ...  0.028062  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.375342  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
40     0    NaN  ...  0.048836  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
41     0    NaN  ...  0.025767  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
42     0    NaN  ...  0.380506  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
43     0    NaN  ...  0.217437  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
44     0    NaN  ...  0.185710  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
45     0    NaN  ...  0.233914  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
46     0    NaN  ...  0.215185  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
47     0    NaN  ...  0.027417  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
48     0    NaN  ...  0.032203  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
49     0    NaN  ...  0.035146  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
50     0    NaN  ...  0.203689  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
51     0    NaN  ...  0.183580  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
52     0    NaN  ...  0.246644  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
53     0    NaN  ...  0.034029  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[54 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.79248046875, 0.1298828125]
[0.8623046875, 0.2135009765625]
[0.74365234375, 0.060760498046875]
[0.6435546875, 0.1905517578125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.79248046875, 0.1298828125]
[0.8623046875, 0.2135009765625]
[0.74365234375, 0.060760498046875]
[0.6435546875, 0.1905517578125]
This is the real loss :  tensor(0.0447, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.055018  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
28     0    NaN  ...  0.202612  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.237297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
30     0    NaN  ...  0.053865  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
31     0    NaN  ...  0.036367  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
32     0    NaN  ...  0.052054  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
33     0    NaN  ...  0.036895  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
34     0    NaN  ...  0.208221  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
35     0    NaN  ...  0.346967  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
36     0    NaN  ...  0.030135  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
37     0    NaN  ...  0.043758  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
38     0    NaN  ...  0.028062  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.375342  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
40     0    NaN  ...  0.048836  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
41     0    NaN  ...  0.025767  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
42     0    NaN  ...  0.380506  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
43     0    NaN  ...  0.217437  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
44     0    NaN  ...  0.185710  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
45     0    NaN  ...  0.233914  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
46     0    NaN  ...  0.215185  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
47     0    NaN  ...  0.027417  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
48     0    NaN  ...  0.032203  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
49     0    NaN  ...  0.035146  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
50     0    NaN  ...  0.203689  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
51     0    NaN  ...  0.183580  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
52     0    NaN  ...  0.246644  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
53     0    NaN  ...  0.034029  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
54     0    NaN  ...  0.044656  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[55 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.98291015625, 0.2215576171875]
[0.86572265625, 0.14697265625]
[0.7822265625, 0.0869140625]
[0.77978515625, 0.0867919921875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.98291015625, 0.2215576171875]
[0.86572265625, 0.14697265625]
[0.7822265625, 0.0869140625]
[0.77978515625, 0.0867919921875]
This is the real loss :  tensor(0.2153, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.055018  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
28     0    NaN  ...  0.202612  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.237297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
30     0    NaN  ...  0.053865  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
31     0    NaN  ...  0.036367  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
32     0    NaN  ...  0.052054  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
33     0    NaN  ...  0.036895  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
34     0    NaN  ...  0.208221  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
35     0    NaN  ...  0.346967  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
36     0    NaN  ...  0.030135  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
37     0    NaN  ...  0.043758  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
38     0    NaN  ...  0.028062  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.375342  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
40     0    NaN  ...  0.048836  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
41     0    NaN  ...  0.025767  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
42     0    NaN  ...  0.380506  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
43     0    NaN  ...  0.217437  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
44     0    NaN  ...  0.185710  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
45     0    NaN  ...  0.233914  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
46     0    NaN  ...  0.215185  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
47     0    NaN  ...  0.027417  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
48     0    NaN  ...  0.032203  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
49     0    NaN  ...  0.035146  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
50     0    NaN  ...  0.203689  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
51     0    NaN  ...  0.183580  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
52     0    NaN  ...  0.246644  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
53     0    NaN  ...  0.034029  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
54     0    NaN  ...  0.044656  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
55     0    NaN  ...  0.215340  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[56 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.85986328125, 0.288330078125]
[0.853515625, 0.048095703125]
[0.9990234375, 0.11572265625]
[0.73974609375, 0.302001953125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.85986328125, 0.288330078125]
[0.853515625, 0.048095703125]
[0.9990234375, 0.11572265625]
[0.73974609375, 0.302001953125]
This is the real loss :  tensor(0.0374, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.055018  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
28     0    NaN  ...  0.202612  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.237297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
30     0    NaN  ...  0.053865  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
31     0    NaN  ...  0.036367  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
32     0    NaN  ...  0.052054  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
33     0    NaN  ...  0.036895  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
34     0    NaN  ...  0.208221  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
35     0    NaN  ...  0.346967  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
36     0    NaN  ...  0.030135  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
37     0    NaN  ...  0.043758  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
38     0    NaN  ...  0.028062  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.375342  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
40     0    NaN  ...  0.048836  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
41     0    NaN  ...  0.025767  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
42     0    NaN  ...  0.380506  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
43     0    NaN  ...  0.217437  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
44     0    NaN  ...  0.185710  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
45     0    NaN  ...  0.233914  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
46     0    NaN  ...  0.215185  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
47     0    NaN  ...  0.027417  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
48     0    NaN  ...  0.032203  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
49     0    NaN  ...  0.035146  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
50     0    NaN  ...  0.203689  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
51     0    NaN  ...  0.183580  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
52     0    NaN  ...  0.246644  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
53     0    NaN  ...  0.034029  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
54     0    NaN  ...  0.044656  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
55     0    NaN  ...  0.215340  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
56     0    NaN  ...  0.037359  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[57 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.7919921875, 0.28564453125]
[1.1455078125, 0.0271148681640625]
[0.78271484375, 0.260009765625]
[0.84912109375, 0.193603515625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7919921875, 0.28564453125]
[1.1455078125, 0.0271148681640625]
[0.78271484375, 0.260009765625]
[0.84912109375, 0.193603515625]
This is the real loss :  tensor(0.1668, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.055018  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
28     0    NaN  ...  0.202612  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.237297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
30     0    NaN  ...  0.053865  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
31     0    NaN  ...  0.036367  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
32     0    NaN  ...  0.052054  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
33     0    NaN  ...  0.036895  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
34     0    NaN  ...  0.208221  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
35     0    NaN  ...  0.346967  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
36     0    NaN  ...  0.030135  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
37     0    NaN  ...  0.043758  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
38     0    NaN  ...  0.028062  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.375342  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
40     0    NaN  ...  0.048836  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
41     0    NaN  ...  0.025767  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
42     0    NaN  ...  0.380506  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
43     0    NaN  ...  0.217437  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
44     0    NaN  ...  0.185710  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
45     0    NaN  ...  0.233914  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
46     0    NaN  ...  0.215185  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
47     0    NaN  ...  0.027417  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
48     0    NaN  ...  0.032203  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
49     0    NaN  ...  0.035146  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
50     0    NaN  ...  0.203689  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
51     0    NaN  ...  0.183580  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
52     0    NaN  ...  0.246644  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
53     0    NaN  ...  0.034029  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
54     0    NaN  ...  0.044656  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
55     0    NaN  ...  0.215340  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
56     0    NaN  ...  0.037359  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
57     0    NaN  ...  0.166816  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[58 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.84326171875, 0.1893310546875]
[0.71240234375, 0.150146484375]
[1.033203125, 0.116943359375]
[0.7890625, 0.238037109375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.84326171875, 0.1893310546875]
[0.71240234375, 0.150146484375]
[1.033203125, 0.116943359375]
[0.7890625, 0.238037109375]
This is the real loss :  tensor(0.2643, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.055018  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
28     0    NaN  ...  0.202612  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.237297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
30     0    NaN  ...  0.053865  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
31     0    NaN  ...  0.036367  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
32     0    NaN  ...  0.052054  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
33     0    NaN  ...  0.036895  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
34     0    NaN  ...  0.208221  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
35     0    NaN  ...  0.346967  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
36     0    NaN  ...  0.030135  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
37     0    NaN  ...  0.043758  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
38     0    NaN  ...  0.028062  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.375342  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
40     0    NaN  ...  0.048836  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
41     0    NaN  ...  0.025767  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
42     0    NaN  ...  0.380506  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
43     0    NaN  ...  0.217437  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
44     0    NaN  ...  0.185710  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
45     0    NaN  ...  0.233914  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
46     0    NaN  ...  0.215185  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
47     0    NaN  ...  0.027417  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
48     0    NaN  ...  0.032203  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
49     0    NaN  ...  0.035146  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
50     0    NaN  ...  0.203689  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
51     0    NaN  ...  0.183580  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
52     0    NaN  ...  0.246644  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
53     0    NaN  ...  0.034029  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
54     0    NaN  ...  0.044656  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
55     0    NaN  ...  0.215340  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
56     0    NaN  ...  0.037359  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
57     0    NaN  ...  0.166816  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
58     0    NaN  ...  0.264265  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[59 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.84375, 0.290283203125]
[0.60888671875, 0.1583251953125]
[0.7041015625, 0.1092529296875]
[0.6474609375, 0.076171875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.84375, 0.290283203125]
[0.60888671875, 0.1583251953125]
[0.7041015625, 0.1092529296875]
[0.6474609375, 0.076171875]
This is the real loss :  tensor(0.0645, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
5      0    NaN  ...  0.602078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
6      0    NaN  ...  0.423532  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
7      0    NaN  ...  0.282531  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
8      0    NaN  ...  0.175786  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
9      0    NaN  ...  0.373020  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
10     0    NaN  ...  0.185867  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
11     0    NaN  ...  0.179918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
12     0    NaN  ...  0.239743  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
13     0    NaN  ...  0.247548  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
14     0    NaN  ...  0.264051  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
15     0    NaN  ...  0.136581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
16     0    NaN  ...  0.223664  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
17     0    NaN  ...  0.100999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
18     0    NaN  ...  0.465569  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
19     0    NaN  ...  0.103598  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
20     0    NaN  ...  0.134595  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
21     0    NaN  ...  0.228068  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
22     0    NaN  ...  0.348926  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
23     0    NaN  ...  0.066839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
24     0    NaN  ...  0.228684  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
25     0    NaN  ...  0.081700  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
26     0    NaN  ...  0.047142  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
27     0    NaN  ...  0.055018  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
28     0    NaN  ...  0.202612  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
29     0    NaN  ...  0.237297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
30     0    NaN  ...  0.053865  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
31     0    NaN  ...  0.036367  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
32     0    NaN  ...  0.052054  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
33     0    NaN  ...  0.036895  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
34     0    NaN  ...  0.208221  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
35     0    NaN  ...  0.346967  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
36     0    NaN  ...  0.030135  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
37     0    NaN  ...  0.043758  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
38     0    NaN  ...  0.028062  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
39     0    NaN  ...  0.375342  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
40     0    NaN  ...  0.048836  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
41     0    NaN  ...  0.025767  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
42     0    NaN  ...  0.380506  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
43     0    NaN  ...  0.217437  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
44     0    NaN  ...  0.185710  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
45     0    NaN  ...  0.233914  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
46     0    NaN  ...  0.215185  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
47     0    NaN  ...  0.027417  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
48     0    NaN  ...  0.032203  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
49     0    NaN  ...  0.035146  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
50     0    NaN  ...  0.203689  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
51     0    NaN  ...  0.183580  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
52     0    NaN  ...  0.246644  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
53     0    NaN  ...  0.034029  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
54     0    NaN  ...  0.044656  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
55     0    NaN  ...  0.215340  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
56     0    NaN  ...  0.037359  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
57     0    NaN  ...  0.166816  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
58     0    NaN  ...  0.264265  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
59     0    NaN  ...  0.064537  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[60 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.865234375, 0.2354736328125]
[0.72412109375, 0.21142578125]
[0.94091796875, 0.1029052734375]
[0.7890625, 0.10369873046875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.865234375, 0.2354736328125]
[0.72412109375, 0.21142578125]
[0.94091796875, 0.1029052734375]
[0.7890625, 0.10369873046875]
This is the real loss :  tensor(0.0330, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
56     0    NaN  ...  0.037359  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
57     0    NaN  ...  0.166816  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
58     0    NaN  ...  0.264265  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
59     0    NaN  ...  0.064537  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
60     0    NaN  ...  0.032968  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[61 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.919921875, 0.2457275390625]
[0.7724609375, 0.132568359375]
[0.92041015625, 0.060211181640625]
[0.81396484375, 0.24658203125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.919921875, 0.2457275390625]
[0.7724609375, 0.132568359375]
[0.92041015625, 0.060211181640625]
[0.81396484375, 0.24658203125]
This is the real loss :  tensor(0.0302, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
57     0    NaN  ...  0.166816  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
58     0    NaN  ...  0.264265  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
59     0    NaN  ...  0.064537  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
60     0    NaN  ...  0.032968  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
61     0    NaN  ...  0.030189  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[62 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.84619140625, 0.226318359375]
[0.87548828125, 0.1976318359375]
[0.98828125, 0.049560546875]
[0.9794921875, 0.131591796875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.84619140625, 0.226318359375]
[0.87548828125, 0.1976318359375]
[0.98828125, 0.049560546875]
[0.9794921875, 0.131591796875]
This is the real loss :  tensor(0.0187, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
58     0    NaN  ...  0.264265  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
59     0    NaN  ...  0.064537  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
60     0    NaN  ...  0.032968  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
61     0    NaN  ...  0.030189  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
62     0    NaN  ...  0.018721  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[63 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.75732421875, 0.2139892578125]
[0.75927734375, 0.2230224609375]
[0.8662109375, 0.1470947265625]
[0.88623046875, 0.1807861328125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.75732421875, 0.2139892578125]
[0.75927734375, 0.2230224609375]
[0.8662109375, 0.1470947265625]
[0.88623046875, 0.1807861328125]
This is the real loss :  tensor(0.0372, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
59     0    NaN  ...  0.064537  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
60     0    NaN  ...  0.032968  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
61     0    NaN  ...  0.030189  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
62     0    NaN  ...  0.018721  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
63     0    NaN  ...  0.037192  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[64 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.74853515625, 0.0775146484375]
[0.75634765625, 0.2227783203125]
[0.998046875, 0.1343994140625]
[0.8154296875, 0.176025390625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.74853515625, 0.0775146484375]
[0.75634765625, 0.2227783203125]
[0.998046875, 0.1343994140625]
[0.8154296875, 0.176025390625]
This is the real loss :  tensor(0.0327, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
60     0    NaN  ...  0.032968  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
61     0    NaN  ...  0.030189  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
62     0    NaN  ...  0.018721  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
63     0    NaN  ...  0.037192  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
64     0    NaN  ...  0.032670  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[65 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.9501953125, 0.23681640625]
[0.8857421875, 0.153564453125]
[1.0068359375, 0.0595703125]
[1.001953125, 0.2352294921875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.9501953125, 0.23681640625]
[0.8857421875, 0.153564453125]
[1.0068359375, 0.0595703125]
[1.001953125, 0.2352294921875]
This is the real loss :  tensor(0.1976, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
61     0    NaN  ...  0.030189  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
62     0    NaN  ...  0.018721  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
63     0    NaN  ...  0.037192  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
64     0    NaN  ...  0.032670  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
65     0    NaN  ...  0.197611  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[66 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.87060546875, 0.11492919921875]
[0.8349609375, 0.06683349609375]
[0.96533203125, 0.217041015625]
[0.865234375, 0.06903076171875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.87060546875, 0.11492919921875]
[0.8349609375, 0.06683349609375]
[0.96533203125, 0.217041015625]
[0.865234375, 0.06903076171875]
This is the real loss :  tensor(0.3926, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
62     0    NaN  ...  0.018721  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
63     0    NaN  ...  0.037192  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
64     0    NaN  ...  0.032670  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
65     0    NaN  ...  0.197611  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
66     0    NaN  ...  0.392603  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[67 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.71533203125, 0.10675048828125]
[0.95166015625, 0.10614013671875]
[0.763671875, 0.2127685546875]
[0.8056640625, 0.16845703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.71533203125, 0.10675048828125]
[0.95166015625, 0.10614013671875]
[0.763671875, 0.2127685546875]
[0.8056640625, 0.16845703125]
This is the real loss :  tensor(0.2455, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
63     0    NaN  ...  0.037192  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
64     0    NaN  ...  0.032670  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
65     0    NaN  ...  0.197611  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
66     0    NaN  ...  0.392603  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
67     0    NaN  ...  0.245542  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[68 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.9599609375, 0.0682373046875]
[0.93603515625, 0.172119140625]
[0.77978515625, 0.1160888671875]
[0.77001953125, 0.1849365234375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.9599609375, 0.0682373046875]
[0.93603515625, 0.172119140625]
[0.77978515625, 0.1160888671875]
[0.77001953125, 0.1849365234375]
This is the real loss :  tensor(0.0236, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
64     0    NaN  ...  0.032670  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
65     0    NaN  ...  0.197611  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
66     0    NaN  ...  0.392603  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
67     0    NaN  ...  0.245542  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
68     0    NaN  ...  0.023630  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[69 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.775390625, 0.151611328125]
[0.8193359375, 0.211181640625]
[0.75, 0.2415771484375]
[1.0703125, 0.0275421142578125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.775390625, 0.151611328125]
[0.8193359375, 0.211181640625]
[0.75, 0.2415771484375]
[1.0703125, 0.0275421142578125]
This is the real loss :  tensor(0.0347, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
65     0    NaN  ...  0.197611  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
66     0    NaN  ...  0.392603  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
67     0    NaN  ...  0.245542  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
68     0    NaN  ...  0.023630  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
69     0    NaN  ...  0.034654  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[70 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.95751953125, 0.108154296875]
[0.8251953125, 0.1915283203125]
[0.88720703125, 0.1781005859375]
[0.95556640625, 0.069091796875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.95751953125, 0.108154296875]
[0.8251953125, 0.1915283203125]
[0.88720703125, 0.1781005859375]
[0.95556640625, 0.069091796875]
This is the real loss :  tensor(0.5645, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
66     0    NaN  ...  0.392603  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
67     0    NaN  ...  0.245542  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
68     0    NaN  ...  0.023630  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
69     0    NaN  ...  0.034654  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
70     0    NaN  ...  0.564526  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[71 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.78076171875, 0.10272216796875]
[0.90185546875, 0.01055145263671875]
[0.75, 0.253173828125]
[0.71728515625, 0.12261962890625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.78076171875, 0.10272216796875]
[0.90185546875, 0.01055145263671875]
[0.75, 0.253173828125]
[0.71728515625, 0.12261962890625]
This is the real loss :  tensor(0.3091, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
67     0    NaN  ...  0.245542  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
68     0    NaN  ...  0.023630  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
69     0    NaN  ...  0.034654  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
70     0    NaN  ...  0.564526  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
71     0    NaN  ...  0.309113  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[72 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.79443359375, 0.1395263671875]
[1.0703125, 0.0911865234375]
[0.85888671875, 0.1895751953125]
[0.72021484375, 0.29345703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.79443359375, 0.1395263671875]
[1.0703125, 0.0911865234375]
[0.85888671875, 0.1895751953125]
[0.72021484375, 0.29345703125]
This is the real loss :  tensor(0.0369, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
68     0    NaN  ...  0.023630  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
69     0    NaN  ...  0.034654  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
70     0    NaN  ...  0.564526  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
71     0    NaN  ...  0.309113  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
72     0    NaN  ...  0.036904  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[73 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.92041015625, 0.010833740234375]
[0.8193359375, 0.315673828125]
[0.87548828125, 0.0609130859375]
[0.78662109375, 0.1954345703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.92041015625, 0.010833740234375]
[0.8193359375, 0.315673828125]
[0.87548828125, 0.0609130859375]
[0.78662109375, 0.1954345703125]
This is the real loss :  tensor(0.0302, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
69     0    NaN  ...  0.034654  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
70     0    NaN  ...  0.564526  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
71     0    NaN  ...  0.309113  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
72     0    NaN  ...  0.036904  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
73     0    NaN  ...  0.030210  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[74 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.826171875, 0.1287841796875]
[0.64111328125, 0.2403564453125]
[0.73779296875, 0.1689453125]
[0.9560546875, 0.10498046875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.826171875, 0.1287841796875]
[0.64111328125, 0.2403564453125]
[0.73779296875, 0.1689453125]
[0.9560546875, 0.10498046875]
This is the real loss :  tensor(0.2854, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
70     0    NaN  ...  0.564526  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
71     0    NaN  ...  0.309113  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
72     0    NaN  ...  0.036904  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
73     0    NaN  ...  0.030210  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
74     0    NaN  ...  0.285354  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[75 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.75146484375, 0.30712890625]
[0.84326171875, 0.1474609375]
[1.0361328125, 0.0247344970703125]
[0.767578125, 0.1888427734375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.75146484375, 0.30712890625]
[0.84326171875, 0.1474609375]
[1.0361328125, 0.0247344970703125]
[0.767578125, 0.1888427734375]
This is the real loss :  tensor(0.3218, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
71     0    NaN  ...  0.309113  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
72     0    NaN  ...  0.036904  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
73     0    NaN  ...  0.030210  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
74     0    NaN  ...  0.285354  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
75     0    NaN  ...  0.321785  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[76 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.8583984375, 0.05328369140625]
[0.56689453125, 0.445556640625]
[0.8837890625, 0.1903076171875]
[0.96484375, 0.06451416015625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8583984375, 0.05328369140625]
[0.56689453125, 0.445556640625]
[0.8837890625, 0.1903076171875]
[0.96484375, 0.06451416015625]
This is the real loss :  tensor(0.0883, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
72     0    NaN  ...  0.036904  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
73     0    NaN  ...  0.030210  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
74     0    NaN  ...  0.285354  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
75     0    NaN  ...  0.321785  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
76     0    NaN  ...  0.088348  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[77 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.861328125, 0.1640625]
[0.52978515625, 0.3642578125]
[0.986328125, 0.0214996337890625]
[0.7353515625, 0.291259765625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.861328125, 0.1640625]
[0.52978515625, 0.3642578125]
[0.986328125, 0.0214996337890625]
[0.7353515625, 0.291259765625]
This is the real loss :  tensor(0.0694, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
73     0    NaN  ...  0.030210  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
74     0    NaN  ...  0.285354  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
75     0    NaN  ...  0.321785  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
76     0    NaN  ...  0.088348  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
77     0    NaN  ...  0.069432  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[78 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.60009765625, 0.267333984375]
[1.025390625, 0.1505126953125]
[0.666015625, 0.2047119140625]
[0.72705078125, 0.224365234375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.60009765625, 0.267333984375]
[1.025390625, 0.1505126953125]
[0.666015625, 0.2047119140625]
[0.72705078125, 0.224365234375]
This is the real loss :  tensor(0.1819, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
74     0    NaN  ...  0.285354  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
75     0    NaN  ...  0.321785  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
76     0    NaN  ...  0.088348  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
77     0    NaN  ...  0.069432  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
78     0    NaN  ...  0.181949  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[79 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.493896484375, 0.517578125]
[0.796875, 0.07098388671875]
[0.98046875, -0.003215789794921875]
[0.576171875, 0.1534423828125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.493896484375, 0.517578125]
[0.796875, 0.07098388671875]
[0.98046875, -0.003215789794921875]
[0.576171875, 0.1534423828125]
This is the real loss :  tensor(0.3427, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
75     0    NaN  ...  0.321785  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
76     0    NaN  ...  0.088348  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
77     0    NaN  ...  0.069432  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
78     0    NaN  ...  0.181949  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
79     0    NaN  ...  0.342658  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[80 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.69970703125, 0.25146484375]
[0.439697265625, 0.572265625]
[1.0, 0.01357269287109375]
[0.99951171875, -0.0303192138671875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.69970703125, 0.25146484375]
[0.439697265625, 0.572265625]
[1.0, 0.01357269287109375]
[0.99951171875, -0.0303192138671875]
This is the real loss :  tensor(0.0995, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
76     0    NaN  ...  0.088348  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
77     0    NaN  ...  0.069432  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
78     0    NaN  ...  0.181949  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
79     0    NaN  ...  0.342658  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
80     0    NaN  ...  0.099493  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[81 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.60693359375, 0.273681640625]
[0.50341796875, 0.50732421875]
[0.74169921875, 0.153076171875]
[1.072265625, -0.051239013671875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.60693359375, 0.273681640625]
[0.50341796875, 0.50732421875]
[0.74169921875, 0.153076171875]
[1.072265625, -0.051239013671875]
This is the real loss :  tensor(0.1029, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
77     0    NaN  ...  0.069432  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
78     0    NaN  ...  0.181949  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
79     0    NaN  ...  0.342658  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
80     0    NaN  ...  0.099493  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
81     0    NaN  ...  0.102945  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[82 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.53466796875, 0.473388671875]
[1.001953125, -0.04180908203125]
[0.6796875, 0.25048828125]
[0.71142578125, 0.2293701171875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.53466796875, 0.473388671875]
[1.001953125, -0.04180908203125]
[0.6796875, 0.25048828125]
[0.71142578125, 0.2293701171875]
This is the real loss :  tensor(0.2288, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
78     0    NaN  ...  0.181949  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
79     0    NaN  ...  0.342658  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
80     0    NaN  ...  0.099493  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
81     0    NaN  ...  0.102945  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
82     0    NaN  ...  0.228785  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[83 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.66796875, 0.0740966796875]
[0.69580078125, 0.0391845703125]
[0.65283203125, 0.059661865234375]
[0.58349609375, 0.499267578125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.66796875, 0.0740966796875]
[0.69580078125, 0.0391845703125]
[0.65283203125, 0.059661865234375]
[0.58349609375, 0.499267578125]
This is the real loss :  tensor(0.0946, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
79     0    NaN  ...  0.342658  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
80     0    NaN  ...  0.099493  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
81     0    NaN  ...  0.102945  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
82     0    NaN  ...  0.228785  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
83     0    NaN  ...  0.094580  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[84 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.6103515625, 0.468017578125]
[0.6748046875, 0.364013671875]
[0.8759765625, 0.029205322265625]
[0.89306640625, 0.025146484375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.6103515625, 0.468017578125]
[0.6748046875, 0.364013671875]
[0.8759765625, 0.029205322265625]
[0.89306640625, 0.025146484375]
This is the real loss :  tensor(0.0797, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
80     0    NaN  ...  0.099493  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
81     0    NaN  ...  0.102945  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
82     0    NaN  ...  0.228785  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
83     0    NaN  ...  0.094580  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
84     0    NaN  ...  0.079678  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[85 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.8505859375, 0.05462646484375]
[0.681640625, 0.385986328125]
[0.6875, 0.1448974609375]
[0.55810546875, 0.280029296875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8505859375, 0.05462646484375]
[0.681640625, 0.385986328125]
[0.6875, 0.1448974609375]
[0.55810546875, 0.280029296875]
This is the real loss :  tensor(0.1574, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
81     0    NaN  ...  0.102945  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
82     0    NaN  ...  0.228785  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
83     0    NaN  ...  0.094580  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
84     0    NaN  ...  0.079678  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
85     0    NaN  ...  0.157412  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[86 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.66455078125, 0.2061767578125]
[0.60400390625, 0.33056640625]
[0.8115234375, 0.053192138671875]
[0.640625, 0.21484375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.66455078125, 0.2061767578125]
[0.60400390625, 0.33056640625]
[0.8115234375, 0.053192138671875]
[0.640625, 0.21484375]
This is the real loss :  tensor(0.1939, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
82     0    NaN  ...  0.228785  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
83     0    NaN  ...  0.094580  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
84     0    NaN  ...  0.079678  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
85     0    NaN  ...  0.157412  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
86     0    NaN  ...  0.193941  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[87 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.65283203125, -0.0213165283203125]
[0.53173828125, 0.27978515625]
[0.52392578125, 0.08062744140625]
[0.43310546875, 0.170654296875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.65283203125, -0.0213165283203125]
[0.53173828125, 0.27978515625]
[0.52392578125, 0.08062744140625]
[0.43310546875, 0.170654296875]
This is the real loss :  tensor(0.3568, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
83     0    NaN  ...  0.094580  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
84     0    NaN  ...  0.079678  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
85     0    NaN  ...  0.157412  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
86     0    NaN  ...  0.193941  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
87     0    NaN  ...  0.356797  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[88 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.73828125, 0.05072021484375]
[0.6728515625, 0.093994140625]
[0.68408203125, 0.226806640625]
[0.75341796875, 0.390869140625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.73828125, 0.05072021484375]
[0.6728515625, 0.093994140625]
[0.68408203125, 0.226806640625]
[0.75341796875, 0.390869140625]
This is the real loss :  tensor(0.0690, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
84     0    NaN  ...  0.079678  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
85     0    NaN  ...  0.157412  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
86     0    NaN  ...  0.193941  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
87     0    NaN  ...  0.356797  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
88     0    NaN  ...  0.068970  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[89 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.83984375, 0.0223541259765625]
[0.62890625, 0.168212890625]
[0.5869140625, 0.29296875]
[0.49658203125, 0.303955078125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.83984375, 0.0223541259765625]
[0.62890625, 0.168212890625]
[0.5869140625, 0.29296875]
[0.49658203125, 0.303955078125]
This is the real loss :  tensor(0.0993, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
85     0    NaN  ...  0.157412  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
86     0    NaN  ...  0.193941  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
87     0    NaN  ...  0.356797  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
88     0    NaN  ...  0.068970  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
89     0    NaN  ...  0.099306  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[90 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.689453125, 0.1959228515625]
[0.8779296875, 0.08966064453125]
[0.67626953125, 0.258544921875]
[0.59521484375, 0.25341796875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.689453125, 0.1959228515625]
[0.8779296875, 0.08966064453125]
[0.67626953125, 0.258544921875]
[0.59521484375, 0.25341796875]
This is the real loss :  tensor(0.0697, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
86     0    NaN  ...  0.193941  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
87     0    NaN  ...  0.356797  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
88     0    NaN  ...  0.068970  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
89     0    NaN  ...  0.099306  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
90     0    NaN  ...  0.069685  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[91 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.82568359375, 0.0635986328125]
[0.7392578125, 0.1748046875]
[0.59228515625, 0.406982421875]
[0.833984375, 0.1309814453125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.82568359375, 0.0635986328125]
[0.7392578125, 0.1748046875]
[0.59228515625, 0.406982421875]
[0.833984375, 0.1309814453125]
This is the real loss :  tensor(0.0637, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
87     0    NaN  ...  0.356797  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
88     0    NaN  ...  0.068970  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
89     0    NaN  ...  0.099306  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
90     0    NaN  ...  0.069685  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
91     0    NaN  ...  0.063695  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[92 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.69580078125, 0.1962890625]
[0.61767578125, 0.274658203125]
[0.79931640625, 0.23828125]
[0.83056640625, 0.116943359375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.69580078125, 0.1962890625]
[0.61767578125, 0.274658203125]
[0.79931640625, 0.23828125]
[0.83056640625, 0.116943359375]
This is the real loss :  tensor(0.1473, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
88     0    NaN  ...  0.068970  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
89     0    NaN  ...  0.099306  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
90     0    NaN  ...  0.069685  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
91     0    NaN  ...  0.063695  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
92     0    NaN  ...  0.147268  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[93 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.80615234375, 0.1611328125]
[0.69970703125, 0.1575927734375]
[0.953125, 0.260986328125]
[0.75146484375, 0.1961669921875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.80615234375, 0.1611328125]
[0.69970703125, 0.1575927734375]
[0.953125, 0.260986328125]
[0.75146484375, 0.1961669921875]
This is the real loss :  tensor(0.0436, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
89     0    NaN  ...  0.099306  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
90     0    NaN  ...  0.069685  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
91     0    NaN  ...  0.063695  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
92     0    NaN  ...  0.147268  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
93     0    NaN  ...  0.043639  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[94 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.7587890625, 0.298828125]
[0.841796875, 0.1636962890625]
[0.7734375, 0.273193359375]
[0.8955078125, 0.177001953125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7587890625, 0.298828125]
[0.841796875, 0.1636962890625]
[0.7734375, 0.273193359375]
[0.8955078125, 0.177001953125]
This is the real loss :  tensor(0.2155, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
90     0    NaN  ...  0.069685  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
91     0    NaN  ...  0.063695  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
92     0    NaN  ...  0.147268  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
93     0    NaN  ...  0.043639  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
94     0    NaN  ...  0.215465  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[95 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.6796875, 0.1904296875]
[0.82861328125, 0.1448974609375]
[0.83740234375, 0.19873046875]
[0.8388671875, 0.1171875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.6796875, 0.1904296875]
[0.82861328125, 0.1448974609375]
[0.83740234375, 0.19873046875]
[0.8388671875, 0.1171875]
This is the real loss :  tensor(0.0369, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
91     0    NaN  ...  0.063695  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
92     0    NaN  ...  0.147268  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
93     0    NaN  ...  0.043639  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
94     0    NaN  ...  0.215465  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
95     0    NaN  ...  0.036858  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[96 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.88720703125, 0.1278076171875]
[0.83349609375, 0.3544921875]
[0.90380859375, 0.03729248046875]
[0.65771484375, 0.2178955078125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.88720703125, 0.1278076171875]
[0.83349609375, 0.3544921875]
[0.90380859375, 0.03729248046875]
[0.65771484375, 0.2178955078125]
This is the real loss :  tensor(0.0447, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
92     0    NaN  ...  0.147268  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
93     0    NaN  ...  0.043639  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
94     0    NaN  ...  0.215465  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
95     0    NaN  ...  0.036858  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
96     0    NaN  ...  0.044716  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[97 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.978515625, 0.03607177734375]
[0.794921875, 0.1634521484375]
[0.828125, 0.10516357421875]
[0.7861328125, 0.1666259765625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.978515625, 0.03607177734375]
[0.794921875, 0.1634521484375]
[0.828125, 0.10516357421875]
[0.7861328125, 0.1666259765625]
This is the real loss :  tensor(0.1809, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
93     0    NaN  ...  0.043639  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
94     0    NaN  ...  0.215465  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
95     0    NaN  ...  0.036858  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
96     0    NaN  ...  0.044716  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
97     0    NaN  ...  0.180947  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[98 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.7744140625, 0.1375732421875]
[0.80078125, 0.22265625]
[1.208984375, 0.05615234375]
[0.78564453125, 0.1279296875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7744140625, 0.1375732421875]
[0.80078125, 0.22265625]
[1.208984375, 0.05615234375]
[0.78564453125, 0.1279296875]
This is the real loss :  tensor(0.0335, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
94     0    NaN  ...  0.215465  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
95     0    NaN  ...  0.036858  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
96     0    NaN  ...  0.044716  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
97     0    NaN  ...  0.180947  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
98     0    NaN  ...  0.033528  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[99 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.78173828125, 0.181884765625]
[0.66015625, 0.2060546875]
[1.1201171875, -0.0430908203125]
[0.7353515625, 0.2232666015625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.78173828125, 0.181884765625]
[0.66015625, 0.2060546875]
[1.1201171875, -0.0430908203125]
[0.7353515625, 0.2232666015625]
This is the real loss :  tensor(0.4657, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:    Epoch  Batch  ...      Loss                                        True Value
0      0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1      0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2      0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3      0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4      0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..   ...    ...  ...       ...                                               ...
95     0    NaN  ...  0.036858  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
96     0    NaN  ...  0.044716  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
97     0    NaN  ...  0.180947  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
98     0    NaN  ...  0.033528  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
99     0    NaN  ...  0.465679  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[100 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.72998046875, 0.2340087890625]
[0.7802734375, 0.263671875]
[1.1748046875, -0.087646484375]
[0.8505859375, 0.231201171875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.72998046875, 0.2340087890625]
[0.7802734375, 0.263671875]
[1.1748046875, -0.087646484375]
[0.8505859375, 0.231201171875]
This is the real loss :  tensor(0.1741, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
96      0    NaN  ...  0.044716  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
97      0    NaN  ...  0.180947  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
98      0    NaN  ...  0.033528  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
99      0    NaN  ...  0.465679  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
100     0    NaN  ...  0.174087  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[101 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.71484375, 0.199951171875]
[0.7265625, 0.1781005859375]
[1.052734375, -0.050323486328125]
[0.736328125, 0.16845703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.71484375, 0.199951171875]
[0.7265625, 0.1781005859375]
[1.052734375, -0.050323486328125]
[0.736328125, 0.16845703125]
This is the real loss :  tensor(0.0414, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
97      0    NaN  ...  0.180947  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
98      0    NaN  ...  0.033528  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
99      0    NaN  ...  0.465679  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
100     0    NaN  ...  0.174087  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
101     0    NaN  ...  0.041375  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[102 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.8828125, 0.1500244140625]
[0.8603515625, 0.1353759765625]
[0.912109375, 0.058197021484375]
[0.857421875, 0.26806640625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8828125, 0.1500244140625]
[0.8603515625, 0.1353759765625]
[0.912109375, 0.058197021484375]
[0.857421875, 0.26806640625]
This is the real loss :  tensor(0.2356, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
98      0    NaN  ...  0.033528  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
99      0    NaN  ...  0.465679  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
100     0    NaN  ...  0.174087  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
101     0    NaN  ...  0.041375  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
102     0    NaN  ...  0.235649  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[103 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.98828125, 0.019561767578125]
[0.89111328125, 0.1878662109375]
[0.81396484375, 0.221923828125]
[0.72021484375, 0.2135009765625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.98828125, 0.019561767578125]
[0.89111328125, 0.1878662109375]
[0.81396484375, 0.221923828125]
[0.72021484375, 0.2135009765625]
This is the real loss :  tensor(0.0319, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
99      0    NaN  ...  0.465679  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
100     0    NaN  ...  0.174087  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
101     0    NaN  ...  0.041375  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
102     0    NaN  ...  0.235649  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
103     0    NaN  ...  0.031924  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[104 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.927734375, 0.06854248046875]
[0.79931640625, 0.2406005859375]
[0.82080078125, 0.119384765625]
[0.8115234375, 0.0936279296875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.927734375, 0.06854248046875]
[0.79931640625, 0.2406005859375]
[0.82080078125, 0.119384765625]
[0.8115234375, 0.0936279296875]
This is the real loss :  tensor(0.5193, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
100     0    NaN  ...  0.174087  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
101     0    NaN  ...  0.041375  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
102     0    NaN  ...  0.235649  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
103     0    NaN  ...  0.031924  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
104     0    NaN  ...  0.519349  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[105 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.673828125, 0.322509765625]
[1.1015625, 0.057159423828125]
[0.77099609375, 0.24560546875]
[0.73828125, 0.293212890625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.673828125, 0.322509765625]
[1.1015625, 0.057159423828125]
[0.77099609375, 0.24560546875]
[0.73828125, 0.293212890625]
This is the real loss :  tensor(0.0614, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
101     0    NaN  ...  0.041375  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
102     0    NaN  ...  0.235649  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
103     0    NaN  ...  0.031924  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
104     0    NaN  ...  0.519349  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
105     0    NaN  ...  0.061402  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[106 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.86572265625, 0.1766357421875]
[1.009765625, 0.08905029296875]
[0.783203125, 0.2265625]
[0.80712890625, 0.1378173828125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.86572265625, 0.1766357421875]
[1.009765625, 0.08905029296875]
[0.783203125, 0.2265625]
[0.80712890625, 0.1378173828125]
This is the real loss :  tensor(0.0265, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
102     0    NaN  ...  0.235649  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
103     0    NaN  ...  0.031924  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
104     0    NaN  ...  0.519349  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
105     0    NaN  ...  0.061402  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
106     0    NaN  ...  0.026473  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[107 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.8369140625, 0.28759765625]
[0.748046875, 0.1678466796875]
[1.1103515625, 0.060821533203125]
[0.7265625, 0.2158203125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8369140625, 0.28759765625]
[0.748046875, 0.1678466796875]
[1.1103515625, 0.060821533203125]
[0.7265625, 0.2158203125]
This is the real loss :  tensor(0.4323, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
103     0    NaN  ...  0.031924  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
104     0    NaN  ...  0.519349  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
105     0    NaN  ...  0.061402  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
106     0    NaN  ...  0.026473  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
107     0    NaN  ...  0.432341  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[108 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.68505859375, 0.27197265625]
[0.7099609375, 0.330810546875]
[1.03125, 0.09112548828125]
[0.77392578125, 0.024810791015625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.68505859375, 0.27197265625]
[0.7099609375, 0.330810546875]
[1.03125, 0.09112548828125]
[0.77392578125, 0.024810791015625]
This is the real loss :  tensor(0.0535, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
104     0    NaN  ...  0.519349  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
105     0    NaN  ...  0.061402  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
106     0    NaN  ...  0.026473  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
107     0    NaN  ...  0.432341  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
108     0    NaN  ...  0.053465  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[109 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.85205078125, 0.130126953125]
[0.72412109375, 0.295654296875]
[0.94091796875, 0.1942138671875]
[0.95654296875, 0.12335205078125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.85205078125, 0.130126953125]
[0.72412109375, 0.295654296875]
[0.94091796875, 0.1942138671875]
[0.95654296875, 0.12335205078125]
This is the real loss :  tensor(0.2193, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
105     0    NaN  ...  0.061402  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
106     0    NaN  ...  0.026473  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
107     0    NaN  ...  0.432341  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
108     0    NaN  ...  0.053465  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
109     0    NaN  ...  0.219258  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[110 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.8369140625, 0.160888671875]
[0.7763671875, 0.2269287109375]
[0.8408203125, 0.0848388671875]
[0.748046875, 0.13134765625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8369140625, 0.160888671875]
[0.7763671875, 0.2269287109375]
[0.8408203125, 0.0848388671875]
[0.748046875, 0.13134765625]
This is the real loss :  tensor(0.0334, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
106     0    NaN  ...  0.026473  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
107     0    NaN  ...  0.432341  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
108     0    NaN  ...  0.053465  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
109     0    NaN  ...  0.219258  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
110     0    NaN  ...  0.033407  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[111 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.97900390625, 0.191162109375]
[0.8037109375, 0.13037109375]
[0.775390625, 0.232177734375]
[0.75390625, 0.260009765625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.97900390625, 0.191162109375]
[0.8037109375, 0.13037109375]
[0.775390625, 0.232177734375]
[0.75390625, 0.260009765625]
This is the real loss :  tensor(0.0406, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
107     0    NaN  ...  0.432341  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
108     0    NaN  ...  0.053465  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
109     0    NaN  ...  0.219258  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
110     0    NaN  ...  0.033407  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
111     0    NaN  ...  0.040629  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[112 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[1.0498046875, 0.1435546875]
[0.779296875, 0.271728515625]
[0.8681640625, 0.1981201171875]
[0.802734375, 0.2034912109375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[1.0498046875, 0.1435546875]
[0.779296875, 0.271728515625]
[0.8681640625, 0.1981201171875]
[0.802734375, 0.2034912109375]
This is the real loss :  tensor(0.1851, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
108     0    NaN  ...  0.053465  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
109     0    NaN  ...  0.219258  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
110     0    NaN  ...  0.033407  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
111     0    NaN  ...  0.040629  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
112     0    NaN  ...  0.185134  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[113 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.7685546875, 0.1348876953125]
[0.89892578125, 0.26806640625]
[0.859375, 0.289306640625]
[0.8115234375, 0.10968017578125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7685546875, 0.1348876953125]
[0.89892578125, 0.26806640625]
[0.859375, 0.289306640625]
[0.8115234375, 0.10968017578125]
This is the real loss :  tensor(0.0381, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
109     0    NaN  ...  0.219258  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
110     0    NaN  ...  0.033407  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
111     0    NaN  ...  0.040629  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
112     0    NaN  ...  0.185134  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
113     0    NaN  ...  0.038108  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[114 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.8388671875, 0.1390380859375]
[0.76708984375, 0.287353515625]
[0.87255859375, 0.08050537109375]
[0.810546875, 0.1888427734375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8388671875, 0.1390380859375]
[0.76708984375, 0.287353515625]
[0.87255859375, 0.08050537109375]
[0.810546875, 0.1888427734375]
This is the real loss :  tensor(0.1545, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
110     0    NaN  ...  0.033407  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
111     0    NaN  ...  0.040629  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
112     0    NaN  ...  0.185134  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
113     0    NaN  ...  0.038108  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
114     0    NaN  ...  0.154483  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[115 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.91357421875, 0.1318359375]
[0.89208984375, 0.1807861328125]
[0.92431640625, 0.1796875]
[0.91064453125, 0.2322998046875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.91357421875, 0.1318359375]
[0.89208984375, 0.1807861328125]
[0.92431640625, 0.1796875]
[0.91064453125, 0.2322998046875]
This is the real loss :  tensor(0.3862, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
111     0    NaN  ...  0.040629  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
112     0    NaN  ...  0.185134  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
113     0    NaN  ...  0.038108  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
114     0    NaN  ...  0.154483  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
115     0    NaN  ...  0.386163  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[116 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.76806640625, 0.287353515625]
[0.8134765625, 0.1888427734375]
[0.92333984375, 0.1534423828125]
[1.0283203125, 0.1551513671875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.76806640625, 0.287353515625]
[0.8134765625, 0.1888427734375]
[0.92333984375, 0.1534423828125]
[1.0283203125, 0.1551513671875]
This is the real loss :  tensor(0.0326, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
112     0    NaN  ...  0.185134  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
113     0    NaN  ...  0.038108  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
114     0    NaN  ...  0.154483  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
115     0    NaN  ...  0.386163  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
116     0    NaN  ...  0.032639  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[117 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.78759765625, 0.1959228515625]
[0.94384765625, 0.2301025390625]
[0.72412109375, 0.212646484375]
[0.88916015625, 0.00800323486328125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.78759765625, 0.1959228515625]
[0.94384765625, 0.2301025390625]
[0.72412109375, 0.212646484375]
[0.88916015625, 0.00800323486328125]
This is the real loss :  tensor(0.1821, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
113     0    NaN  ...  0.038108  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
114     0    NaN  ...  0.154483  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
115     0    NaN  ...  0.386163  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
116     0    NaN  ...  0.032639  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
117     0    NaN  ...  0.182078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[118 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.74755859375, 0.2247314453125]
[0.81689453125, 0.25048828125]
[0.84423828125, 0.07147216796875]
[0.78857421875, 0.1148681640625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.74755859375, 0.2247314453125]
[0.81689453125, 0.25048828125]
[0.84423828125, 0.07147216796875]
[0.78857421875, 0.1148681640625]
This is the real loss :  tensor(0.0372, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
114     0    NaN  ...  0.154483  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
115     0    NaN  ...  0.386163  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
116     0    NaN  ...  0.032639  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
117     0    NaN  ...  0.182078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
118     0    NaN  ...  0.037221  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[119 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.92822265625, 0.12353515625]
[0.91650390625, 0.201416015625]
[0.8935546875, 0.158203125]
[0.80419921875, 0.313232421875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.92822265625, 0.12353515625]
[0.91650390625, 0.201416015625]
[0.8935546875, 0.158203125]
[0.80419921875, 0.313232421875]
This is the real loss :  tensor(0.0301, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
115     0    NaN  ...  0.386163  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
116     0    NaN  ...  0.032639  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
117     0    NaN  ...  0.182078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
118     0    NaN  ...  0.037221  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
119     0    NaN  ...  0.030096  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[120 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.7666015625, 0.1783447265625]
[0.94482421875, 0.1209716796875]
[0.8603515625, 0.07568359375]
[0.7255859375, 0.301025390625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7666015625, 0.1783447265625]
[0.94482421875, 0.1209716796875]
[0.8603515625, 0.07568359375]
[0.7255859375, 0.301025390625]
This is the real loss :  tensor(0.1840, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
116     0    NaN  ...  0.032639  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
117     0    NaN  ...  0.182078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
118     0    NaN  ...  0.037221  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
119     0    NaN  ...  0.030096  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
120     0    NaN  ...  0.183953  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[121 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.87451171875, 0.13232421875]
[0.8173828125, 0.17919921875]
[0.81396484375, 0.1815185546875]
[0.9140625, 0.2344970703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.87451171875, 0.13232421875]
[0.8173828125, 0.17919921875]
[0.81396484375, 0.1815185546875]
[0.9140625, 0.2344970703125]
This is the real loss :  tensor(0.0286, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
117     0    NaN  ...  0.182078  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
118     0    NaN  ...  0.037221  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
119     0    NaN  ...  0.030096  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
120     0    NaN  ...  0.183953  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
121     0    NaN  ...  0.028581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[122 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.6728515625, 0.11737060546875]
[0.7431640625, 0.352783203125]
[0.69140625, 0.02496337890625]
[0.7275390625, 0.093017578125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.6728515625, 0.11737060546875]
[0.7431640625, 0.352783203125]
[0.69140625, 0.02496337890625]
[0.7275390625, 0.093017578125]
This is the real loss :  tensor(0.2977, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
118     0    NaN  ...  0.037221  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
119     0    NaN  ...  0.030096  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
120     0    NaN  ...  0.183953  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
121     0    NaN  ...  0.028581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
122     0    NaN  ...  0.297711  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[123 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.89306640625, 0.1514892578125]
[0.93408203125, 0.1407470703125]
[0.90771484375, 0.1129150390625]
[0.79736328125, 0.314208984375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.89306640625, 0.1514892578125]
[0.93408203125, 0.1407470703125]
[0.90771484375, 0.1129150390625]
[0.79736328125, 0.314208984375]
This is the real loss :  tensor(0.0274, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
119     0    NaN  ...  0.030096  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
120     0    NaN  ...  0.183953  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
121     0    NaN  ...  0.028581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
122     0    NaN  ...  0.297711  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
123     0    NaN  ...  0.027449  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[124 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.7490234375, 0.1475830078125]
[0.7333984375, 0.1815185546875]
[0.84423828125, 0.118408203125]
[0.92724609375, 0.1741943359375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7490234375, 0.1475830078125]
[0.7333984375, 0.1815185546875]
[0.84423828125, 0.118408203125]
[0.92724609375, 0.1741943359375]
This is the real loss :  tensor(0.2211, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
120     0    NaN  ...  0.183953  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
121     0    NaN  ...  0.028581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
122     0    NaN  ...  0.297711  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
123     0    NaN  ...  0.027449  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
124     0    NaN  ...  0.221102  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[125 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.857421875, 0.146240234375]
[0.82763671875, 0.1553955078125]
[0.65673828125, 0.228515625]
[0.9326171875, 0.11700439453125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.857421875, 0.146240234375]
[0.82763671875, 0.1553955078125]
[0.65673828125, 0.228515625]
[0.9326171875, 0.11700439453125]
This is the real loss :  tensor(0.0355, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
121     0    NaN  ...  0.028581  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
122     0    NaN  ...  0.297711  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
123     0    NaN  ...  0.027449  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
124     0    NaN  ...  0.221102  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
125     0    NaN  ...  0.035481  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[126 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.94677734375, 0.134765625]
[0.8798828125, 0.215087890625]
[0.87060546875, 0.1129150390625]
[0.75390625, 0.2177734375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.94677734375, 0.134765625]
[0.8798828125, 0.215087890625]
[0.87060546875, 0.1129150390625]
[0.75390625, 0.2177734375]
This is the real loss :  tensor(0.1936, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
122     0    NaN  ...  0.297711  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
123     0    NaN  ...  0.027449  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
124     0    NaN  ...  0.221102  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
125     0    NaN  ...  0.035481  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
126     0    NaN  ...  0.193594  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[127 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.890625, 0.042755126953125]
[0.7060546875, 0.1419677734375]
[0.81396484375, 0.098388671875]
[0.81884765625, 0.2369384765625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.890625, 0.042755126953125]
[0.7060546875, 0.1419677734375]
[0.81396484375, 0.098388671875]
[0.81884765625, 0.2369384765625]
This is the real loss :  tensor(0.1727, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
123     0    NaN  ...  0.027449  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
124     0    NaN  ...  0.221102  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
125     0    NaN  ...  0.035481  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
126     0    NaN  ...  0.193594  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
127     0    NaN  ...  0.172721  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[128 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.84521484375, 0.213623046875]
[0.7509765625, 0.2418212890625]
[0.9482421875, 0.110107421875]
[0.9638671875, 0.144775390625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.84521484375, 0.213623046875]
[0.7509765625, 0.2418212890625]
[0.9482421875, 0.110107421875]
[0.9638671875, 0.144775390625]
This is the real loss :  tensor(0.3136, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
124     0    NaN  ...  0.221102  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
125     0    NaN  ...  0.035481  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
126     0    NaN  ...  0.193594  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
127     0    NaN  ...  0.172721  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
128     0    NaN  ...  0.313581  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[129 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.95556640625, 0.03070068359375]
[0.85009765625, 0.2244873046875]
[0.80810546875, 0.1546630859375]
[0.75634765625, 0.2266845703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.95556640625, 0.03070068359375]
[0.85009765625, 0.2244873046875]
[0.80810546875, 0.1546630859375]
[0.75634765625, 0.2266845703125]
This is the real loss :  tensor(0.1873, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
125     0    NaN  ...  0.035481  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
126     0    NaN  ...  0.193594  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
127     0    NaN  ...  0.172721  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
128     0    NaN  ...  0.313581  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
129     0    NaN  ...  0.187312  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[130 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.8232421875, 0.060394287109375]
[1.01953125, 0.058258056640625]
[0.7353515625, 0.1920166015625]
[0.66015625, 0.330810546875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8232421875, 0.060394287109375]
[1.01953125, 0.058258056640625]
[0.7353515625, 0.1920166015625]
[0.66015625, 0.330810546875]
This is the real loss :  tensor(0.4773, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
126     0    NaN  ...  0.193594  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
127     0    NaN  ...  0.172721  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
128     0    NaN  ...  0.313581  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
129     0    NaN  ...  0.187312  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
130     0    NaN  ...  0.477343  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[131 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.888671875, 0.0997314453125]
[0.8212890625, 0.2083740234375]
[0.57080078125, 0.290283203125]
[0.83837890625, 0.10736083984375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.888671875, 0.0997314453125]
[0.8212890625, 0.2083740234375]
[0.57080078125, 0.290283203125]
[0.83837890625, 0.10736083984375]
This is the real loss :  tensor(0.2477, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
127     0    NaN  ...  0.172721  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
128     0    NaN  ...  0.313581  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
129     0    NaN  ...  0.187312  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
130     0    NaN  ...  0.477343  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
131     0    NaN  ...  0.247713  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[132 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.88134765625, 0.123046875]
[0.9501953125, 0.03607177734375]
[0.73193359375, 0.28076171875]
[0.6435546875, 0.288818359375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.88134765625, 0.123046875]
[0.9501953125, 0.03607177734375]
[0.73193359375, 0.28076171875]
[0.6435546875, 0.288818359375]
This is the real loss :  tensor(0.0493, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
128     0    NaN  ...  0.313581  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
129     0    NaN  ...  0.187312  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
130     0    NaN  ...  0.477343  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
131     0    NaN  ...  0.247713  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
132     0    NaN  ...  0.049270  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[133 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.86181640625, 0.08551025390625]
[0.67236328125, 0.086181640625]
[0.6953125, 0.295654296875]
[0.8076171875, 0.186767578125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.86181640625, 0.08551025390625]
[0.67236328125, 0.086181640625]
[0.6953125, 0.295654296875]
[0.8076171875, 0.186767578125]
This is the real loss :  tensor(0.0492, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
129     0    NaN  ...  0.187312  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
130     0    NaN  ...  0.477343  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
131     0    NaN  ...  0.247713  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
132     0    NaN  ...  0.049270  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
133     0    NaN  ...  0.049165  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[134 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.8720703125, 0.08294677734375]
[0.69921875, 0.393310546875]
[0.66064453125, 0.201171875]
[0.861328125, 0.130126953125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8720703125, 0.08294677734375]
[0.69921875, 0.393310546875]
[0.66064453125, 0.201171875]
[0.861328125, 0.130126953125]
This is the real loss :  tensor(0.1340, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
130     0    NaN  ...  0.477343  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
131     0    NaN  ...  0.247713  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
132     0    NaN  ...  0.049270  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
133     0    NaN  ...  0.049165  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
134     0    NaN  ...  0.134003  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[135 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.8330078125, 0.130126953125]
[0.72705078125, 0.334716796875]
[0.77734375, 0.195556640625]
[0.70556640625, 0.15966796875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8330078125, 0.130126953125]
[0.72705078125, 0.334716796875]
[0.77734375, 0.195556640625]
[0.70556640625, 0.15966796875]
This is the real loss :  tensor(0.2296, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
131     0    NaN  ...  0.247713  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
132     0    NaN  ...  0.049270  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
133     0    NaN  ...  0.049165  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
134     0    NaN  ...  0.134003  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
135     0    NaN  ...  0.229640  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[136 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.6962890625, 0.29150390625]
[0.79931640625, 0.25341796875]
[0.82861328125, 0.136962890625]
[0.91455078125, 0.2005615234375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.6962890625, 0.29150390625]
[0.79931640625, 0.25341796875]
[0.82861328125, 0.136962890625]
[0.91455078125, 0.2005615234375]
This is the real loss :  tensor(0.1836, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
132     0    NaN  ...  0.049270  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
133     0    NaN  ...  0.049165  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
134     0    NaN  ...  0.134003  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
135     0    NaN  ...  0.229640  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
136     0    NaN  ...  0.183646  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[137 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.72802734375, 0.275146484375]
[0.8115234375, 0.183349609375]
[0.79052734375, 0.263671875]
[0.80712890625, 0.1654052734375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.72802734375, 0.275146484375]
[0.8115234375, 0.183349609375]
[0.79052734375, 0.263671875]
[0.80712890625, 0.1654052734375]
This is the real loss :  tensor(0.0496, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
133     0    NaN  ...  0.049165  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
134     0    NaN  ...  0.134003  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
135     0    NaN  ...  0.229640  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
136     0    NaN  ...  0.183646  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
137     0    NaN  ...  0.049597  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[138 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.65234375, 0.18212890625]
[0.6640625, 0.322998046875]
[0.6201171875, 0.1671142578125]
[0.81494140625, 0.0762939453125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.65234375, 0.18212890625]
[0.6640625, 0.322998046875]
[0.6201171875, 0.1671142578125]
[0.81494140625, 0.0762939453125]
This is the real loss :  tensor(0.1582, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
134     0    NaN  ...  0.134003  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
135     0    NaN  ...  0.229640  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
136     0    NaN  ...  0.183646  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
137     0    NaN  ...  0.049597  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
138     0    NaN  ...  0.158206  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[139 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.70849609375, 0.338623046875]
[0.72998046875, 0.21337890625]
[0.8857421875, 0.1324462890625]
[0.8056640625, 0.163818359375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.70849609375, 0.338623046875]
[0.72998046875, 0.21337890625]
[0.8857421875, 0.1324462890625]
[0.8056640625, 0.163818359375]
This is the real loss :  tensor(0.0517, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
135     0    NaN  ...  0.229640  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
136     0    NaN  ...  0.183646  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
137     0    NaN  ...  0.049597  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
138     0    NaN  ...  0.158206  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
139     0    NaN  ...  0.051660  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[140 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.66552734375, 0.345703125]
[0.85595703125, 0.144775390625]
[0.79443359375, 0.1983642578125]
[0.79443359375, 0.1795654296875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.66552734375, 0.345703125]
[0.85595703125, 0.144775390625]
[0.79443359375, 0.1983642578125]
[0.79443359375, 0.1795654296875]
This is the real loss :  tensor(0.0536, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
136     0    NaN  ...  0.183646  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
137     0    NaN  ...  0.049597  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
138     0    NaN  ...  0.158206  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
139     0    NaN  ...  0.051660  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
140     0    NaN  ...  0.053650  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[141 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.91455078125, 0.07867431640625]
[0.90869140625, 0.2281494140625]
[0.80419921875, 0.32568359375]
[0.7392578125, 0.3115234375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.91455078125, 0.07867431640625]
[0.90869140625, 0.2281494140625]
[0.80419921875, 0.32568359375]
[0.7392578125, 0.3115234375]
This is the real loss :  tensor(0.3250, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
137     0    NaN  ...  0.049597  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
138     0    NaN  ...  0.158206  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
139     0    NaN  ...  0.051660  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
140     0    NaN  ...  0.053650  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
141     0    NaN  ...  0.324984  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[142 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.8720703125, 0.1695556640625]
[0.74853515625, 0.286865234375]
[0.7666015625, 0.233154296875]
[0.84716796875, 0.193603515625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8720703125, 0.1695556640625]
[0.74853515625, 0.286865234375]
[0.7666015625, 0.233154296875]
[0.84716796875, 0.193603515625]
This is the real loss :  tensor(0.0450, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
138     0    NaN  ...  0.158206  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
139     0    NaN  ...  0.051660  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
140     0    NaN  ...  0.053650  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
141     0    NaN  ...  0.324984  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
142     0    NaN  ...  0.045040  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[143 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.95361328125, 0.046295166015625]
[0.8037109375, 0.2381591796875]
[0.751953125, 0.2685546875]
[0.775390625, 0.20849609375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.95361328125, 0.046295166015625]
[0.8037109375, 0.2381591796875]
[0.751953125, 0.2685546875]
[0.775390625, 0.20849609375]
This is the real loss :  tensor(0.1823, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
139     0    NaN  ...  0.051660  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
140     0    NaN  ...  0.053650  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
141     0    NaN  ...  0.324984  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
142     0    NaN  ...  0.045040  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
143     0    NaN  ...  0.182277  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[144 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.86865234375, 0.0816650390625]
[0.8623046875, 0.19189453125]
[0.75390625, 0.363037109375]
[0.79541015625, 0.150146484375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.86865234375, 0.0816650390625]
[0.8623046875, 0.19189453125]
[0.75390625, 0.363037109375]
[0.79541015625, 0.150146484375]
This is the real loss :  tensor(0.0421, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
140     0    NaN  ...  0.053650  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
141     0    NaN  ...  0.324984  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
142     0    NaN  ...  0.045040  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
143     0    NaN  ...  0.182277  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
144     0    NaN  ...  0.042058  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[145 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [0., 1.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.8203125, 0.38623046875]
[0.8486328125, 0.1285400390625]
[0.8330078125, 0.03155517578125]
[0.65576171875, 0.187255859375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8203125, 0.38623046875]
[0.8486328125, 0.1285400390625]
[0.8330078125, 0.03155517578125]
[0.65576171875, 0.187255859375]
This is the real loss :  tensor(0.6565, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
141     0    NaN  ...  0.324984  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
142     0    NaN  ...  0.045040  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
143     0    NaN  ...  0.182277  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
144     0    NaN  ...  0.042058  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
145     0    NaN  ...  0.656451  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[146 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.70849609375, 0.1468505859375]
[0.69140625, 0.0677490234375]
[0.73828125, 0.1551513671875]
[0.6171875, 0.27294921875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.70849609375, 0.1468505859375]
[0.69140625, 0.0677490234375]
[0.73828125, 0.1551513671875]
[0.6171875, 0.27294921875]
This is the real loss :  tensor(0.0650, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
142     0    NaN  ...  0.045040  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
143     0    NaN  ...  0.182277  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
144     0    NaN  ...  0.042058  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
145     0    NaN  ...  0.656451  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
146     0    NaN  ...  0.064997  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[147 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.6796875, 0.228271484375]
[0.90185546875, -0.0137786865234375]
[0.72705078125, 0.3330078125]
[0.69775390625, 0.278564453125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.6796875, 0.228271484375]
[0.90185546875, -0.0137786865234375]
[0.72705078125, 0.3330078125]
[0.69775390625, 0.278564453125]
This is the real loss :  tensor(0.1634, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
143     0    NaN  ...  0.182277  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
144     0    NaN  ...  0.042058  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
145     0    NaN  ...  0.656451  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
146     0    NaN  ...  0.064997  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
147     0    NaN  ...  0.163370  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[148 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.7431640625, 0.16015625]
[0.87646484375, 0.03631591796875]
[0.712890625, 0.187744140625]
[0.68212890625, 0.3681640625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7431640625, 0.16015625]
[0.87646484375, 0.03631591796875]
[0.712890625, 0.187744140625]
[0.68212890625, 0.3681640625]
This is the real loss :  tensor(0.1363, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
144     0    NaN  ...  0.042058  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
145     0    NaN  ...  0.656451  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
146     0    NaN  ...  0.064997  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
147     0    NaN  ...  0.163370  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
148     0    NaN  ...  0.136299  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[149 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.66650390625, 0.3271484375]
[0.75927734375, 0.250732421875]
[0.94091796875, 0.1480712890625]
[0.7529296875, 0.186767578125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.66650390625, 0.3271484375]
[0.75927734375, 0.250732421875]
[0.94091796875, 0.1480712890625]
[0.7529296875, 0.186767578125]
This is the real loss :  tensor(0.1991, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
145     0    NaN  ...  0.656451  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
146     0    NaN  ...  0.064997  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
147     0    NaN  ...  0.163370  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
148     0    NaN  ...  0.136299  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
149     0    NaN  ...  0.199091  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[150 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.78125, 0.1650390625]
[0.7197265625, 0.298095703125]
[0.71533203125, 0.1439208984375]
[0.751953125, 0.234375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.78125, 0.1650390625]
[0.7197265625, 0.298095703125]
[0.71533203125, 0.1439208984375]
[0.751953125, 0.234375]
This is the real loss :  tensor(0.0576, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
146     0    NaN  ...  0.064997  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
147     0    NaN  ...  0.163370  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
148     0    NaN  ...  0.136299  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
149     0    NaN  ...  0.199091  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
150     0    NaN  ...  0.057589  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[151 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.7294921875, 0.233154296875]
[0.80322265625, 0.26953125]
[0.88916015625, 0.1265869140625]
[0.736328125, 0.291259765625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7294921875, 0.233154296875]
[0.80322265625, 0.26953125]
[0.88916015625, 0.1265869140625]
[0.736328125, 0.291259765625]
This is the real loss :  tensor(0.3674, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
147     0    NaN  ...  0.163370  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
148     0    NaN  ...  0.136299  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
149     0    NaN  ...  0.199091  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
150     0    NaN  ...  0.057589  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
151     0    NaN  ...  0.367424  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[152 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.7138671875, 0.278564453125]
[0.6640625, 0.385009765625]
[0.888671875, 0.061004638671875]
[0.8701171875, 0.10791015625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7138671875, 0.278564453125]
[0.6640625, 0.385009765625]
[0.888671875, 0.061004638671875]
[0.8701171875, 0.10791015625]
This is the real loss :  tensor(0.0581, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
148     0    NaN  ...  0.136299  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
149     0    NaN  ...  0.199091  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
150     0    NaN  ...  0.057589  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
151     0    NaN  ...  0.367424  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
152     0    NaN  ...  0.058148  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[153 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.68115234375, 0.251708984375]
[0.798828125, 0.22802734375]
[0.7158203125, 0.277587890625]
[0.8525390625, 0.0716552734375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.68115234375, 0.251708984375]
[0.798828125, 0.22802734375]
[0.7158203125, 0.277587890625]
[0.8525390625, 0.0716552734375]
This is the real loss :  tensor(0.0553, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
149     0    NaN  ...  0.199091  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
150     0    NaN  ...  0.057589  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
151     0    NaN  ...  0.367424  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
152     0    NaN  ...  0.058148  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
153     0    NaN  ...  0.055273  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[154 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.72412109375, 0.2406005859375]
[0.8017578125, 0.1785888671875]
[0.939453125, 0.1905517578125]
[0.67578125, 0.33447265625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.72412109375, 0.2406005859375]
[0.8017578125, 0.1785888671875]
[0.939453125, 0.1905517578125]
[0.67578125, 0.33447265625]
This is the real loss :  tensor(0.0578, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
150     0    NaN  ...  0.057589  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
151     0    NaN  ...  0.367424  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
152     0    NaN  ...  0.058148  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
153     0    NaN  ...  0.055273  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
154     0    NaN  ...  0.057770  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[155 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.80126953125, 0.209716796875]
[0.71826171875, 0.2548828125]
[0.7802734375, 0.2410888671875]
[0.8203125, 0.194580078125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.80126953125, 0.209716796875]
[0.71826171875, 0.2548828125]
[0.7802734375, 0.2410888671875]
[0.8203125, 0.194580078125]
This is the real loss :  tensor(0.0505, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
151     0    NaN  ...  0.367424  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
152     0    NaN  ...  0.058148  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
153     0    NaN  ...  0.055273  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
154     0    NaN  ...  0.057770  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
155     0    NaN  ...  0.050546  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[156 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.904296875, 0.08258056640625]
[0.78857421875, 0.268798828125]
[0.70361328125, 0.380615234375]
[0.86767578125, 0.171142578125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.904296875, 0.08258056640625]
[0.78857421875, 0.268798828125]
[0.70361328125, 0.380615234375]
[0.86767578125, 0.171142578125]
This is the real loss :  tensor(0.1323, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
152     0    NaN  ...  0.058148  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
153     0    NaN  ...  0.055273  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
154     0    NaN  ...  0.057770  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
155     0    NaN  ...  0.050546  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
156     0    NaN  ...  0.132305  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[157 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.7421875, 0.1751708984375]
[0.71484375, 0.1337890625]
[0.779296875, 0.298095703125]
[0.66015625, 0.229736328125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7421875, 0.1751708984375]
[0.71484375, 0.1337890625]
[0.779296875, 0.298095703125]
[0.66015625, 0.229736328125]
This is the real loss :  tensor(0.1831, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
153     0    NaN  ...  0.055273  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
154     0    NaN  ...  0.057770  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
155     0    NaN  ...  0.050546  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
156     0    NaN  ...  0.132305  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
157     0    NaN  ...  0.183076  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[158 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.6884765625, 0.2437744140625]
[0.7939453125, 0.219482421875]
[0.7001953125, 0.11541748046875]
[0.73291015625, 0.20947265625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.6884765625, 0.2437744140625]
[0.7939453125, 0.219482421875]
[0.7001953125, 0.11541748046875]
[0.73291015625, 0.20947265625]
This is the real loss :  tensor(0.1890, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
154     0    NaN  ...  0.057770  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
155     0    NaN  ...  0.050546  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
156     0    NaN  ...  0.132305  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
157     0    NaN  ...  0.183076  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
158     0    NaN  ...  0.189050  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[159 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.7880859375, 0.280517578125]
[0.8076171875, 0.1273193359375]
[0.75634765625, 0.075927734375]
[0.69873046875, 0.284912109375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7880859375, 0.280517578125]
[0.8076171875, 0.1273193359375]
[0.75634765625, 0.075927734375]
[0.69873046875, 0.284912109375]
This is the real loss :  tensor(0.4522, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
155     0    NaN  ...  0.050546  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
156     0    NaN  ...  0.132305  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
157     0    NaN  ...  0.183076  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
158     0    NaN  ...  0.189050  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
159     0    NaN  ...  0.452188  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[160 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [0., 1.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.7109375, 0.346923828125]
[1.013671875, -0.0199127197265625]
[0.72509765625, 0.221435546875]
[0.75634765625, 0.271240234375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7109375, 0.346923828125]
[1.013671875, -0.0199127197265625]
[0.72509765625, 0.221435546875]
[0.75634765625, 0.271240234375]
This is the real loss :  tensor(0.3960, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
156     0    NaN  ...  0.132305  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
157     0    NaN  ...  0.183076  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
158     0    NaN  ...  0.189050  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
159     0    NaN  ...  0.452188  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
160     0    NaN  ...  0.395951  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[161 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.81103515625, 0.252197265625]
[0.75390625, 0.23681640625]
[0.8662109375, 0.1292724609375]
[0.78173828125, 0.21240234375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.81103515625, 0.252197265625]
[0.75390625, 0.23681640625]
[0.8662109375, 0.1292724609375]
[0.78173828125, 0.21240234375]
This is the real loss :  tensor(0.2271, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
157     0    NaN  ...  0.183076  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
158     0    NaN  ...  0.189050  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
159     0    NaN  ...  0.452188  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
160     0    NaN  ...  0.395951  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
161     0    NaN  ...  0.227150  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[162 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.828125, 0.205078125]
[0.80517578125, 0.1497802734375]
[0.73095703125, 0.251708984375]
[0.6806640625, 0.301025390625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.828125, 0.205078125]
[0.80517578125, 0.1497802734375]
[0.73095703125, 0.251708984375]
[0.6806640625, 0.301025390625]
This is the real loss :  tensor(0.0575, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
158     0    NaN  ...  0.189050  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
159     0    NaN  ...  0.452188  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
160     0    NaN  ...  0.395951  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
161     0    NaN  ...  0.227150  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
162     0    NaN  ...  0.057540  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[163 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.76416015625, 0.11285400390625]
[0.60400390625, 0.302978515625]
[0.71142578125, 0.1783447265625]
[0.75244140625, 0.1275634765625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.76416015625, 0.11285400390625]
[0.60400390625, 0.302978515625]
[0.71142578125, 0.1783447265625]
[0.75244140625, 0.1275634765625]
This is the real loss :  tensor(0.1390, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
159     0    NaN  ...  0.452188  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
160     0    NaN  ...  0.395951  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
161     0    NaN  ...  0.227150  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
162     0    NaN  ...  0.057540  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
163     0    NaN  ...  0.138957  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[164 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.91748046875, 0.1739501953125]
[0.65380859375, 0.3828125]
[0.66748046875, 0.2490234375]
[0.94677734375, 0.049285888671875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.91748046875, 0.1739501953125]
[0.65380859375, 0.3828125]
[0.66748046875, 0.2490234375]
[0.94677734375, 0.049285888671875]
This is the real loss :  tensor(0.1648, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
160     0    NaN  ...  0.395951  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
161     0    NaN  ...  0.227150  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
162     0    NaN  ...  0.057540  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
163     0    NaN  ...  0.138957  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
164     0    NaN  ...  0.164777  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[165 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.6767578125, 0.275390625]
[0.81640625, 0.271240234375]
[0.78125, 0.1329345703125]
[0.71923828125, 0.261474609375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.6767578125, 0.275390625]
[0.81640625, 0.271240234375]
[0.78125, 0.1329345703125]
[0.71923828125, 0.261474609375]
This is the real loss :  tensor(0.0625, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
161     0    NaN  ...  0.227150  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
162     0    NaN  ...  0.057540  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
163     0    NaN  ...  0.138957  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
164     0    NaN  ...  0.164777  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
165     0    NaN  ...  0.062540  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[166 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.6953125, 0.360107421875]
[0.70751953125, 0.22998046875]
[0.73583984375, 0.1268310546875]
[0.72607421875, 0.1646728515625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.6953125, 0.360107421875]
[0.70751953125, 0.22998046875]
[0.73583984375, 0.1268310546875]
[0.72607421875, 0.1646728515625]
This is the real loss :  tensor(0.1880, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
162     0    NaN  ...  0.057540  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
163     0    NaN  ...  0.138957  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
164     0    NaN  ...  0.164777  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
165     0    NaN  ...  0.062540  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
166     0    NaN  ...  0.188006  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[167 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.85595703125, 0.08740234375]
[0.6826171875, 0.2841796875]
[0.7412109375, 0.344482421875]
[0.79248046875, 0.1763916015625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.85595703125, 0.08740234375]
[0.6826171875, 0.2841796875]
[0.7412109375, 0.344482421875]
[0.79248046875, 0.1763916015625]
This is the real loss :  tensor(0.0587, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
163     0    NaN  ...  0.138957  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
164     0    NaN  ...  0.164777  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
165     0    NaN  ...  0.062540  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
166     0    NaN  ...  0.188006  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
167     0    NaN  ...  0.058712  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[168 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.76806640625, 0.1888427734375]
[0.720703125, 0.2479248046875]
[0.69091796875, 0.3515625]
[0.86083984375, 0.1434326171875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.76806640625, 0.1888427734375]
[0.720703125, 0.2479248046875]
[0.69091796875, 0.3515625]
[0.86083984375, 0.1434326171875]
This is the real loss :  tensor(0.0610, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
164     0    NaN  ...  0.164777  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
165     0    NaN  ...  0.062540  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
166     0    NaN  ...  0.188006  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
167     0    NaN  ...  0.058712  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
168     0    NaN  ...  0.060999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[169 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.6767578125, 0.345947265625]
[0.67431640625, 0.1588134765625]
[0.75, 0.1717529296875]
[0.77294921875, 0.1846923828125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.6767578125, 0.345947265625]
[0.67431640625, 0.1588134765625]
[0.75, 0.1717529296875]
[0.77294921875, 0.1846923828125]
This is the real loss :  tensor(0.2137, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
165     0    NaN  ...  0.062540  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
166     0    NaN  ...  0.188006  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
167     0    NaN  ...  0.058712  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
168     0    NaN  ...  0.060999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
169     0    NaN  ...  0.213704  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[170 rows x 5 columns]checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3

Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.8359375, 0.0958251953125]
[0.58984375, 0.423095703125]
[0.921875, 0.00782012939453125]
[0.63134765625, 0.2137451171875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8359375, 0.0958251953125]
[0.58984375, 0.423095703125]
[0.921875, 0.00782012939453125]
[0.63134765625, 0.2137451171875]
This is the real loss :  tensor(0.0714, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
166     0    NaN  ...  0.188006  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
167     0    NaN  ...  0.058712  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
168     0    NaN  ...  0.060999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
169     0    NaN  ...  0.213704  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
170     0    NaN  ...  0.071387  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[171 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.7421875, 0.2315673828125]
[0.87255859375, 0.14453125]
[0.65966796875, 0.2232666015625]
[0.7001953125, 0.30859375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7421875, 0.2315673828125]
[0.87255859375, 0.14453125]
[0.65966796875, 0.2232666015625]
[0.7001953125, 0.30859375]
This is the real loss :  tensor(0.1912, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
167     0    NaN  ...  0.058712  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
168     0    NaN  ...  0.060999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
169     0    NaN  ...  0.213704  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
170     0    NaN  ...  0.071387  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
171     0    NaN  ...  0.191156  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[172 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.8408203125, 0.08197021484375]
[0.6298828125, 0.2783203125]
[0.93310546875, 0.033447265625]
[0.60009765625, 0.393310546875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8408203125, 0.08197021484375]
[0.6298828125, 0.2783203125]
[0.93310546875, 0.033447265625]
[0.60009765625, 0.393310546875]
This is the real loss :  tensor(0.0708, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
168     0    NaN  ...  0.060999  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
169     0    NaN  ...  0.213704  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
170     0    NaN  ...  0.071387  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
171     0    NaN  ...  0.191156  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
172     0    NaN  ...  0.070839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[173 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.83984375, 0.08544921875]
[0.947265625, 0.1131591796875]
[0.634765625, 0.30712890625]
[0.6953125, 0.284912109375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.83984375, 0.08544921875]
[0.947265625, 0.1131591796875]
[0.634765625, 0.30712890625]
[0.6953125, 0.284912109375]
This is the real loss :  tensor(0.1589, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
169     0    NaN  ...  0.213704  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
170     0    NaN  ...  0.071387  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
171     0    NaN  ...  0.191156  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
172     0    NaN  ...  0.070839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
173     0    NaN  ...  0.158884  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[174 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.671875, 0.31787109375]
[0.796875, 0.0212249755859375]
[0.666015625, 0.32080078125]
[0.869140625, 0.12646484375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.671875, 0.31787109375]
[0.796875, 0.0212249755859375]
[0.666015625, 0.32080078125]
[0.869140625, 0.12646484375]
This is the real loss :  tensor(0.2479, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
170     0    NaN  ...  0.071387  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
171     0    NaN  ...  0.191156  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
172     0    NaN  ...  0.070839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
173     0    NaN  ...  0.158884  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
174     0    NaN  ...  0.247918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[175 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.79541015625, 0.1431884765625]
[0.7158203125, 0.26708984375]
[0.71875, 0.1875]
[0.833984375, 0.2208251953125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.79541015625, 0.1431884765625]
[0.7158203125, 0.26708984375]
[0.71875, 0.1875]
[0.833984375, 0.2208251953125]
This is the real loss :  tensor(0.1628, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
171     0    NaN  ...  0.191156  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
172     0    NaN  ...  0.070839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
173     0    NaN  ...  0.158884  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
174     0    NaN  ...  0.247918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
175     0    NaN  ...  0.162812  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[176 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.8349609375, 0.24853515625]
[0.765625, 0.22998046875]
[0.8203125, 0.1712646484375]
[0.86474609375, 0.2060546875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8349609375, 0.24853515625]
[0.765625, 0.22998046875]
[0.8203125, 0.1712646484375]
[0.86474609375, 0.2060546875]
This is the real loss :  tensor(0.1865, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
172     0    NaN  ...  0.070839  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
173     0    NaN  ...  0.158884  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
174     0    NaN  ...  0.247918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
175     0    NaN  ...  0.162812  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
176     0    NaN  ...  0.186507  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[177 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.8623046875, 0.040802001953125]
[0.67919921875, 0.341552734375]
[0.935546875, 0.08404541015625]
[0.72509765625, 0.2900390625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8623046875, 0.040802001953125]
[0.67919921875, 0.341552734375]
[0.935546875, 0.08404541015625]
[0.72509765625, 0.2900390625]
This is the real loss :  tensor(0.1358, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
173     0    NaN  ...  0.158884  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
174     0    NaN  ...  0.247918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
175     0    NaN  ...  0.162812  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
176     0    NaN  ...  0.186507  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
177     0    NaN  ...  0.135800  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[178 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.75146484375, 0.2822265625]
[0.6474609375, 0.186279296875]
[0.71728515625, 0.063232421875]
[0.68603515625, 0.1611328125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.75146484375, 0.2822265625]
[0.6474609375, 0.186279296875]
[0.71728515625, 0.063232421875]
[0.68603515625, 0.1611328125]
This is the real loss :  tensor(0.1789, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
174     0    NaN  ...  0.247918  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
175     0    NaN  ...  0.162812  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
176     0    NaN  ...  0.186507  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
177     0    NaN  ...  0.135800  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
178     0    NaN  ...  0.178904  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[179 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.75732421875, 0.3076171875]
[0.873046875, 0.1617431640625]
[0.78466796875, 0.1478271484375]
[0.77880859375, 0.2255859375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.75732421875, 0.3076171875]
[0.873046875, 0.1617431640625]
[0.78466796875, 0.1478271484375]
[0.77880859375, 0.2255859375]
This is the real loss :  tensor(0.0455, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
175     0    NaN  ...  0.162812  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
176     0    NaN  ...  0.186507  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
177     0    NaN  ...  0.135800  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
178     0    NaN  ...  0.178904  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
179     0    NaN  ...  0.045479  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[180 rows x 5 columns]
Training labels: tensor([[0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.88232421875, 0.08746337890625]
[0.751953125, 0.260009765625]
[0.71240234375, 0.337158203125]
[0.81103515625, 0.137451171875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.88232421875, 0.08746337890625]
[0.751953125, 0.260009765625]
[0.71240234375, 0.337158203125]
[0.81103515625, 0.137451171875]
This is the real loss :  tensor(0.2489, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
176     0    NaN  ...  0.186507  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
177     0    NaN  ...  0.135800  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
178     0    NaN  ...  0.178904  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
179     0    NaN  ...  0.045479  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
180     0    NaN  ...  0.248918  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[181 rows x 5 columns]
Training labels: tensor([[1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.]], device='cuda:0')
Logits shape before squeeze: torch.Size([4, 2])
[0.67041015625, 0.217041015625]
[0.7705078125, 0.283203125]
[0.841796875, 0.131591796875]
[0.93115234375, 0.14306640625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.67041015625, 0.217041015625]
[0.7705078125, 0.283203125]
[0.841796875, 0.131591796875]
[0.93115234375, 0.14306640625]
This is the real loss :  tensor(0.2221, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
177     0    NaN  ...  0.135800  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
178     0    NaN  ...  0.178904  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
179     0    NaN  ...  0.045479  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
180     0    NaN  ...  0.248918  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
181     0    NaN  ...  0.222071  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[182 rows x 5 columns]
Epoch 2/50
Logits shape before squeeze: torch.Size([4, 2])
[0.90380859375, 0.054229736328125]
[0.69580078125, 0.391845703125]
[0.814453125, 0.24853515625]
[0.9345703125, 0.1435546875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.90380859375, 0.054229736328125]
[0.69580078125, 0.391845703125]
[0.814453125, 0.24853515625]
[0.9345703125, 0.1435546875]
This is the real loss :  tensor(0.0474, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
178     0    NaN  ...  0.178904  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
179     0    NaN  ...  0.045479  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
180     0    NaN  ...  0.248918  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
181     0    NaN  ...  0.222071  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
182     1    NaN  ...  0.047420  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[183 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.75341796875, 0.30322265625]
[0.76171875, 0.316650390625]
[0.82177734375, 0.0927734375]
[0.77294921875, 0.08807373046875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.75341796875, 0.30322265625]
[0.76171875, 0.316650390625]
[0.82177734375, 0.0927734375]
[0.77294921875, 0.08807373046875]
This is the real loss :  tensor(0.0512, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
179     0    NaN  ...  0.045479  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
180     0    NaN  ...  0.248918  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
181     0    NaN  ...  0.222071  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
182     1    NaN  ...  0.047420  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
183     1    NaN  ...  0.051184  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[184 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.6884765625, 0.291015625]
[0.80419921875, 0.1922607421875]
[0.93212890625, 0.1258544921875]
[0.86669921875, 0.11859130859375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.6884765625, 0.291015625]
[0.80419921875, 0.1922607421875]
[0.93212890625, 0.1258544921875]
[0.86669921875, 0.11859130859375]
This is the real loss :  tensor(0.3396, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
180     0    NaN  ...  0.248918  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
181     0    NaN  ...  0.222071  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
182     1    NaN  ...  0.047420  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
183     1    NaN  ...  0.051184  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
184     1    NaN  ...  0.339599  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[185 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.74169921875, 0.24560546875]
[0.953125, 0.09625244140625]
[0.7998046875, 0.36181640625]
[0.8076171875, 0.1700439453125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.74169921875, 0.24560546875]
[0.953125, 0.09625244140625]
[0.7998046875, 0.36181640625]
[0.8076171875, 0.1700439453125]
This is the real loss :  tensor(0.0469, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
181     0    NaN  ...  0.222071  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
182     1    NaN  ...  0.047420  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
183     1    NaN  ...  0.051184  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
184     1    NaN  ...  0.339599  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
185     1    NaN  ...  0.046927  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[186 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.82763671875, 0.219970703125]
[0.9716796875, 0.1654052734375]
[0.7294921875, 0.271728515625]
[0.8681640625, 0.172119140625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.82763671875, 0.219970703125]
[0.9716796875, 0.1654052734375]
[0.7294921875, 0.271728515625]
[0.8681640625, 0.172119140625]
This is the real loss :  tensor(0.1520, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
182     1    NaN  ...  0.047420  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
183     1    NaN  ...  0.051184  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
184     1    NaN  ...  0.339599  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
185     1    NaN  ...  0.046927  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
186     1    NaN  ...  0.151975  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[187 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.91650390625, 0.1719970703125]
[1.037109375, 0.07806396484375]
[0.8681640625, 0.1820068359375]
[0.69970703125, 0.36181640625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.91650390625, 0.1719970703125]
[1.037109375, 0.07806396484375]
[0.8681640625, 0.1820068359375]
[0.69970703125, 0.36181640625]
This is the real loss :  tensor(0.2256, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
183     1    NaN  ...  0.051184  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
184     1    NaN  ...  0.339599  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
185     1    NaN  ...  0.046927  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
186     1    NaN  ...  0.151975  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
187     1    NaN  ...  0.225579  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[188 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.7822265625, 0.263916015625]
[1.060546875, 0.08551025390625]
[0.72412109375, 0.270751953125]
[0.78955078125, 0.1883544921875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7822265625, 0.263916015625]
[1.060546875, 0.08551025390625]
[0.72412109375, 0.270751953125]
[0.78955078125, 0.1883544921875]
This is the real loss :  tensor(0.2884, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
184     1    NaN  ...  0.339599  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
185     1    NaN  ...  0.046927  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
186     1    NaN  ...  0.151975  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
187     1    NaN  ...  0.225579  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
188     1    NaN  ...  0.288414  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[189 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.95068359375, 0.1968994140625]
[0.7373046875, 0.2763671875]
[0.78271484375, 0.1868896484375]
[0.9140625, 0.193115234375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.95068359375, 0.1968994140625]
[0.7373046875, 0.2763671875]
[0.78271484375, 0.1868896484375]
[0.9140625, 0.193115234375]
This is the real loss :  tensor(0.2194, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
185     1    NaN  ...  0.046927  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
186     1    NaN  ...  0.151975  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
187     1    NaN  ...  0.225579  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
188     1    NaN  ...  0.288414  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
189     1    NaN  ...  0.219413  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[190 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.76708984375, 0.24755859375]
[0.841796875, 0.22216796875]
[0.79345703125, 0.251953125]
[0.9072265625, 0.07318115234375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.76708984375, 0.24755859375]
[0.841796875, 0.22216796875]
[0.79345703125, 0.251953125]
[0.9072265625, 0.07318115234375]
This is the real loss :  tensor(0.0388, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
186     1    NaN  ...  0.151975  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
187     1    NaN  ...  0.225579  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
188     1    NaN  ...  0.288414  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
189     1    NaN  ...  0.219413  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
190     1    NaN  ...  0.038753  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[191 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.7666015625, 0.302734375]
[0.92041015625, 0.1480712890625]
[0.8798828125, 0.1802978515625]
[0.81787109375, 0.2362060546875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7666015625, 0.302734375]
[0.92041015625, 0.1480712890625]
[0.8798828125, 0.1802978515625]
[0.81787109375, 0.2362060546875]
This is the real loss :  tensor(0.4068, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
187     1    NaN  ...  0.225579  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
188     1    NaN  ...  0.288414  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
189     1    NaN  ...  0.219413  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
190     1    NaN  ...  0.038753  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
191     1    NaN  ...  0.406766  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[192 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.77685546875, 0.2369384765625]
[0.68212890625, 0.26318359375]
[0.8232421875, 0.163818359375]
[0.88427734375, 0.1529541015625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.77685546875, 0.2369384765625]
[0.68212890625, 0.26318359375]
[0.8232421875, 0.163818359375]
[0.88427734375, 0.1529541015625]
This is the real loss :  tensor(0.0464, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
188     1    NaN  ...  0.288414  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
189     1    NaN  ...  0.219413  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
190     1    NaN  ...  0.038753  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
191     1    NaN  ...  0.406766  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
192     1    NaN  ...  0.046388  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[193 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.74755859375, 0.275146484375]
[0.7421875, 0.1229248046875]
[0.79931640625, 0.176513671875]
[0.7841796875, 0.19287109375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.74755859375, 0.275146484375]
[0.7421875, 0.1229248046875]
[0.79931640625, 0.176513671875]
[0.7841796875, 0.19287109375]
This is the real loss :  tensor(0.0470, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
189     1    NaN  ...  0.219413  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
190     1    NaN  ...  0.038753  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
191     1    NaN  ...  0.406766  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
192     1    NaN  ...  0.046388  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
193     1    NaN  ...  0.047027  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[194 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.75927734375, 0.292724609375]
[0.662109375, 0.1474609375]
[0.78564453125, 0.1494140625]
[0.8046875, 0.152099609375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.75927734375, 0.292724609375]
[0.662109375, 0.1474609375]
[0.78564453125, 0.1494140625]
[0.8046875, 0.152099609375]
This is the real loss :  tensor(0.2143, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
190     1    NaN  ...  0.038753  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
191     1    NaN  ...  0.406766  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
192     1    NaN  ...  0.046388  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
193     1    NaN  ...  0.047027  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
194     1    NaN  ...  0.214285  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[195 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.6923828125, 0.17724609375]
[0.7724609375, 0.30517578125]
[0.80419921875, 0.1435546875]
[0.81103515625, 0.1348876953125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.6923828125, 0.17724609375]
[0.7724609375, 0.30517578125]
[0.80419921875, 0.1435546875]
[0.81103515625, 0.1348876953125]
This is the real loss :  tensor(0.2131, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
191     1    NaN  ...  0.406766  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
192     1    NaN  ...  0.046388  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
193     1    NaN  ...  0.047027  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
194     1    NaN  ...  0.214285  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
195     1    NaN  ...  0.213136  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[196 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.70458984375, 0.190673828125]
[0.72900390625, 0.07098388671875]
[0.79931640625, 0.25390625]
[0.77197265625, 0.2313232421875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.70458984375, 0.190673828125]
[0.72900390625, 0.07098388671875]
[0.79931640625, 0.25390625]
[0.77197265625, 0.2313232421875]
This is the real loss :  tensor(0.1800, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
192     1    NaN  ...  0.046388  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
193     1    NaN  ...  0.047027  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
194     1    NaN  ...  0.214285  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
195     1    NaN  ...  0.213136  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
196     1    NaN  ...  0.180023  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[197 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.71533203125, 0.10931396484375]
[0.79296875, 0.21826171875]
[0.734375, 0.236572265625]
[0.7705078125, 0.132080078125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.71533203125, 0.10931396484375]
[0.79296875, 0.21826171875]
[0.734375, 0.236572265625]
[0.7705078125, 0.132080078125]
This is the real loss :  tensor(0.0475, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
193     1    NaN  ...  0.047027  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
194     1    NaN  ...  0.214285  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
195     1    NaN  ...  0.213136  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
196     1    NaN  ...  0.180023  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
197     1    NaN  ...  0.047515  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[198 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.8662109375, 0.2423095703125]
[0.81884765625, 0.180908203125]
[0.77734375, 0.19677734375]
[0.7861328125, 0.20947265625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8662109375, 0.2423095703125]
[0.81884765625, 0.180908203125]
[0.77734375, 0.19677734375]
[0.7861328125, 0.20947265625]
This is the real loss :  tensor(0.1995, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
194     1    NaN  ...  0.214285  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
195     1    NaN  ...  0.213136  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
196     1    NaN  ...  0.180023  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
197     1    NaN  ...  0.047515  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
198     1    NaN  ...  0.199494  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[199 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.79052734375, 0.186767578125]
[0.86181640625, 0.22216796875]
[0.79345703125, 0.196533203125]
[0.77392578125, 0.152099609375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.79052734375, 0.186767578125]
[0.86181640625, 0.22216796875]
[0.79345703125, 0.196533203125]
[0.77392578125, 0.152099609375]
This is the real loss :  tensor(0.0378, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
195     1    NaN  ...  0.213136  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
196     1    NaN  ...  0.180023  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
197     1    NaN  ...  0.047515  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
198     1    NaN  ...  0.199494  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
199     1    NaN  ...  0.037843  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[200 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.78125, 0.186279296875]
[0.81494140625, 0.28466796875]
[0.82666015625, 0.1700439453125]
[0.82666015625, 0.1512451171875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.78125, 0.186279296875]
[0.81494140625, 0.28466796875]
[0.82666015625, 0.1700439453125]
[0.82666015625, 0.1512451171875]
This is the real loss :  tensor(0.3200, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
196     1    NaN  ...  0.180023  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
197     1    NaN  ...  0.047515  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
198     1    NaN  ...  0.199494  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
199     1    NaN  ...  0.037843  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
200     1    NaN  ...  0.320026  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[201 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.86669921875, 0.2099609375]
[0.833984375, 0.1898193359375]
[0.896484375, 0.198974609375]
[0.8056640625, 0.22119140625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.86669921875, 0.2099609375]
[0.833984375, 0.1898193359375]
[0.896484375, 0.198974609375]
[0.8056640625, 0.22119140625]
This is the real loss :  tensor(0.0328, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
197     1    NaN  ...  0.047515  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
198     1    NaN  ...  0.199494  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
199     1    NaN  ...  0.037843  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
200     1    NaN  ...  0.320026  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
201     1    NaN  ...  0.032805  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[202 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.78515625, 0.181640625]
[0.8935546875, 0.145263671875]
[0.82177734375, 0.1365966796875]
[0.810546875, 0.28076171875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.78515625, 0.181640625]
[0.8935546875, 0.145263671875]
[0.82177734375, 0.1365966796875]
[0.810546875, 0.28076171875]
This is the real loss :  tensor(0.0346, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
198     1    NaN  ...  0.199494  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
199     1    NaN  ...  0.037843  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
200     1    NaN  ...  0.320026  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
201     1    NaN  ...  0.032805  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
202     1    NaN  ...  0.034591  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[203 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.87744140625, 0.1943359375]
[0.84765625, 0.1920166015625]
[0.80224609375, 0.2451171875]
[0.87548828125, 0.1883544921875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.87744140625, 0.1943359375]
[0.84765625, 0.1920166015625]
[0.80224609375, 0.2451171875]
[0.87548828125, 0.1883544921875]
This is the real loss :  tensor(0.1722, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
199     1    NaN  ...  0.037843  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
200     1    NaN  ...  0.320026  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
201     1    NaN  ...  0.032805  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
202     1    NaN  ...  0.034591  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
203     1    NaN  ...  0.172162  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[204 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.8310546875, 0.1961669921875]
[0.8359375, 0.204833984375]
[0.888671875, 0.264892578125]
[0.861328125, 0.125732421875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8310546875, 0.1961669921875]
[0.8359375, 0.204833984375]
[0.888671875, 0.264892578125]
[0.861328125, 0.125732421875]
This is the real loss :  tensor(0.3734, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
200     1    NaN  ...  0.320026  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
201     1    NaN  ...  0.032805  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
202     1    NaN  ...  0.034591  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
203     1    NaN  ...  0.172162  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
204     1    NaN  ...  0.373362  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[205 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.92626953125, 0.1575927734375]
[0.916015625, 0.177978515625]
[0.8310546875, 0.2369384765625]
[0.83642578125, 0.23828125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.92626953125, 0.1575927734375]
[0.916015625, 0.177978515625]
[0.8310546875, 0.2369384765625]
[0.83642578125, 0.23828125]
This is the real loss :  tensor(0.0297, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
201     1    NaN  ...  0.032805  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
202     1    NaN  ...  0.034591  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
203     1    NaN  ...  0.172162  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
204     1    NaN  ...  0.373362  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
205     1    NaN  ...  0.029652  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[206 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.8212890625, 0.207275390625]
[0.822265625, 0.2069091796875]
[0.8798828125, 0.138916015625]
[0.78857421875, 0.188232421875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8212890625, 0.207275390625]
[0.822265625, 0.2069091796875]
[0.8798828125, 0.138916015625]
[0.78857421875, 0.188232421875]
This is the real loss :  tensor(0.1830, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
202     1    NaN  ...  0.034591  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
203     1    NaN  ...  0.172162  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
204     1    NaN  ...  0.373362  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
205     1    NaN  ...  0.029652  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
206     1    NaN  ...  0.182980  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[207 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.84423828125, 0.18359375]
[0.84228515625, 0.1614990234375]
[0.82275390625, 0.2344970703125]
[0.857421875, 0.1927490234375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.84423828125, 0.18359375]
[0.84228515625, 0.1614990234375]
[0.82275390625, 0.2344970703125]
[0.857421875, 0.1927490234375]
This is the real loss :  tensor(0.0316, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
203     1    NaN  ...  0.172162  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
204     1    NaN  ...  0.373362  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
205     1    NaN  ...  0.029652  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
206     1    NaN  ...  0.182980  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
207     1    NaN  ...  0.031601  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[208 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.849609375, 0.1705322265625]
[0.8857421875, 0.1824951171875]
[0.880859375, 0.189697265625]
[0.85498046875, 0.2034912109375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.849609375, 0.1705322265625]
[0.8857421875, 0.1824951171875]
[0.880859375, 0.189697265625]
[0.85498046875, 0.2034912109375]
This is the real loss :  tensor(0.0263, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
204     1    NaN  ...  0.373362  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
205     1    NaN  ...  0.029652  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
206     1    NaN  ...  0.182980  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
207     1    NaN  ...  0.031601  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
208     1    NaN  ...  0.026335  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[209 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.84375, 0.0364990234375]
[0.82763671875, 0.191162109375]
[0.79345703125, 0.179443359375]
[0.79052734375, 0.1890869140625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.84375, 0.0364990234375]
[0.82763671875, 0.191162109375]
[0.79345703125, 0.179443359375]
[0.79052734375, 0.1890869140625]
This is the real loss :  tensor(0.0308, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
205     1    NaN  ...  0.029652  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
206     1    NaN  ...  0.182980  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
207     1    NaN  ...  0.031601  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
208     1    NaN  ...  0.026335  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
209     1    NaN  ...  0.030811  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[210 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.84228515625, 0.181396484375]
[0.96826171875, 0.19140625]
[0.90478515625, 0.1339111328125]
[0.84619140625, 0.2254638671875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.84228515625, 0.181396484375]
[0.96826171875, 0.19140625]
[0.90478515625, 0.1339111328125]
[0.84619140625, 0.2254638671875]
This is the real loss :  tensor(0.1798, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
206     1    NaN  ...  0.182980  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
207     1    NaN  ...  0.031601  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
208     1    NaN  ...  0.026335  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
209     1    NaN  ...  0.030811  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
210     1    NaN  ...  0.179796  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[211 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.86572265625, 0.1346435546875]
[0.8115234375, 0.1990966796875]
[0.80078125, 0.1961669921875]
[0.79833984375, 0.1756591796875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.86572265625, 0.1346435546875]
[0.8115234375, 0.1990966796875]
[0.80078125, 0.1961669921875]
[0.79833984375, 0.1756591796875]
This is the real loss :  tensor(0.1883, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
207     1    NaN  ...  0.031601  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
208     1    NaN  ...  0.026335  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
209     1    NaN  ...  0.030811  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
210     1    NaN  ...  0.179796  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
211     1    NaN  ...  0.188297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[212 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.85546875, 0.11126708984375]
[0.8076171875, 0.1312255859375]
[0.80859375, 0.18310546875]
[0.79443359375, 0.1923828125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.85546875, 0.11126708984375]
[0.8076171875, 0.1312255859375]
[0.80859375, 0.18310546875]
[0.79443359375, 0.1923828125]
This is the real loss :  tensor(0.0296, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
208     1    NaN  ...  0.026335  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
209     1    NaN  ...  0.030811  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
210     1    NaN  ...  0.179796  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
211     1    NaN  ...  0.188297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
212     1    NaN  ...  0.029617  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[213 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.82763671875, 0.1412353515625]
[0.80029296875, 0.113525390625]
[0.76904296875, 0.162353515625]
[0.85791015625, 0.1864013671875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.82763671875, 0.1412353515625]
[0.80029296875, 0.113525390625]
[0.76904296875, 0.162353515625]
[0.85791015625, 0.1864013671875]
This is the real loss :  tensor(0.0296, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
209     1    NaN  ...  0.030811  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
210     1    NaN  ...  0.179796  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
211     1    NaN  ...  0.188297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
212     1    NaN  ...  0.029617  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
213     1    NaN  ...  0.029633  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[214 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.86474609375, 0.1168212890625]
[0.7607421875, 0.22216796875]
[0.86572265625, 0.1109619140625]
[0.87353515625, 0.075927734375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.86474609375, 0.1168212890625]
[0.7607421875, 0.22216796875]
[0.86572265625, 0.1109619140625]
[0.87353515625, 0.075927734375]
This is the real loss :  tensor(0.0238, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
210     1    NaN  ...  0.179796  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
211     1    NaN  ...  0.188297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
212     1    NaN  ...  0.029617  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
213     1    NaN  ...  0.029633  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
214     1    NaN  ...  0.023831  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[215 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.9306640625, 0.10040283203125]
[0.7880859375, 0.11370849609375]
[0.8046875, 0.08392333984375]
[0.82275390625, 0.19580078125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.9306640625, 0.10040283203125]
[0.7880859375, 0.11370849609375]
[0.8046875, 0.08392333984375]
[0.82275390625, 0.19580078125]
This is the real loss :  tensor(0.0235, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
211     1    NaN  ...  0.188297  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
212     1    NaN  ...  0.029617  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
213     1    NaN  ...  0.029633  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
214     1    NaN  ...  0.023831  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
215     1    NaN  ...  0.023459  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[216 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.83154296875, 0.1307373046875]
[0.75, 0.1900634765625]
[0.78564453125, 0.084716796875]
[0.919921875, 0.05938720703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.83154296875, 0.1307373046875]
[0.75, 0.1900634765625]
[0.78564453125, 0.084716796875]
[0.919921875, 0.05938720703125]
This is the real loss :  tensor(0.2011, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
212     1    NaN  ...  0.029617  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
213     1    NaN  ...  0.029633  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
214     1    NaN  ...  0.023831  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
215     1    NaN  ...  0.023459  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
216     1    NaN  ...  0.201096  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[217 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.93896484375, 0.004734039306640625]
[0.82568359375, 0.1595458984375]
[0.845703125, 0.2386474609375]
[0.82275390625, 0.1593017578125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.93896484375, 0.004734039306640625]
[0.82568359375, 0.1595458984375]
[0.845703125, 0.2386474609375]
[0.82275390625, 0.1593017578125]
This is the real loss :  tensor(0.3570, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
213     1    NaN  ...  0.029633  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
214     1    NaN  ...  0.023831  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
215     1    NaN  ...  0.023459  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
216     1    NaN  ...  0.201096  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
217     1    NaN  ...  0.357040  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[218 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[1.0205078125, 0.0073089599609375]
[0.83349609375, 0.196533203125]
[0.810546875, 0.205078125]
[0.82861328125, 0.1365966796875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[1.0205078125, 0.0073089599609375]
[0.83349609375, 0.196533203125]
[0.810546875, 0.205078125]
[0.82861328125, 0.1365966796875]
This is the real loss :  tensor(0.0241, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
214     1    NaN  ...  0.023831  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
215     1    NaN  ...  0.023459  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
216     1    NaN  ...  0.201096  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
217     1    NaN  ...  0.357040  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
218     1    NaN  ...  0.024101  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[219 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.78564453125, 0.1275634765625]
[0.9306640625, 0.1363525390625]
[0.8388671875, 0.043365478515625]
[0.77978515625, 0.2044677734375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.78564453125, 0.1275634765625]
[0.9306640625, 0.1363525390625]
[0.8388671875, 0.043365478515625]
[0.77978515625, 0.2044677734375]
This is the real loss :  tensor(0.0255, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
215     1    NaN  ...  0.023459  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
216     1    NaN  ...  0.201096  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
217     1    NaN  ...  0.357040  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
218     1    NaN  ...  0.024101  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
219     1    NaN  ...  0.025471  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[220 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.9833984375, 0.031097412109375]
[0.7578125, 0.1287841796875]
[0.81298828125, 0.2186279296875]
[0.92041015625, 0.138427734375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.9833984375, 0.031097412109375]
[0.7578125, 0.1287841796875]
[0.81298828125, 0.2186279296875]
[0.92041015625, 0.138427734375]
This is the real loss :  tensor(0.0231, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
216     1    NaN  ...  0.201096  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
217     1    NaN  ...  0.357040  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
218     1    NaN  ...  0.024101  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
219     1    NaN  ...  0.025471  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
220     1    NaN  ...  0.023094  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[221 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.93359375, 0.1285400390625]
[0.84765625, 0.1805419921875]
[0.74853515625, 0.1531982421875]
[1.01953125, 0.06817626953125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.93359375, 0.1285400390625]
[0.84765625, 0.1805419921875]
[0.74853515625, 0.1531982421875]
[1.01953125, 0.06817626953125]
This is the real loss :  tensor(0.3891, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
217     1    NaN  ...  0.357040  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
218     1    NaN  ...  0.024101  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
219     1    NaN  ...  0.025471  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
220     1    NaN  ...  0.023094  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
221     1    NaN  ...  0.389101  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[222 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.76318359375, 0.17138671875]
[1.0048828125, 0.0016918182373046875]
[0.81103515625, 0.259521484375]
[0.9052734375, 0.053192138671875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.76318359375, 0.17138671875]
[1.0048828125, 0.0016918182373046875]
[0.81103515625, 0.259521484375]
[0.9052734375, 0.053192138671875]
This is the real loss :  tensor(0.0250, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
218     1    NaN  ...  0.024101  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
219     1    NaN  ...  0.025471  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
220     1    NaN  ...  0.023094  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
221     1    NaN  ...  0.389101  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
222     1    NaN  ...  0.025043  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[223 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.95751953125, 0.0257110595703125]
[0.90380859375, 0.045440673828125]
[0.7509765625, 0.1783447265625]
[0.9140625, 0.096923828125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.95751953125, 0.0257110595703125]
[0.90380859375, 0.045440673828125]
[0.7509765625, 0.1783447265625]
[0.9140625, 0.096923828125]
This is the real loss :  tensor(0.0155, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
219     1    NaN  ...  0.025471  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
220     1    NaN  ...  0.023094  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
221     1    NaN  ...  0.389101  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
222     1    NaN  ...  0.025043  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
223     1    NaN  ...  0.015548  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[224 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.68017578125, 0.1380615234375]
[1.03125, -0.0501708984375]
[0.80810546875, 0.11138916015625]
[0.802734375, 0.1728515625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.68017578125, 0.1380615234375]
[1.03125, -0.0501708984375]
[0.80810546875, 0.11138916015625]
[0.802734375, 0.1728515625]
This is the real loss :  tensor(0.3620, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
220     1    NaN  ...  0.023094  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
221     1    NaN  ...  0.389101  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
222     1    NaN  ...  0.025043  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
223     1    NaN  ...  0.015548  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
224     1    NaN  ...  0.362008  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[225 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.76708984375, 0.1527099609375]
[0.76611328125, 0.11639404296875]
[0.7607421875, 0.1591796875]
[0.90478515625, 0.0926513671875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.76708984375, 0.1527099609375]
[0.76611328125, 0.11639404296875]
[0.7607421875, 0.1591796875]
[0.90478515625, 0.0926513671875]
This is the real loss :  tensor(0.2338, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
221     1    NaN  ...  0.389101  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
222     1    NaN  ...  0.025043  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
223     1    NaN  ...  0.015548  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
224     1    NaN  ...  0.362008  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
225     1    NaN  ...  0.233790  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[226 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.8173828125, 0.20458984375]
[1.0244140625, -0.041473388671875]
[0.857421875, 0.188720703125]
[0.80419921875, 0.1981201171875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8173828125, 0.20458984375]
[1.0244140625, -0.041473388671875]
[0.857421875, 0.188720703125]
[0.80419921875, 0.1981201171875]
This is the real loss :  tensor(0.1796, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
222     1    NaN  ...  0.025043  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
223     1    NaN  ...  0.015548  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
224     1    NaN  ...  0.362008  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
225     1    NaN  ...  0.233790  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
226     1    NaN  ...  0.179580  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[227 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.97998046875, -0.031524658203125]
[0.74560546875, 0.25146484375]
[0.92333984375, 0.0207061767578125]
[0.9384765625, 0.175537109375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.97998046875, -0.031524658203125]
[0.74560546875, 0.25146484375]
[0.92333984375, 0.0207061767578125]
[0.9384765625, 0.175537109375]
This is the real loss :  tensor(0.2120, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
223     1    NaN  ...  0.015548  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
224     1    NaN  ...  0.362008  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
225     1    NaN  ...  0.233790  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
226     1    NaN  ...  0.179580  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
227     1    NaN  ...  0.212016  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[228 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.87646484375, 0.203125]
[0.7578125, 0.12310791015625]
[0.8017578125, 0.09954833984375]
[0.8330078125, 0.1041259765625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.87646484375, 0.203125]
[0.7578125, 0.12310791015625]
[0.8017578125, 0.09954833984375]
[0.8330078125, 0.1041259765625]
This is the real loss :  tensor(0.2028, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
224     1    NaN  ...  0.362008  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
225     1    NaN  ...  0.233790  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
226     1    NaN  ...  0.179580  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
227     1    NaN  ...  0.212016  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
228     1    NaN  ...  0.202836  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[229 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.81884765625, 0.2105712890625]
[0.75146484375, 0.202392578125]
[0.8408203125, 0.013885498046875]
[0.8076171875, 0.06182861328125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.81884765625, 0.2105712890625]
[0.75146484375, 0.202392578125]
[0.8408203125, 0.013885498046875]
[0.8076171875, 0.06182861328125]
This is the real loss :  tensor(0.0308, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
225     1    NaN  ...  0.233790  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
226     1    NaN  ...  0.179580  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
227     1    NaN  ...  0.212016  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
228     1    NaN  ...  0.202836  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
229     1    NaN  ...  0.030782  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[230 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.892578125, 0.03631591796875]
[0.74853515625, 0.23828125]
[0.927734375, 0.09686279296875]
[0.775390625, 0.210693359375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.892578125, 0.03631591796875]
[0.74853515625, 0.23828125]
[0.927734375, 0.09686279296875]
[0.775390625, 0.210693359375]
This is the real loss :  tensor(0.0303, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
226     1    NaN  ...  0.179580  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
227     1    NaN  ...  0.212016  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
228     1    NaN  ...  0.202836  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
229     1    NaN  ...  0.030782  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
230     1    NaN  ...  0.030290  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[231 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.70703125, 0.18603515625]
[0.82861328125, -0.0203857421875]
[0.8076171875, 0.24658203125]
[0.84326171875, 0.0399169921875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.70703125, 0.18603515625]
[0.82861328125, -0.0203857421875]
[0.8076171875, 0.24658203125]
[0.84326171875, 0.0399169921875]
This is the real loss :  tensor(0.0343, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
227     1    NaN  ...  0.212016  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
228     1    NaN  ...  0.202836  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
229     1    NaN  ...  0.030782  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
230     1    NaN  ...  0.030290  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
231     1    NaN  ...  0.034275  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[232 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[1.0380859375, -0.00435638427734375]
[0.7578125, 0.1558837890625]
[0.736328125, 0.242431640625]
[0.84619140625, 0.251220703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[1.0380859375, -0.00435638427734375]
[0.7578125, 0.1558837890625]
[0.736328125, 0.242431640625]
[0.84619140625, 0.251220703125]
This is the real loss :  tensor(0.1879, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
228     1    NaN  ...  0.202836  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
229     1    NaN  ...  0.030782  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
230     1    NaN  ...  0.030290  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
231     1    NaN  ...  0.034275  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
232     1    NaN  ...  0.187918  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[233 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.65380859375, 0.1927490234375]
[0.86328125, 0.11083984375]
[0.712890625, 0.171142578125]
[0.81201171875, 0.0290985107421875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.65380859375, 0.1927490234375]
[0.86328125, 0.11083984375]
[0.712890625, 0.171142578125]
[0.81201171875, 0.0290985107421875]
This is the real loss :  tensor(0.1573, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
229     1    NaN  ...  0.030782  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
230     1    NaN  ...  0.030290  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
231     1    NaN  ...  0.034275  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
232     1    NaN  ...  0.187918  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
233     1    NaN  ...  0.157251  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[234 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.7607421875, 0.1951904296875]
[0.87255859375, 0.1678466796875]
[0.90625, 0.11541748046875]
[0.794921875, 0.178466796875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7607421875, 0.1951904296875]
[0.87255859375, 0.1678466796875]
[0.90625, 0.11541748046875]
[0.794921875, 0.178466796875]
This is the real loss :  tensor(0.2272, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
230     1    NaN  ...  0.030290  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
231     1    NaN  ...  0.034275  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
232     1    NaN  ...  0.187918  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
233     1    NaN  ...  0.157251  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
234     1    NaN  ...  0.227180  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[235 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.9677734375, -0.006511688232421875]
[0.73095703125, 0.25]
[0.83544921875, 0.20263671875]
[0.8017578125, 0.14892578125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.9677734375, -0.006511688232421875]
[0.73095703125, 0.25]
[0.83544921875, 0.20263671875]
[0.8017578125, 0.14892578125]
This is the real loss :  tensor(0.0332, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
231     1    NaN  ...  0.034275  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
232     1    NaN  ...  0.187918  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
233     1    NaN  ...  0.157251  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
234     1    NaN  ...  0.227180  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
235     1    NaN  ...  0.033198  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[236 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.66748046875, 0.1405029296875]
[0.822265625, 0.2242431640625]
[0.74462890625, 0.045074462890625]
[0.6455078125, 0.1248779296875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.66748046875, 0.1405029296875]
[0.822265625, 0.2242431640625]
[0.74462890625, 0.045074462890625]
[0.6455078125, 0.1248779296875]
This is the real loss :  tensor(0.0526, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
232     1    NaN  ...  0.187918  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
233     1    NaN  ...  0.157251  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
234     1    NaN  ...  0.227180  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
235     1    NaN  ...  0.033198  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
236     1    NaN  ...  0.052586  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[237 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.98876953125, 0.249755859375]
[0.84619140625, 0.07440185546875]
[0.80224609375, 0.09765625]
[0.7978515625, 0.128662109375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.98876953125, 0.249755859375]
[0.84619140625, 0.07440185546875]
[0.80224609375, 0.09765625]
[0.7978515625, 0.128662109375]
This is the real loss :  tensor(0.2095, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
233     1    NaN  ...  0.157251  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
234     1    NaN  ...  0.227180  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
235     1    NaN  ...  0.033198  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
236     1    NaN  ...  0.052586  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
237     1    NaN  ...  0.209473  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[238 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.837890625, 0.2890625]
[0.95556640625, -0.01352691650390625]
[0.89306640625, 0.053314208984375]
[0.734375, 0.326171875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.837890625, 0.2890625]
[0.95556640625, -0.01352691650390625]
[0.89306640625, 0.053314208984375]
[0.734375, 0.326171875]
This is the real loss :  tensor(0.0379, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
234     1    NaN  ...  0.227180  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
235     1    NaN  ...  0.033198  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
236     1    NaN  ...  0.052586  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
237     1    NaN  ...  0.209473  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
238     1    NaN  ...  0.037902  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[239 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.765625, 0.2322998046875]
[1.0439453125, 0.0001455545425415039]
[0.77294921875, 0.267822265625]
[0.87060546875, 0.1339111328125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.765625, 0.2322998046875]
[1.0439453125, 0.0001455545425415039]
[0.77294921875, 0.267822265625]
[0.87060546875, 0.1339111328125]
This is the real loss :  tensor(0.1669, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
235     1    NaN  ...  0.033198  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
236     1    NaN  ...  0.052586  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
237     1    NaN  ...  0.209473  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
238     1    NaN  ...  0.037902  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
239     1    NaN  ...  0.166929  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[240 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.787109375, 0.10028076171875]
[0.76171875, 0.1485595703125]
[0.8291015625, 0.16650390625]
[0.912109375, 0.2261962890625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.787109375, 0.10028076171875]
[0.76171875, 0.1485595703125]
[0.8291015625, 0.16650390625]
[0.912109375, 0.2261962890625]
This is the real loss :  tensor(0.1969, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
236     1    NaN  ...  0.052586  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
237     1    NaN  ...  0.209473  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
238     1    NaN  ...  0.037902  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
239     1    NaN  ...  0.166929  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
240     1    NaN  ...  0.196905  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[241 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.7900390625, 0.2353515625]
[0.70654296875, 0.1719970703125]
[0.7685546875, 0.055023193359375]
[0.74658203125, 0.03277587890625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7900390625, 0.2353515625]
[0.70654296875, 0.1719970703125]
[0.7685546875, 0.055023193359375]
[0.74658203125, 0.03277587890625]
This is the real loss :  tensor(0.0421, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
237     1    NaN  ...  0.209473  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
238     1    NaN  ...  0.037902  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
239     1    NaN  ...  0.166929  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
240     1    NaN  ...  0.196905  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
241     1    NaN  ...  0.042133  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[242 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.83984375, 0.23046875]
[0.7421875, 0.2225341796875]
[0.943359375, 0.06866455078125]
[0.9033203125, 0.040863037109375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.83984375, 0.23046875]
[0.7421875, 0.2225341796875]
[0.943359375, 0.06866455078125]
[0.9033203125, 0.040863037109375]
This is the real loss :  tensor(0.0267, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
238     1    NaN  ...  0.037902  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
239     1    NaN  ...  0.166929  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
240     1    NaN  ...  0.196905  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
241     1    NaN  ...  0.042133  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
242     1    NaN  ...  0.026712  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[243 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.9501953125, 0.0723876953125]
[0.8310546875, 0.197509765625]
[0.90576171875, 0.16455078125]
[0.8369140625, 0.2392578125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.9501953125, 0.0723876953125]
[0.8310546875, 0.197509765625]
[0.90576171875, 0.16455078125]
[0.8369140625, 0.2392578125]
This is the real loss :  tensor(0.0244, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
239     1    NaN  ...  0.166929  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
240     1    NaN  ...  0.196905  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
241     1    NaN  ...  0.042133  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
242     1    NaN  ...  0.026712  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
243     1    NaN  ...  0.024384  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[244 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.84619140625, 0.1480712890625]
[0.84521484375, 0.205078125]
[0.9248046875, 0.1700439453125]
[0.9375, 0.1387939453125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.84619140625, 0.1480712890625]
[0.84521484375, 0.205078125]
[0.9248046875, 0.1700439453125]
[0.9375, 0.1387939453125]
This is the real loss :  tensor(0.0212, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
240     1    NaN  ...  0.196905  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
241     1    NaN  ...  0.042133  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
242     1    NaN  ...  0.026712  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
243     1    NaN  ...  0.024384  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
244     1    NaN  ...  0.021167  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[245 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.93408203125, 0.18115234375]
[0.8076171875, 0.2235107421875]
[0.8984375, 0.11517333984375]
[0.916015625, 0.0819091796875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.93408203125, 0.18115234375]
[0.8076171875, 0.2235107421875]
[0.8984375, 0.11517333984375]
[0.916015625, 0.0819091796875]
This is the real loss :  tensor(0.0202, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
241     1    NaN  ...  0.042133  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
242     1    NaN  ...  0.026712  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
243     1    NaN  ...  0.024384  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
244     1    NaN  ...  0.021167  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
245     1    NaN  ...  0.020184  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[246 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.9716796875, -0.0007543563842773438]
[0.7802734375, 0.214111328125]
[0.96826171875, 0.0810546875]
[0.7802734375, 0.2125244140625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.9716796875, -0.0007543563842773438]
[0.7802734375, 0.214111328125]
[0.96826171875, 0.0810546875]
[0.7802734375, 0.2125244140625]
This is the real loss :  tensor(0.0245, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
242     1    NaN  ...  0.026712  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
243     1    NaN  ...  0.024384  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
244     1    NaN  ...  0.021167  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
245     1    NaN  ...  0.020184  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
246     1    NaN  ...  0.024494  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[247 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.89990234375, 0.18994140625]
[0.9248046875, 0.15576171875]
[0.96630859375, 0.153564453125]
[0.9501953125, 0.1715087890625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.89990234375, 0.18994140625]
[0.9248046875, 0.15576171875]
[0.96630859375, 0.153564453125]
[0.9501953125, 0.1715087890625]
This is the real loss :  tensor(0.1941, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
243     1    NaN  ...  0.024384  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
244     1    NaN  ...  0.021167  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
245     1    NaN  ...  0.020184  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
246     1    NaN  ...  0.024494  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
247     1    NaN  ...  0.194069  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[248 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.88330078125, 0.0992431640625]
[0.90673828125, 0.115234375]
[0.884765625, 0.17919921875]
[0.91552734375, 0.15087890625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.88330078125, 0.0992431640625]
[0.90673828125, 0.115234375]
[0.884765625, 0.17919921875]
[0.91552734375, 0.15087890625]
This is the real loss :  tensor(0.3875, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
244     1    NaN  ...  0.021167  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
245     1    NaN  ...  0.020184  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
246     1    NaN  ...  0.024494  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
247     1    NaN  ...  0.194069  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
248     1    NaN  ...  0.387498  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[249 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.8427734375, 0.1072998046875]
[0.84326171875, 0.1148681640625]
[0.8095703125, 0.1558837890625]
[0.8525390625, 0.137939453125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8427734375, 0.1072998046875]
[0.84326171875, 0.1148681640625]
[0.8095703125, 0.1558837890625]
[0.8525390625, 0.137939453125]
This is the real loss :  tensor(0.2040, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
245     1    NaN  ...  0.020184  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
246     1    NaN  ...  0.024494  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
247     1    NaN  ...  0.194069  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
248     1    NaN  ...  0.387498  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
249     1    NaN  ...  0.204015  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[250 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[1.17578125, 0.04925537109375]
[0.88330078125, 0.2137451171875]
[0.82275390625, 0.155517578125]
[0.8037109375, 0.17626953125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[1.17578125, 0.04925537109375]
[0.88330078125, 0.2137451171875]
[0.82275390625, 0.155517578125]
[0.8037109375, 0.17626953125]
This is the real loss :  tensor(0.0272, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
246     1    NaN  ...  0.024494  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
247     1    NaN  ...  0.194069  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
248     1    NaN  ...  0.387498  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
249     1    NaN  ...  0.204015  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
250     1    NaN  ...  0.027229  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[251 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.8076171875, 0.2022705078125]
[0.80322265625, 0.1923828125]
[0.79345703125, 0.2025146484375]
[1.2265625, 0.0034542083740234375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8076171875, 0.2022705078125]
[0.80322265625, 0.1923828125]
[0.79345703125, 0.2025146484375]
[1.2265625, 0.0034542083740234375]
This is the real loss :  tensor(0.0361, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
247     1    NaN  ...  0.194069  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
248     1    NaN  ...  0.387498  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
249     1    NaN  ...  0.204015  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
250     1    NaN  ...  0.027229  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
251     1    NaN  ...  0.036084  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[252 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[1.015625, 0.1195068359375]
[0.837890625, 0.1988525390625]
[0.8388671875, 0.148193359375]
[1.078125, 0.06787109375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[1.015625, 0.1195068359375]
[0.837890625, 0.1988525390625]
[0.8388671875, 0.148193359375]
[1.078125, 0.06787109375]
This is the real loss :  tensor(0.5738, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
248     1    NaN  ...  0.387498  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
249     1    NaN  ...  0.204015  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
250     1    NaN  ...  0.027229  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
251     1    NaN  ...  0.036084  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
252     1    NaN  ...  0.573830  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[253 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.8251953125, 0.11004638671875]
[0.955078125, 0.01529693603515625]
[0.78271484375, 0.1995849609375]
[0.81201171875, 0.1380615234375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8251953125, 0.11004638671875]
[0.955078125, 0.01529693603515625]
[0.78271484375, 0.1995849609375]
[0.81201171875, 0.1380615234375]
This is the real loss :  tensor(0.3376, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
249     1    NaN  ...  0.204015  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
250     1    NaN  ...  0.027229  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
251     1    NaN  ...  0.036084  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
252     1    NaN  ...  0.573830  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
253     1    NaN  ...  0.337566  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[254 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.8134765625, 0.143310546875]
[1.0322265625, 0.1329345703125]
[0.9609375, 0.1630859375]
[0.796875, 0.219482421875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8134765625, 0.143310546875]
[1.0322265625, 0.1329345703125]
[0.9609375, 0.1630859375]
[0.796875, 0.219482421875]
This is the real loss :  tensor(0.0239, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
250     1    NaN  ...  0.027229  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
251     1    NaN  ...  0.036084  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
252     1    NaN  ...  0.573830  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
253     1    NaN  ...  0.337566  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
254     1    NaN  ...  0.023949  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[255 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.9150390625, 0.061309814453125]
[0.8095703125, 0.2047119140625]
[0.8818359375, 0.09600830078125]
[0.84521484375, 0.150390625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.9150390625, 0.061309814453125]
[0.8095703125, 0.2047119140625]
[0.8818359375, 0.09600830078125]
[0.84521484375, 0.150390625]
This is the real loss :  tensor(0.0199, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
251     1    NaN  ...  0.036084  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
252     1    NaN  ...  0.573830  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
253     1    NaN  ...  0.337566  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
254     1    NaN  ...  0.023949  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
255     1    NaN  ...  0.019863  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[256 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.91259765625, 0.1119384765625]
[0.77587890625, 0.1761474609375]
[0.83642578125, 0.1575927734375]
[0.80126953125, 0.212158203125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.91259765625, 0.1119384765625]
[0.77587890625, 0.1761474609375]
[0.83642578125, 0.1575927734375]
[0.80126953125, 0.212158203125]
This is the real loss :  tensor(0.3493, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
252     1    NaN  ...  0.573830  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
253     1    NaN  ...  0.337566  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
254     1    NaN  ...  0.023949  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
255     1    NaN  ...  0.019863  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
256     1    NaN  ...  0.349332  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]

[257 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.8056640625, 0.214111328125]
[0.79931640625, 0.1529541015625]
[0.958984375, 0.056243896484375]
[0.76708984375, 0.1666259765625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8056640625, 0.214111328125]
[0.79931640625, 0.1529541015625]
[0.958984375, 0.056243896484375]
[0.76708984375, 0.1666259765625]
This is the real loss :  tensor(0.3387, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
253     1    NaN  ...  0.337566  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
254     1    NaN  ...  0.023949  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
255     1    NaN  ...  0.019863  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
256     1    NaN  ...  0.349332  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
257     1    NaN  ...  0.338746  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[258 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.87060546875, 0.09228515625]
[0.7060546875, 0.288330078125]
[0.80126953125, 0.2327880859375]
[0.9287109375, 0.08050537109375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.87060546875, 0.09228515625]
[0.7060546875, 0.288330078125]
[0.80126953125, 0.2327880859375]
[0.9287109375, 0.08050537109375]
This is the real loss :  tensor(0.1419, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
254     1    NaN  ...  0.023949  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
255     1    NaN  ...  0.019863  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
256     1    NaN  ...  0.349332  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
257     1    NaN  ...  0.338746  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
258     1    NaN  ...  0.141937  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[259 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.84619140625, 0.13330078125]
[0.736328125, 0.2099609375]
[0.87841796875, 0.109130859375]
[0.755859375, 0.23876953125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.84619140625, 0.13330078125]
[0.736328125, 0.2099609375]
[0.87841796875, 0.109130859375]
[0.755859375, 0.23876953125]
This is the real loss :  tensor(0.0373, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
255     1    NaN  ...  0.019863  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
256     1    NaN  ...  0.349332  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
257     1    NaN  ...  0.338746  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
258     1    NaN  ...  0.141937  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
259     1    NaN  ...  0.037292  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[260 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.70263671875, 0.1661376953125]
[0.81396484375, 0.138916015625]
[0.76416015625, 0.2154541015625]
[0.76953125, 0.09625244140625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.70263671875, 0.1661376953125]
[0.81396484375, 0.138916015625]
[0.76416015625, 0.2154541015625]
[0.76953125, 0.09625244140625]
This is the real loss :  tensor(0.1790, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
256     1    NaN  ...  0.349332  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [1.0, 0.0]]
257     1    NaN  ...  0.338746  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
258     1    NaN  ...  0.141937  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
259     1    NaN  ...  0.037292  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
260     1    NaN  ...  0.178971  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[261 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.8779296875, 0.2041015625]
[0.75146484375, 0.180419921875]
[0.90966796875, 0.1746826171875]
[0.701171875, 0.1739501953125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8779296875, 0.2041015625]
[0.75146484375, 0.180419921875]
[0.90966796875, 0.1746826171875]
[0.701171875, 0.1739501953125]
This is the real loss :  tensor(0.2224, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
257     1    NaN  ...  0.338746  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
258     1    NaN  ...  0.141937  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
259     1    NaN  ...  0.037292  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
260     1    NaN  ...  0.178971  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
261     1    NaN  ...  0.222385  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[262 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.732421875, 0.20947265625]
[0.630859375, 0.31640625]
[0.91064453125, 0.0767822265625]
[0.93994140625, 0.037689208984375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.732421875, 0.20947265625]
[0.630859375, 0.31640625]
[0.91064453125, 0.0767822265625]
[0.93994140625, 0.037689208984375]
This is the real loss :  tensor(0.0463, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
258     1    NaN  ...  0.141937  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
259     1    NaN  ...  0.037292  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
260     1    NaN  ...  0.178971  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
261     1    NaN  ...  0.222385  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
262     1    NaN  ...  0.046345  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[263 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.6826171875, 0.252197265625]
[0.67822265625, 0.332763671875]
[0.767578125, 0.1700439453125]
[0.9228515625, 0.035858154296875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.6826171875, 0.252197265625]
[0.67822265625, 0.332763671875]
[0.767578125, 0.1700439453125]
[0.9228515625, 0.035858154296875]
This is the real loss :  tensor(0.1450, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
259     1    NaN  ...  0.037292  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
260     1    NaN  ...  0.178971  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
261     1    NaN  ...  0.222385  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
262     1    NaN  ...  0.046345  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
263     1    NaN  ...  0.144962  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[264 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.7177734375, 0.299072265625]
[0.853515625, 0.010528564453125]
[0.6953125, 0.22998046875]
[0.7138671875, 0.193603515625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7177734375, 0.299072265625]
[0.853515625, 0.010528564453125]
[0.6953125, 0.22998046875]
[0.7138671875, 0.193603515625]
This is the real loss :  tensor(0.2917, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
260     1    NaN  ...  0.178971  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
261     1    NaN  ...  0.222385  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
262     1    NaN  ...  0.046345  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
263     1    NaN  ...  0.144962  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
264     1    NaN  ...  0.291709  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[265 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.68115234375, 0.10003662109375]
[0.689453125, 0.1063232421875]
[0.6591796875, 0.0892333984375]
[0.744140625, 0.298095703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.68115234375, 0.10003662109375]
[0.689453125, 0.1063232421875]
[0.6591796875, 0.0892333984375]
[0.744140625, 0.298095703125]
This is the real loss :  tensor(0.0622, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
261     1    NaN  ...  0.222385  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
262     1    NaN  ...  0.046345  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
263     1    NaN  ...  0.144962  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
264     1    NaN  ...  0.291709  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
265     1    NaN  ...  0.062233  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[266 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.67236328125, 0.31884765625]
[0.6728515625, 0.283935546875]
[0.8671875, 0.08197021484375]
[0.90478515625, 0.0760498046875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.67236328125, 0.31884765625]
[0.6728515625, 0.283935546875]
[0.8671875, 0.08197021484375]
[0.90478515625, 0.0760498046875]
This is the real loss :  tensor(0.0545, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
262     1    NaN  ...  0.046345  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
263     1    NaN  ...  0.144962  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
264     1    NaN  ...  0.291709  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
265     1    NaN  ...  0.062233  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
266     1    NaN  ...  0.054483  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[267 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.79931640625, 0.14111328125]
[0.8203125, 0.2174072265625]
[0.765625, 0.2027587890625]
[0.67724609375, 0.278076171875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.79931640625, 0.14111328125]
[0.8203125, 0.2174072265625]
[0.765625, 0.2027587890625]
[0.67724609375, 0.278076171875]
This is the real loss :  tensor(0.2029, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
263     1    NaN  ...  0.144962  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
264     1    NaN  ...  0.291709  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
265     1    NaN  ...  0.062233  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
266     1    NaN  ...  0.054483  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
267     1    NaN  ...  0.202886  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[268 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.70166015625, 0.215576171875]
[0.689453125, 0.294677734375]
[0.9365234375, 0.0389404296875]
[0.70263671875, 0.19091796875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.70166015625, 0.215576171875]
[0.689453125, 0.294677734375]
[0.9365234375, 0.0389404296875]
[0.70263671875, 0.19091796875]
This is the real loss :  tensor(0.1777, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
264     1    NaN  ...  0.291709  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
265     1    NaN  ...  0.062233  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
266     1    NaN  ...  0.054483  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
267     1    NaN  ...  0.202886  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
268     1    NaN  ...  0.177668  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[269 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.6923828125, 0.07568359375]
[0.65087890625, 0.289306640625]
[0.59619140625, 0.1219482421875]
[0.5419921875, 0.1864013671875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.6923828125, 0.07568359375]
[0.65087890625, 0.289306640625]
[0.59619140625, 0.1219482421875]
[0.5419921875, 0.1864013671875]
This is the real loss :  tensor(0.3356, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
265     1    NaN  ...  0.062233  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
266     1    NaN  ...  0.054483  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
267     1    NaN  ...  0.202886  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
268     1    NaN  ...  0.177668  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
269     1    NaN  ...  0.335617  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[270 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.7822265625, 0.11181640625]
[0.69091796875, 0.14453125]
[0.69384765625, 0.19384765625]
[0.7294921875, 0.3603515625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7822265625, 0.11181640625]
[0.69091796875, 0.14453125]
[0.69384765625, 0.19384765625]
[0.7294921875, 0.3603515625]
This is the real loss :  tensor(0.0638, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
266     1    NaN  ...  0.054483  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
267     1    NaN  ...  0.202886  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
268     1    NaN  ...  0.177668  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
269     1    NaN  ...  0.335617  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
270     1    NaN  ...  0.063835  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[271 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.78857421875, 0.12347412109375]
[0.6943359375, 0.2235107421875]
[0.69482421875, 0.2381591796875]
[0.6201171875, 0.328857421875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.78857421875, 0.12347412109375]
[0.6943359375, 0.2235107421875]
[0.69482421875, 0.2381591796875]
[0.6201171875, 0.328857421875]
This is the real loss :  tensor(0.0757, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
267     1    NaN  ...  0.202886  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
268     1    NaN  ...  0.177668  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
269     1    NaN  ...  0.335617  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
270     1    NaN  ...  0.063835  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
271     1    NaN  ...  0.075706  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[272 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.748046875, 0.241943359375]
[0.8564453125, 0.07830810546875]
[0.6767578125, 0.284912109375]
[0.64208984375, 0.2247314453125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.748046875, 0.241943359375]
[0.8564453125, 0.07830810546875]
[0.6767578125, 0.284912109375]
[0.64208984375, 0.2247314453125]
This is the real loss :  tensor(0.0641, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
268     1    NaN  ...  0.177668  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
269     1    NaN  ...  0.335617  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
270     1    NaN  ...  0.063835  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
271     1    NaN  ...  0.075706  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
272     1    NaN  ...  0.064128  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[273 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.86328125, 0.10272216796875]
[0.78759765625, 0.1685791015625]
[0.64453125, 0.380126953125]
[0.76123046875, 0.194091796875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.86328125, 0.10272216796875]
[0.78759765625, 0.1685791015625]
[0.64453125, 0.380126953125]
[0.76123046875, 0.194091796875]
This is the real loss :  tensor(0.0585, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
269     1    NaN  ...  0.335617  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
270     1    NaN  ...  0.063835  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
271     1    NaN  ...  0.075706  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
272     1    NaN  ...  0.064128  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
273     1    NaN  ...  0.058539  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[274 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.76171875, 0.1668701171875]
[0.65625, 0.268310546875]
[0.71533203125, 0.3056640625]
[0.822265625, 0.179931640625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.76171875, 0.1668701171875]
[0.65625, 0.268310546875]
[0.71533203125, 0.3056640625]
[0.822265625, 0.179931640625]
This is the real loss :  tensor(0.1611, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
270     1    NaN  ...  0.063835  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
271     1    NaN  ...  0.075706  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
272     1    NaN  ...  0.064128  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
273     1    NaN  ...  0.058539  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
274     1    NaN  ...  0.161136  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[275 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.7841796875, 0.1875]
[0.74365234375, 0.20458984375]
[0.7705078125, 0.3212890625]
[0.88916015625, 0.1942138671875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7841796875, 0.1875]
[0.74365234375, 0.20458984375]
[0.7705078125, 0.3212890625]
[0.88916015625, 0.1942138671875]
This is the real loss :  tensor(0.0494, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
271     1    NaN  ...  0.075706  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
272     1    NaN  ...  0.064128  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
273     1    NaN  ...  0.058539  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
274     1    NaN  ...  0.161136  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
275     1    NaN  ...  0.049400  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[276 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.66650390625, 0.332275390625]
[0.748046875, 0.2281494140625]
[0.86376953125, 0.2200927734375]
[0.9921875, 0.0938720703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.66650390625, 0.332275390625]
[0.748046875, 0.2281494140625]
[0.86376953125, 0.2200927734375]
[0.9921875, 0.0938720703125]
This is the real loss :  tensor(0.1816, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
272     1    NaN  ...  0.064128  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
273     1    NaN  ...  0.058539  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
274     1    NaN  ...  0.161136  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
275     1    NaN  ...  0.049400  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
276     1    NaN  ...  0.181603  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[277 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.71484375, 0.174072265625]
[0.7138671875, 0.246826171875]
[0.7392578125, 0.29345703125]
[0.919921875, 0.06884765625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.71484375, 0.174072265625]
[0.7138671875, 0.246826171875]
[0.7392578125, 0.29345703125]
[0.919921875, 0.06884765625]
This is the real loss :  tensor(0.0525, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
273     1    NaN  ...  0.058539  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
274     1    NaN  ...  0.161136  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
275     1    NaN  ...  0.049400  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
276     1    NaN  ...  0.181603  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
277     1    NaN  ...  0.052458  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[278 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.92236328125, 0.0965576171875]
[0.78076171875, 0.3525390625]
[0.7783203125, 0.2001953125]
[0.74072265625, 0.1829833984375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.92236328125, 0.0965576171875]
[0.78076171875, 0.3525390625]
[0.7783203125, 0.2001953125]
[0.74072265625, 0.1829833984375]
This is the real loss :  tensor(0.0472, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
274     1    NaN  ...  0.161136  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
275     1    NaN  ...  0.049400  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
276     1    NaN  ...  0.181603  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
277     1    NaN  ...  0.052458  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
278     1    NaN  ...  0.047203  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[279 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.78662109375, 0.146240234375]
[0.83935546875, 0.149658203125]
[0.8232421875, 0.107421875]
[0.693359375, 0.255615234375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.78662109375, 0.146240234375]
[0.83935546875, 0.149658203125]
[0.8232421875, 0.107421875]
[0.693359375, 0.255615234375]
This is the real loss :  tensor(0.2121, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
275     1    NaN  ...  0.049400  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
276     1    NaN  ...  0.181603  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
277     1    NaN  ...  0.052458  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
278     1    NaN  ...  0.047203  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
279     1    NaN  ...  0.212083  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[280 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.93896484375, 0.1153564453125]
[0.78515625, 0.207275390625]
[0.8642578125, 0.290283203125]
[0.79541015625, 0.13720703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.93896484375, 0.1153564453125]
[0.78515625, 0.207275390625]
[0.8642578125, 0.290283203125]
[0.79541015625, 0.13720703125]
This is the real loss :  tensor(0.0337, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
276     1    NaN  ...  0.181603  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
277     1    NaN  ...  0.052458  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
278     1    NaN  ...  0.047203  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
279     1    NaN  ...  0.212083  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
280     1    NaN  ...  0.033691  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[281 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.86083984375, 0.1357421875]
[0.77783203125, 0.22607421875]
[0.890625, 0.207275390625]
[0.75146484375, 0.2276611328125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.86083984375, 0.1357421875]
[0.77783203125, 0.22607421875]
[0.890625, 0.207275390625]
[0.75146484375, 0.2276611328125]
This is the real loss :  tensor(0.3401, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
277     1    NaN  ...  0.052458  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
278     1    NaN  ...  0.047203  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
279     1    NaN  ...  0.212083  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
280     1    NaN  ...  0.033691  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
281     1    NaN  ...  0.340136  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[282 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.77197265625, 0.1893310546875]
[0.84814453125, 0.2021484375]
[0.935546875, 0.18017578125]
[0.90869140625, 0.170166015625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.77197265625, 0.1893310546875]
[0.84814453125, 0.2021484375]
[0.935546875, 0.18017578125]
[0.90869140625, 0.170166015625]
This is the real loss :  tensor(0.1897, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
278     1    NaN  ...  0.047203  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
279     1    NaN  ...  0.212083  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
280     1    NaN  ...  0.033691  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
281     1    NaN  ...  0.340136  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
282     1    NaN  ...  0.189709  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[283 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.736328125, 0.1978759765625]
[0.7666015625, 0.144775390625]
[0.884765625, 0.1844482421875]
[0.8203125, 0.1474609375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.736328125, 0.1978759765625]
[0.7666015625, 0.144775390625]
[0.884765625, 0.1844482421875]
[0.8203125, 0.1474609375]
This is the real loss :  tensor(0.0357, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
279     1    NaN  ...  0.212083  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
280     1    NaN  ...  0.033691  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
281     1    NaN  ...  0.340136  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
282     1    NaN  ...  0.189709  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
283     1    NaN  ...  0.035681  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[284 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.80517578125, 0.158203125]
[0.81787109375, 0.26806640625]
[0.89013671875, 0.172119140625]
[0.8408203125, 0.17578125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.80517578125, 0.158203125]
[0.81787109375, 0.26806640625]
[0.89013671875, 0.172119140625]
[0.8408203125, 0.17578125]
This is the real loss :  tensor(0.2127, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
280     1    NaN  ...  0.033691  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
281     1    NaN  ...  0.340136  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
282     1    NaN  ...  0.189709  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
283     1    NaN  ...  0.035681  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
284     1    NaN  ...  0.212748  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[285 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.87744140625, 0.2205810546875]
[0.9462890625, 0.1298828125]
[0.8720703125, 0.158203125]
[0.75390625, 0.177978515625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.87744140625, 0.2205810546875]
[0.9462890625, 0.1298828125]
[0.8720703125, 0.158203125]
[0.75390625, 0.177978515625]
This is the real loss :  tensor(0.0271, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
281     1    NaN  ...  0.340136  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
282     1    NaN  ...  0.189709  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
283     1    NaN  ...  0.035681  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
284     1    NaN  ...  0.212748  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
285     1    NaN  ...  0.027133  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[286 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.89892578125, 0.0335693359375]
[0.69775390625, 0.1795654296875]
[0.79638671875, 0.2171630859375]
[0.78759765625, 0.1705322265625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.89892578125, 0.0335693359375]
[0.69775390625, 0.1795654296875]
[0.79638671875, 0.2171630859375]
[0.78759765625, 0.1705322265625]
This is the real loss :  tensor(0.4658, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
282     1    NaN  ...  0.189709  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
283     1    NaN  ...  0.035681  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
284     1    NaN  ...  0.212748  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
285     1    NaN  ...  0.027133  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
286     1    NaN  ...  0.465839  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[287 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.78125, 0.279296875]
[1.1005859375, 0.061981201171875]
[0.7998046875, 0.213134765625]
[0.76513671875, 0.266357421875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.78125, 0.279296875]
[1.1005859375, 0.061981201171875]
[0.7998046875, 0.213134765625]
[0.76513671875, 0.266357421875]
This is the real loss :  tensor(0.0439, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
283     1    NaN  ...  0.035681  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
284     1    NaN  ...  0.212748  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
285     1    NaN  ...  0.027133  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
286     1    NaN  ...  0.465839  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
287     1    NaN  ...  0.043929  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[288 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.8310546875, 0.140380859375]
[0.98486328125, 0.042236328125]
[0.76416015625, 0.26220703125]
[0.77392578125, 0.226318359375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8310546875, 0.140380859375]
[0.98486328125, 0.042236328125]
[0.76416015625, 0.26220703125]
[0.77392578125, 0.226318359375]
This is the real loss :  tensor(0.0346, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
284     1    NaN  ...  0.212748  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
285     1    NaN  ...  0.027133  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
286     1    NaN  ...  0.465839  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
287     1    NaN  ...  0.043929  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
288     1    NaN  ...  0.034621  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[289 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.8369140625, 0.303466796875]
[0.791015625, 0.1904296875]
[0.982421875, 0.03680419921875]
[0.72802734375, 0.15478515625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8369140625, 0.303466796875]
[0.791015625, 0.1904296875]
[0.982421875, 0.03680419921875]
[0.72802734375, 0.15478515625]
This is the real loss :  tensor(0.4170, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
285     1    NaN  ...  0.027133  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
286     1    NaN  ...  0.465839  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
287     1    NaN  ...  0.043929  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
288     1    NaN  ...  0.034621  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
289     1    NaN  ...  0.416992  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[290 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.71435546875, 0.249755859375]
[0.7490234375, 0.266357421875]
[0.9697265625, 0.10943603515625]
[0.86474609375, 0.1331787109375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.71435546875, 0.249755859375]
[0.7490234375, 0.266357421875]
[0.9697265625, 0.10943603515625]
[0.86474609375, 0.1331787109375]
This is the real loss :  tensor(0.0409, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
286     1    NaN  ...  0.465839  [[1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
287     1    NaN  ...  0.043929  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
288     1    NaN  ...  0.034621  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
289     1    NaN  ...  0.416992  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
290     1    NaN  ...  0.040854  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[291 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.9287109375, 0.15771484375]
[0.74755859375, 0.2393798828125]
[0.8623046875, 0.1650390625]
[0.88720703125, 0.10748291015625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.9287109375, 0.15771484375]
[0.74755859375, 0.2393798828125]
[0.8623046875, 0.1650390625]
[0.88720703125, 0.10748291015625]
This is the real loss :  tensor(0.2020, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
287     1    NaN  ...  0.043929  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
288     1    NaN  ...  0.034621  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
289     1    NaN  ...  0.416992  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
290     1    NaN  ...  0.040854  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
291     1    NaN  ...  0.201999  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[292 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.86376953125, 0.1741943359375]
[0.69482421875, 0.1998291015625]
[0.794921875, 0.1055908203125]
[0.7880859375, 0.14990234375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.86376953125, 0.1741943359375]
[0.69482421875, 0.1998291015625]
[0.794921875, 0.1055908203125]
[0.7880859375, 0.14990234375]
This is the real loss :  tensor(0.0378, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
288     1    NaN  ...  0.034621  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
289     1    NaN  ...  0.416992  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
290     1    NaN  ...  0.040854  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
291     1    NaN  ...  0.201999  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
292     1    NaN  ...  0.037819  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[293 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.87353515625, 0.1409912109375]
[0.87646484375, 0.186279296875]
[0.837890625, 0.1982421875]
[0.68310546875, 0.208251953125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.87353515625, 0.1409912109375]
[0.87646484375, 0.186279296875]
[0.837890625, 0.1982421875]
[0.68310546875, 0.208251953125]
This is the real loss :  tensor(0.0369, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
289     1    NaN  ...  0.416992  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
290     1    NaN  ...  0.040854  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
291     1    NaN  ...  0.201999  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
292     1    NaN  ...  0.037819  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
293     1    NaN  ...  0.036900  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[294 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.97314453125, 0.19140625]
[0.7548828125, 0.220703125]
[0.830078125, 0.208740234375]
[0.833984375, 0.15625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.97314453125, 0.19140625]
[0.7548828125, 0.220703125]
[0.830078125, 0.208740234375]
[0.833984375, 0.15625]
This is the real loss :  tensor(0.2033, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
290     1    NaN  ...  0.040854  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
291     1    NaN  ...  0.201999  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
292     1    NaN  ...  0.037819  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
293     1    NaN  ...  0.036900  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
294     1    NaN  ...  0.203255  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[295 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.82421875, 0.11700439453125]
[0.84033203125, 0.2015380859375]
[0.79736328125, 0.2127685546875]
[0.85791015625, 0.1273193359375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.82421875, 0.11700439453125]
[0.84033203125, 0.2015380859375]
[0.79736328125, 0.2127685546875]
[0.85791015625, 0.1273193359375]
This is the real loss :  tensor(0.0292, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
291     1    NaN  ...  0.201999  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
292     1    NaN  ...  0.037819  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
293     1    NaN  ...  0.036900  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
294     1    NaN  ...  0.203255  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
295     1    NaN  ...  0.029179  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[296 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.9228515625, 0.12286376953125]
[0.76220703125, 0.267333984375]
[0.87451171875, 0.1124267578125]
[0.81787109375, 0.17529296875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.9228515625, 0.12286376953125]
[0.76220703125, 0.267333984375]
[0.87451171875, 0.1124267578125]
[0.81787109375, 0.17529296875]
This is the real loss :  tensor(0.1539, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
292     1    NaN  ...  0.037819  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
293     1    NaN  ...  0.036900  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
294     1    NaN  ...  0.203255  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
295     1    NaN  ...  0.029179  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
296     1    NaN  ...  0.153886  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[297 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.8955078125, 0.1534423828125]
[0.90625, 0.158935546875]
[0.8828125, 0.17578125]
[0.85546875, 0.2257080078125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8955078125, 0.1534423828125]
[0.90625, 0.158935546875]
[0.8828125, 0.17578125]
[0.85546875, 0.2257080078125]
This is the real loss :  tensor(0.3661, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
293     1    NaN  ...  0.036900  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
294     1    NaN  ...  0.203255  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
295     1    NaN  ...  0.029179  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
296     1    NaN  ...  0.153886  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
297     1    NaN  ...  0.366079  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[298 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.69970703125, 0.27587890625]
[0.79638671875, 0.1895751953125]
[0.90576171875, 0.1341552734375]
[1.0390625, 0.1302490234375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.69970703125, 0.27587890625]
[0.79638671875, 0.1895751953125]
[0.90576171875, 0.1341552734375]
[1.0390625, 0.1302490234375]
This is the real loss :  tensor(0.0361, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
294     1    NaN  ...  0.203255  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
295     1    NaN  ...  0.029179  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
296     1    NaN  ...  0.153886  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
297     1    NaN  ...  0.366079  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
298     1    NaN  ...  0.036131  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[299 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.70068359375, 0.1732177734375]
[0.94189453125, 0.2354736328125]
[0.71630859375, 0.1851806640625]
[0.91259765625, 0.0201263427734375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.70068359375, 0.1732177734375]
[0.94189453125, 0.2354736328125]
[0.71630859375, 0.1851806640625]
[0.91259765625, 0.0201263427734375]
This is the real loss :  tensor(0.1695, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
295     1    NaN  ...  0.029179  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
296     1    NaN  ...  0.153886  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
297     1    NaN  ...  0.366079  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
298     1    NaN  ...  0.036131  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
299     1    NaN  ...  0.169521  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[300 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.71240234375, 0.1871337890625]
[0.81201171875, 0.22900390625]
[0.98828125, 0.07427978515625]
[0.81982421875, 0.09674072265625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.71240234375, 0.1871337890625]
[0.81201171875, 0.22900390625]
[0.98828125, 0.07427978515625]
[0.81982421875, 0.09674072265625]
This is the real loss :  tensor(0.0316, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
296     1    NaN  ...  0.153886  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
297     1    NaN  ...  0.366079  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
298     1    NaN  ...  0.036131  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
299     1    NaN  ...  0.169521  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
300     1    NaN  ...  0.031624  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[301 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.9130859375, 0.0980224609375]
[0.904296875, 0.2020263671875]
[0.90966796875, 0.125]
[0.80126953125, 0.277099609375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.9130859375, 0.0980224609375]
[0.904296875, 0.2020263671875]
[0.90966796875, 0.125]
[0.80126953125, 0.277099609375]
This is the real loss :  tensor(0.0259, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
297     1    NaN  ...  0.366079  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
298     1    NaN  ...  0.036131  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
299     1    NaN  ...  0.169521  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
300     1    NaN  ...  0.031624  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
301     1    NaN  ...  0.025900  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[302 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.76416015625, 0.1373291015625]
[0.8955078125, 0.1495361328125]
[0.8857421875, 0.0849609375]
[0.70361328125, 0.2783203125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.76416015625, 0.1373291015625]
[0.8955078125, 0.1495361328125]
[0.8857421875, 0.0849609375]
[0.70361328125, 0.2783203125]
This is the real loss :  tensor(0.1934, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
298     1    NaN  ...  0.036131  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
299     1    NaN  ...  0.169521  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
300     1    NaN  ...  0.031624  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
301     1    NaN  ...  0.025900  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
302     1    NaN  ...  0.193375  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[303 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.85205078125, 0.1513671875]
[0.83984375, 0.179443359375]
[0.91259765625, 0.11907958984375]
[0.8193359375, 0.299072265625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.85205078125, 0.1513671875]
[0.83984375, 0.179443359375]
[0.91259765625, 0.11907958984375]
[0.8193359375, 0.299072265625]
This is the real loss :  tensor(0.0308, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
299     1    NaN  ...  0.169521  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
300     1    NaN  ...  0.031624  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
301     1    NaN  ...  0.025900  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
302     1    NaN  ...  0.193375  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
303     1    NaN  ...  0.030819  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[304 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.70849609375, 0.12213134765625]
[0.6748046875, 0.30322265625]
[0.74609375, 0.08648681640625]
[0.84716796875, 0.11114501953125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.70849609375, 0.12213134765625]
[0.6748046875, 0.30322265625]
[0.74609375, 0.08648681640625]
[0.84716796875, 0.11114501953125]
This is the real loss :  tensor(0.2901, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
300     1    NaN  ...  0.031624  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
301     1    NaN  ...  0.025900  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
302     1    NaN  ...  0.193375  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
303     1    NaN  ...  0.030819  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
304     1    NaN  ...  0.290142  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[305 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.888671875, 0.188232421875]
[0.89697265625, 0.133544921875]
[0.8173828125, 0.1376953125]
[0.77880859375, 0.237548828125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.888671875, 0.188232421875]
[0.89697265625, 0.133544921875]
[0.8173828125, 0.1376953125]
[0.77880859375, 0.237548828125]
This is the real loss :  tensor(0.0292, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
301     1    NaN  ...  0.025900  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
302     1    NaN  ...  0.193375  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
303     1    NaN  ...  0.030819  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
304     1    NaN  ...  0.290142  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
305     1    NaN  ...  0.029242  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[306 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.71044921875, 0.1322021484375]
[0.69287109375, 0.1551513671875]
[0.82421875, 0.111083984375]
[0.82568359375, 0.1932373046875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.71044921875, 0.1322021484375]
[0.69287109375, 0.1551513671875]
[0.82421875, 0.111083984375]
[0.82568359375, 0.1932373046875]
This is the real loss :  tensor(0.1994, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
302     1    NaN  ...  0.193375  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
303     1    NaN  ...  0.030819  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
304     1    NaN  ...  0.290142  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
305     1    NaN  ...  0.029242  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
306     1    NaN  ...  0.199447  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[307 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.8701171875, 0.139404296875]
[0.83154296875, 0.14404296875]
[0.70947265625, 0.245361328125]
[0.896484375, 0.06146240234375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8701171875, 0.139404296875]
[0.83154296875, 0.14404296875]
[0.70947265625, 0.245361328125]
[0.896484375, 0.06146240234375]
This is the real loss :  tensor(0.0306, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
303     1    NaN  ...  0.030819  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
304     1    NaN  ...  0.290142  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
305     1    NaN  ...  0.029242  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
306     1    NaN  ...  0.199447  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
307     1    NaN  ...  0.030566  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[308 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.86474609375, 0.136962890625]
[0.87353515625, 0.2020263671875]
[0.91259765625, 0.073974609375]
[0.72900390625, 0.2381591796875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.86474609375, 0.136962890625]
[0.87353515625, 0.2020263671875]
[0.91259765625, 0.073974609375]
[0.72900390625, 0.2381591796875]
This is the real loss :  tensor(0.1975, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
304     1    NaN  ...  0.290142  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
305     1    NaN  ...  0.029242  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
306     1    NaN  ...  0.199447  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
307     1    NaN  ...  0.030566  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
308     1    NaN  ...  0.197519  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[309 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.93359375, 0.0278167724609375]
[0.6572265625, 0.121337890625]
[0.791015625, 0.075439453125]
[0.73193359375, 0.252685546875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.93359375, 0.0278167724609375]
[0.6572265625, 0.121337890625]
[0.791015625, 0.075439453125]
[0.73193359375, 0.252685546875]
This is the real loss :  tensor(0.1743, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
305     1    NaN  ...  0.029242  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
306     1    NaN  ...  0.199447  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
307     1    NaN  ...  0.030566  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
308     1    NaN  ...  0.197519  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
309     1    NaN  ...  0.174282  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[310 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.83544921875, 0.249267578125]
[0.703125, 0.2071533203125]
[0.888671875, 0.0660400390625]
[0.890625, 0.1329345703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.83544921875, 0.249267578125]
[0.703125, 0.2071533203125]
[0.888671875, 0.0660400390625]
[0.890625, 0.1329345703125]
This is the real loss :  tensor(0.3039, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
306     1    NaN  ...  0.199447  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
307     1    NaN  ...  0.030566  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
308     1    NaN  ...  0.197519  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
309     1    NaN  ...  0.174282  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
310     1    NaN  ...  0.303869  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[311 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.8798828125, 0.062225341796875]
[0.8330078125, 0.210205078125]
[0.794921875, 0.1307373046875]
[0.7080078125, 0.1964111328125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8798828125, 0.062225341796875]
[0.8330078125, 0.210205078125]
[0.794921875, 0.1307373046875]
[0.7080078125, 0.1964111328125]
This is the real loss :  tensor(0.1899, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
307     1    NaN  ...  0.030566  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
308     1    NaN  ...  0.197519  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
309     1    NaN  ...  0.174282  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
310     1    NaN  ...  0.303869  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
311     1    NaN  ...  0.189871  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[312 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.904296875, 0.14013671875]
[0.9521484375, 0.08349609375]
[0.72705078125, 0.1990966796875]
[0.712890625, 0.290771484375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.904296875, 0.14013671875]
[0.9521484375, 0.08349609375]
[0.72705078125, 0.1990966796875]
[0.712890625, 0.290771484375]
This is the real loss :  tensor(0.4481, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
308     1    NaN  ...  0.197519  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
309     1    NaN  ...  0.174282  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
310     1    NaN  ...  0.303869  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
311     1    NaN  ...  0.189871  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
312     1    NaN  ...  0.448101  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[313 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.84423828125, 0.1834716796875]
[0.87890625, 0.1949462890625]
[0.72314453125, 0.29443359375]
[0.96484375, 0.0947265625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.84423828125, 0.1834716796875]
[0.87890625, 0.1949462890625]
[0.72314453125, 0.29443359375]
[0.96484375, 0.0947265625]
This is the real loss :  tensor(0.2007, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
309     1    NaN  ...  0.174282  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
310     1    NaN  ...  0.303869  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
311     1    NaN  ...  0.189871  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
312     1    NaN  ...  0.448101  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
313     1    NaN  ...  0.200709  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[314 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.80517578125, 0.13671875]
[0.86572265625, 0.03887939453125]
[0.72119140625, 0.2471923828125]
[0.7001953125, 0.24267578125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.80517578125, 0.13671875]
[0.86572265625, 0.03887939453125]
[0.72119140625, 0.2471923828125]
[0.7001953125, 0.24267578125]
This is the real loss :  tensor(0.0455, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
310     1    NaN  ...  0.303869  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
311     1    NaN  ...  0.189871  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
312     1    NaN  ...  0.448101  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
313     1    NaN  ...  0.200709  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
314     1    NaN  ...  0.045475  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[315 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.89697265625, 0.09759521484375]
[0.7275390625, 0.1368408203125]
[0.75634765625, 0.185302734375]
[0.74951171875, 0.160400390625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.89697265625, 0.09759521484375]
[0.7275390625, 0.1368408203125]
[0.75634765625, 0.185302734375]
[0.74951171875, 0.160400390625]
This is the real loss :  tensor(0.0369, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
311     1    NaN  ...  0.189871  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
312     1    NaN  ...  0.448101  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
313     1    NaN  ...  0.200709  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
314     1    NaN  ...  0.045475  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
315     1    NaN  ...  0.036910  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[316 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.95458984375, 0.052886962890625]
[0.6923828125, 0.3623046875]
[0.66943359375, 0.181640625]
[0.83447265625, 0.1419677734375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.95458984375, 0.052886962890625]
[0.6923828125, 0.3623046875]
[0.66943359375, 0.181640625]
[0.83447265625, 0.1419677734375]
This is the real loss :  tensor(0.1351, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
312     1    NaN  ...  0.448101  [[0.0, 1.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
313     1    NaN  ...  0.200709  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
314     1    NaN  ...  0.045475  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
315     1    NaN  ...  0.036910  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
316     1    NaN  ...  0.135091  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[317 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.7822265625, 0.1259765625]
[0.693359375, 0.319091796875]
[0.75634765625, 0.1695556640625]
[0.73388671875, 0.1614990234375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7822265625, 0.1259765625]
[0.693359375, 0.319091796875]
[0.75634765625, 0.1695556640625]
[0.73388671875, 0.1614990234375]
This is the real loss :  tensor(0.2196, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
313     1    NaN  ...  0.200709  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
314     1    NaN  ...  0.045475  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
315     1    NaN  ...  0.036910  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
316     1    NaN  ...  0.135091  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
317     1    NaN  ...  0.219582  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[318 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.7177734375, 0.29638671875]
[0.7041015625, 0.246337890625]
[0.85888671875, 0.1392822265625]
[0.87548828125, 0.181396484375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7177734375, 0.29638671875]
[0.7041015625, 0.246337890625]
[0.85888671875, 0.1392822265625]
[0.87548828125, 0.181396484375]
This is the real loss :  tensor(0.1649, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
314     1    NaN  ...  0.045475  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
315     1    NaN  ...  0.036910  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
316     1    NaN  ...  0.135091  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
317     1    NaN  ...  0.219582  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
318     1    NaN  ...  0.164873  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[319 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.7724609375, 0.270751953125]
[0.7265625, 0.1923828125]
[0.73388671875, 0.2330322265625]
[0.7841796875, 0.144287109375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7724609375, 0.270751953125]
[0.7265625, 0.1923828125]
[0.73388671875, 0.2330322265625]
[0.7841796875, 0.144287109375]
This is the real loss :  tensor(0.0537, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
315     1    NaN  ...  0.036910  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
316     1    NaN  ...  0.135091  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
317     1    NaN  ...  0.219582  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
318     1    NaN  ...  0.164873  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
319     1    NaN  ...  0.053672  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[320 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.7177734375, 0.2120361328125]
[0.673828125, 0.298828125]
[0.67529296875, 0.2161865234375]
[0.833984375, 0.07330322265625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7177734375, 0.2120361328125]
[0.673828125, 0.298828125]
[0.67529296875, 0.2161865234375]
[0.833984375, 0.07330322265625]
This is the real loss :  tensor(0.1569, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
316     1    NaN  ...  0.135091  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
317     1    NaN  ...  0.219582  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
318     1    NaN  ...  0.164873  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
319     1    NaN  ...  0.053672  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
320     1    NaN  ...  0.156925  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[321 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.73681640625, 0.31298828125]
[0.84423828125, 0.1732177734375]
[0.76806640625, 0.1492919921875]
[0.73876953125, 0.25830078125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.73681640625, 0.31298828125]
[0.84423828125, 0.1732177734375]
[0.76806640625, 0.1492919921875]
[0.73876953125, 0.25830078125]
This is the real loss :  tensor(0.0541, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
317     1    NaN  ...  0.219582  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
318     1    NaN  ...  0.164873  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
319     1    NaN  ...  0.053672  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
320     1    NaN  ...  0.156925  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
321     1    NaN  ...  0.054067  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[322 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.69873046875, 0.3359375]
[0.7685546875, 0.1427001953125]
[0.853515625, 0.133544921875]
[0.84716796875, 0.1441650390625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.69873046875, 0.3359375]
[0.7685546875, 0.1427001953125]
[0.853515625, 0.133544921875]
[0.84716796875, 0.1441650390625]
This is the real loss :  tensor(0.0451, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
318     1    NaN  ...  0.164873  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
319     1    NaN  ...  0.053672  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
320     1    NaN  ...  0.156925  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
321     1    NaN  ...  0.054067  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
322     1    NaN  ...  0.045123  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[323 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.935546875, 0.0247802734375]
[0.7470703125, 0.19140625]
[0.771484375, 0.261962890625]
[0.7265625, 0.312255859375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.935546875, 0.0247802734375]
[0.7470703125, 0.19140625]
[0.771484375, 0.261962890625]
[0.7265625, 0.312255859375]
This is the real loss :  tensor(0.2923, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
319     1    NaN  ...  0.053672  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
320     1    NaN  ...  0.156925  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
321     1    NaN  ...  0.054067  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
322     1    NaN  ...  0.045123  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
323     1    NaN  ...  0.292304  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]

[324 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.88720703125, 0.109130859375]
[0.7001953125, 0.2412109375]
[0.79638671875, 0.2376708984375]
[0.7734375, 0.2423095703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.88720703125, 0.109130859375]
[0.7001953125, 0.2412109375]
[0.79638671875, 0.2376708984375]
[0.7734375, 0.2423095703125]
This is the real loss :  tensor(0.0476, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
320     1    NaN  ...  0.156925  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
321     1    NaN  ...  0.054067  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
322     1    NaN  ...  0.045123  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
323     1    NaN  ...  0.292304  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
324     1    NaN  ...  0.047586  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[325 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.90283203125, 0.059295654296875]
[0.74609375, 0.199462890625]
[0.84130859375, 0.2607421875]
[0.84130859375, 0.173828125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.90283203125, 0.059295654296875]
[0.74609375, 0.199462890625]
[0.84130859375, 0.2607421875]
[0.84130859375, 0.173828125]
This is the real loss :  tensor(0.1699, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
321     1    NaN  ...  0.054067  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
322     1    NaN  ...  0.045123  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
323     1    NaN  ...  0.292304  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
324     1    NaN  ...  0.047586  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
325     1    NaN  ...  0.169880  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[326 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.9482421875, 0.114013671875]
[0.8232421875, 0.1712646484375]
[0.8076171875, 0.273681640625]
[0.7216796875, 0.169189453125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.9482421875, 0.114013671875]
[0.8232421875, 0.1712646484375]
[0.8076171875, 0.273681640625]
[0.7216796875, 0.169189453125]
This is the real loss :  tensor(0.0368, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
322     1    NaN  ...  0.045123  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
323     1    NaN  ...  0.292304  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
324     1    NaN  ...  0.047586  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
325     1    NaN  ...  0.169880  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
326     1    NaN  ...  0.036782  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[327 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.77392578125, 0.378173828125]
[0.861328125, 0.1055908203125]
[0.84228515625, 0.07757568359375]
[0.6640625, 0.220458984375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.77392578125, 0.378173828125]
[0.861328125, 0.1055908203125]
[0.84228515625, 0.07757568359375]
[0.6640625, 0.220458984375]
This is the real loss :  tensor(0.6421, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
323     1    NaN  ...  0.292304  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]]
324     1    NaN  ...  0.047586  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
325     1    NaN  ...  0.169880  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
326     1    NaN  ...  0.036782  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
327     1    NaN  ...  0.642057  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]

[328 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.67333984375, 0.147705078125]
[0.7265625, 0.107421875]
[0.84814453125, 0.1295166015625]
[0.6689453125, 0.32275390625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.67333984375, 0.147705078125]
[0.7265625, 0.107421875]
[0.84814453125, 0.1295166015625]
[0.6689453125, 0.32275390625]
This is the real loss :  tensor(0.0586, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
324     1    NaN  ...  0.047586  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
325     1    NaN  ...  0.169880  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
326     1    NaN  ...  0.036782  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
327     1    NaN  ...  0.642057  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
328     1    NaN  ...  0.058554  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[329 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.66259765625, 0.251708984375]
[0.9990234375, -0.04327392578125]
[0.7412109375, 0.287841796875]
[0.72119140625, 0.29052734375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.66259765625, 0.251708984375]
[0.9990234375, -0.04327392578125]
[0.7412109375, 0.287841796875]
[0.72119140625, 0.29052734375]
This is the real loss :  tensor(0.1747, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
325     1    NaN  ...  0.169880  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
326     1    NaN  ...  0.036782  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
327     1    NaN  ...  0.642057  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
328     1    NaN  ...  0.058554  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
329     1    NaN  ...  0.174722  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[330 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.73828125, 0.1802978515625]
[0.96337890625, -0.0009765625]
[0.68359375, 0.228515625]
[0.716796875, 0.3623046875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.73828125, 0.1802978515625]
[0.96337890625, -0.0009765625]
[0.68359375, 0.228515625]
[0.716796875, 0.3623046875]
This is the real loss :  tensor(0.1469, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
326     1    NaN  ...  0.036782  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
327     1    NaN  ...  0.642057  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
328     1    NaN  ...  0.058554  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
329     1    NaN  ...  0.174722  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
330     1    NaN  ...  0.146891  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[331 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.6806640625, 0.354736328125]
[0.744140625, 0.2493896484375]
[0.96875, 0.1041259765625]
[0.7314453125, 0.1968994140625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.6806640625, 0.354736328125]
[0.744140625, 0.2493896484375]
[0.96875, 0.1041259765625]
[0.7314453125, 0.1968994140625]
This is the real loss :  tensor(0.1934, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
327     1    NaN  ...  0.642057  [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]
328     1    NaN  ...  0.058554  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
329     1    NaN  ...  0.174722  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
330     1    NaN  ...  0.146891  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
331     1    NaN  ...  0.193409  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[332 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.8388671875, 0.1417236328125]
[0.72802734375, 0.34375]
[0.8173828125, 0.1719970703125]
[0.76953125, 0.2216796875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8388671875, 0.1417236328125]
[0.72802734375, 0.34375]
[0.8173828125, 0.1719970703125]
[0.76953125, 0.2216796875]
This is the real loss :  tensor(0.0504, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
328     1    NaN  ...  0.058554  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
329     1    NaN  ...  0.174722  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
330     1    NaN  ...  0.146891  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
331     1    NaN  ...  0.193409  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
332     1    NaN  ...  0.050422  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[333 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.705078125, 0.2783203125]
[0.81103515625, 0.2127685546875]
[0.88134765625, 0.099609375]
[0.74560546875, 0.31298828125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.705078125, 0.2783203125]
[0.81103515625, 0.2127685546875]
[0.88134765625, 0.099609375]
[0.74560546875, 0.31298828125]
This is the real loss :  tensor(0.3561, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
329     1    NaN  ...  0.174722  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
330     1    NaN  ...  0.146891  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
331     1    NaN  ...  0.193409  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
332     1    NaN  ...  0.050422  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
333     1    NaN  ...  0.356136  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[334 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.7021484375, 0.295654296875]
[0.689453125, 0.391845703125]
[0.98291015625, 0.02825927734375]
[0.8642578125, 0.09686279296875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7021484375, 0.295654296875]
[0.689453125, 0.391845703125]
[0.98291015625, 0.02825927734375]
[0.8642578125, 0.09686279296875]
This is the real loss :  tensor(0.0569, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
330     1    NaN  ...  0.146891  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
331     1    NaN  ...  0.193409  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
332     1    NaN  ...  0.050422  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
333     1    NaN  ...  0.356136  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
334     1    NaN  ...  0.056876  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[335 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.7216796875, 0.298583984375]
[0.81640625, 0.2305908203125]
[0.787109375, 0.2529296875]
[0.88330078125, 0.063720703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7216796875, 0.298583984375]
[0.81640625, 0.2305908203125]
[0.787109375, 0.2529296875]
[0.88330078125, 0.063720703125]
This is the real loss :  tensor(0.0476, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
331     1    NaN  ...  0.193409  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
332     1    NaN  ...  0.050422  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
333     1    NaN  ...  0.356136  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
334     1    NaN  ...  0.056876  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
335     1    NaN  ...  0.047559  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[336 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.74462890625, 0.240234375]
[0.79248046875, 0.2220458984375]
[0.90380859375, 0.145263671875]
[0.68505859375, 0.343994140625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.74462890625, 0.240234375]
[0.79248046875, 0.2220458984375]
[0.90380859375, 0.145263671875]
[0.68505859375, 0.343994140625]
This is the real loss :  tensor(0.0579, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
332     1    NaN  ...  0.050422  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
333     1    NaN  ...  0.356136  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
334     1    NaN  ...  0.056876  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
335     1    NaN  ...  0.047559  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
336     1    NaN  ...  0.057896  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[337 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.89208984375, 0.2061767578125]
[0.70849609375, 0.23583984375]
[0.76513671875, 0.279052734375]
[0.7626953125, 0.202392578125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.89208984375, 0.2061767578125]
[0.70849609375, 0.23583984375]
[0.76513671875, 0.279052734375]
[0.7626953125, 0.202392578125]
This is the real loss :  tensor(0.0531, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
333     1    NaN  ...  0.356136  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
334     1    NaN  ...  0.056876  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
335     1    NaN  ...  0.047559  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
336     1    NaN  ...  0.057896  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
337     1    NaN  ...  0.053132  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[338 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.90380859375, 0.0770263671875]
[0.794921875, 0.297607421875]
[0.7333984375, 0.330078125]
[0.86669921875, 0.125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.90380859375, 0.0770263671875]
[0.794921875, 0.297607421875]
[0.7333984375, 0.330078125]
[0.86669921875, 0.125]
This is the real loss :  tensor(0.1457, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
334     1    NaN  ...  0.056876  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
335     1    NaN  ...  0.047559  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
336     1    NaN  ...  0.057896  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
337     1    NaN  ...  0.053132  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
338     1    NaN  ...  0.145734  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[339 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.818359375, 0.1614990234375]
[0.8212890625, 0.140380859375]
[0.70068359375, 0.294189453125]
[0.67236328125, 0.246337890625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.818359375, 0.1614990234375]
[0.8212890625, 0.140380859375]
[0.70068359375, 0.294189453125]
[0.67236328125, 0.246337890625]
This is the real loss :  tensor(0.1585, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
335     1    NaN  ...  0.047559  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
336     1    NaN  ...  0.057896  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
337     1    NaN  ...  0.053132  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
338     1    NaN  ...  0.145734  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
339     1    NaN  ...  0.158484  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[340 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.6953125, 0.2030029296875]
[0.84228515625, 0.228271484375]
[0.68798828125, 0.1683349609375]
[0.68212890625, 0.2091064453125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.6953125, 0.2030029296875]
[0.84228515625, 0.228271484375]
[0.68798828125, 0.1683349609375]
[0.68212890625, 0.2091064453125]
This is the real loss :  tensor(0.1784, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
336     1    NaN  ...  0.057896  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
337     1    NaN  ...  0.053132  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
338     1    NaN  ...  0.145734  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
339     1    NaN  ...  0.158484  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
340     1    NaN  ...  0.178441  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[341 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.779296875, 0.30712890625]
[0.783203125, 0.10443115234375]
[0.88427734375, 0.045928955078125]
[0.6904296875, 0.288818359375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.779296875, 0.30712890625]
[0.783203125, 0.10443115234375]
[0.88427734375, 0.045928955078125]
[0.6904296875, 0.288818359375]
This is the real loss :  tensor(0.4775, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
337     1    NaN  ...  0.053132  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
338     1    NaN  ...  0.145734  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
339     1    NaN  ...  0.158484  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
340     1    NaN  ...  0.178441  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
341     1    NaN  ...  0.477494  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[342 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.6943359375, 0.33056640625]
[1.1015625, -0.0804443359375]
[0.703125, 0.2113037109375]
[0.75927734375, 0.29638671875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.6943359375, 0.33056640625]
[1.1015625, -0.0804443359375]
[0.703125, 0.2113037109375]
[0.75927734375, 0.29638671875]
This is the real loss :  tensor(0.3919, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
338     1    NaN  ...  0.145734  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
339     1    NaN  ...  0.158484  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
340     1    NaN  ...  0.178441  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
341     1    NaN  ...  0.477494  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
342     1    NaN  ...  0.391879  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]

[343 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.8271484375, 0.2357177734375]
[0.80126953125, 0.243408203125]
[0.81982421875, 0.1337890625]
[0.83447265625, 0.2186279296875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8271484375, 0.2357177734375]
[0.80126953125, 0.243408203125]
[0.81982421875, 0.1337890625]
[0.83447265625, 0.2186279296875]
This is the real loss :  tensor(0.2102, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
339     1    NaN  ...  0.158484  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
340     1    NaN  ...  0.178441  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
341     1    NaN  ...  0.477494  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
342     1    NaN  ...  0.391879  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
343     1    NaN  ...  0.210227  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[344 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.78369140625, 0.19677734375]
[0.89306640625, 0.152587890625]
[0.716796875, 0.2548828125]
[0.66845703125, 0.31494140625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.78369140625, 0.19677734375]
[0.89306640625, 0.152587890625]
[0.716796875, 0.2548828125]
[0.66845703125, 0.31494140625]
This is the real loss :  tensor(0.0593, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
340     1    NaN  ...  0.178441  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
341     1    NaN  ...  0.477494  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
342     1    NaN  ...  0.391879  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
343     1    NaN  ...  0.210227  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
344     1    NaN  ...  0.059313  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[345 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.84130859375, 0.11468505859375]
[0.65869140625, 0.33154296875]
[0.69873046875, 0.151611328125]
[0.72119140625, 0.1427001953125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.84130859375, 0.11468505859375]
[0.65869140625, 0.33154296875]
[0.69873046875, 0.151611328125]
[0.72119140625, 0.1427001953125]
This is the real loss :  tensor(0.1414, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
341     1    NaN  ...  0.477494  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
342     1    NaN  ...  0.391879  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
343     1    NaN  ...  0.210227  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
344     1    NaN  ...  0.059313  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
345     1    NaN  ...  0.141361  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[346 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.78857421875, 0.1629638671875]
[0.6845703125, 0.34326171875]
[0.703125, 0.2369384765625]
[0.919921875, 0.033721923828125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.78857421875, 0.1629638671875]
[0.6845703125, 0.34326171875]
[0.703125, 0.2369384765625]
[0.919921875, 0.033721923828125]
This is the real loss :  tensor(0.1716, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
342     1    NaN  ...  0.391879  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]]
343     1    NaN  ...  0.210227  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
344     1    NaN  ...  0.059313  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
345     1    NaN  ...  0.141361  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
346     1    NaN  ...  0.171597  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[347 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.70361328125, 0.2880859375]
[0.8095703125, 0.24072265625]
[0.78955078125, 0.1539306640625]
[0.77734375, 0.23583984375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.70361328125, 0.2880859375]
[0.8095703125, 0.24072265625]
[0.78955078125, 0.1539306640625]
[0.77734375, 0.23583984375]
This is the real loss :  tensor(0.0548, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
343     1    NaN  ...  0.210227  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
344     1    NaN  ...  0.059313  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
345     1    NaN  ...  0.141361  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
346     1    NaN  ...  0.171597  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
347     1    NaN  ...  0.054779  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[348 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.69287109375, 0.358154296875]
[0.71728515625, 0.2073974609375]
[0.70947265625, 0.1424560546875]
[0.7421875, 0.141845703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.69287109375, 0.358154296875]
[0.71728515625, 0.2073974609375]
[0.70947265625, 0.1424560546875]
[0.7421875, 0.141845703125]
This is the real loss :  tensor(0.1946, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
344     1    NaN  ...  0.059313  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
345     1    NaN  ...  0.141361  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
346     1    NaN  ...  0.171597  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
347     1    NaN  ...  0.054779  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
348     1    NaN  ...  0.194576  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[349 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.8935546875, 0.122314453125]
[0.69677734375, 0.2388916015625]
[0.72607421875, 0.27001953125]
[0.83544921875, 0.1708984375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8935546875, 0.122314453125]
[0.69677734375, 0.2388916015625]
[0.72607421875, 0.27001953125]
[0.83544921875, 0.1708984375]
This is the real loss :  tensor(0.0474, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
345     1    NaN  ...  0.141361  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
346     1    NaN  ...  0.171597  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
347     1    NaN  ...  0.054779  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
348     1    NaN  ...  0.194576  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
349     1    NaN  ...  0.047442  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[350 rows x 5 columns]checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3

Logits shape before squeeze: torch.Size([4, 2])
[0.8916015625, 0.18212890625]
[0.736328125, 0.259765625]
[0.70654296875, 0.300537109375]
[0.87744140625, 0.189208984375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8916015625, 0.18212890625]
[0.736328125, 0.259765625]
[0.70654296875, 0.300537109375]
[0.87744140625, 0.189208984375]
This is the real loss :  tensor(0.0511, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
346     1    NaN  ...  0.171597  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
347     1    NaN  ...  0.054779  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
348     1    NaN  ...  0.194576  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
349     1    NaN  ...  0.047442  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
350     1    NaN  ...  0.051148  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[351 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.61962890625, 0.333251953125]
[0.67431640625, 0.1759033203125]
[0.7958984375, 0.174560546875]
[0.7236328125, 0.209716796875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.61962890625, 0.333251953125]
[0.67431640625, 0.1759033203125]
[0.7958984375, 0.174560546875]
[0.7236328125, 0.209716796875]
This is the real loss :  tensor(0.2016, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
347     1    NaN  ...  0.054779  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
348     1    NaN  ...  0.194576  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
349     1    NaN  ...  0.047442  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
350     1    NaN  ...  0.051148  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
351     1    NaN  ...  0.201634  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[352 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.80126953125, 0.145751953125]
[0.5849609375, 0.397216796875]
[1.015625, -0.0228118896484375]
[0.58984375, 0.244384765625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.80126953125, 0.145751953125]
[0.5849609375, 0.397216796875]
[1.015625, -0.0228118896484375]
[0.58984375, 0.244384765625]
This is the real loss :  tensor(0.0774, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
348     1    NaN  ...  0.194576  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
349     1    NaN  ...  0.047442  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
350     1    NaN  ...  0.051148  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
351     1    NaN  ...  0.201634  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
352     1    NaN  ...  0.077437  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[353 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.7607421875, 0.194091796875]
[0.94091796875, 0.1824951171875]
[0.634765625, 0.27783203125]
[0.65283203125, 0.282470703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7607421875, 0.194091796875]
[0.94091796875, 0.1824951171875]
[0.634765625, 0.27783203125]
[0.65283203125, 0.282470703125]
This is the real loss :  tensor(0.2095, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
349     1    NaN  ...  0.047442  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
350     1    NaN  ...  0.051148  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
351     1    NaN  ...  0.201634  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
352     1    NaN  ...  0.077437  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
353     1    NaN  ...  0.209489  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[354 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.9296875, 0.070068359375]
[0.68017578125, 0.2279052734375]
[0.89111328125, 0.0511474609375]
[0.6328125, 0.374755859375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.9296875, 0.070068359375]
[0.68017578125, 0.2279052734375]
[0.89111328125, 0.0511474609375]
[0.6328125, 0.374755859375]
This is the real loss :  tensor(0.0567, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
350     1    NaN  ...  0.051148  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
351     1    NaN  ...  0.201634  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
352     1    NaN  ...  0.077437  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
353     1    NaN  ...  0.209489  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
354     1    NaN  ...  0.056728  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[355 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.8642578125, 0.1214599609375]
[0.939453125, 0.0985107421875]
[0.65869140625, 0.309814453125]
[0.6640625, 0.297119140625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8642578125, 0.1214599609375]
[0.939453125, 0.0985107421875]
[0.65869140625, 0.309814453125]
[0.6640625, 0.297119140625]
This is the real loss :  tensor(0.1493, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
351     1    NaN  ...  0.201634  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
352     1    NaN  ...  0.077437  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
353     1    NaN  ...  0.209489  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
354     1    NaN  ...  0.056728  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
355     1    NaN  ...  0.149256  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[356 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.64599609375, 0.3193359375]
[0.978515625, 0.0433349609375]
[0.64404296875, 0.3310546875]
[0.8896484375, 0.135986328125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.64599609375, 0.3193359375]
[0.978515625, 0.0433349609375]
[0.64404296875, 0.3310546875]
[0.8896484375, 0.135986328125]
This is the real loss :  tensor(0.2505, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
352     1    NaN  ...  0.077437  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
353     1    NaN  ...  0.209489  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
354     1    NaN  ...  0.056728  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
355     1    NaN  ...  0.149256  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
356     1    NaN  ...  0.250491  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]

[357 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.9296875, 0.154052734375]
[0.80712890625, 0.2529296875]
[0.83544921875, 0.179931640625]
[0.7998046875, 0.2408447265625]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.9296875, 0.154052734375]
[0.80712890625, 0.2529296875]
[0.83544921875, 0.179931640625]
[0.7998046875, 0.2408447265625]
This is the real loss :  tensor(0.1745, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
353     1    NaN  ...  0.209489  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
354     1    NaN  ...  0.056728  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
355     1    NaN  ...  0.149256  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
356     1    NaN  ...  0.250491  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
357     1    NaN  ...  0.174473  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[358 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.763671875, 0.287109375]
[0.8486328125, 0.128173828125]
[0.86669921875, 0.191650390625]
[0.8095703125, 0.243408203125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.763671875, 0.287109375]
[0.8486328125, 0.128173828125]
[0.86669921875, 0.191650390625]
[0.8095703125, 0.243408203125]
This is the real loss :  tensor(0.1601, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
354     1    NaN  ...  0.056728  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
355     1    NaN  ...  0.149256  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
356     1    NaN  ...  0.250491  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
357     1    NaN  ...  0.174473  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
358     1    NaN  ...  0.160095  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[359 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.93505859375, 0.019927978515625]
[0.7197265625, 0.358154296875]
[0.7919921875, 0.11956787109375]
[0.82666015625, 0.21484375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.93505859375, 0.019927978515625]
[0.7197265625, 0.358154296875]
[0.7919921875, 0.11956787109375]
[0.82666015625, 0.21484375]
This is the real loss :  tensor(0.1335, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
355     1    NaN  ...  0.149256  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
356     1    NaN  ...  0.250491  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
357     1    NaN  ...  0.174473  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
358     1    NaN  ...  0.160095  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
359     1    NaN  ...  0.133544  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[360 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.7158203125, 0.30712890625]
[0.63720703125, 0.2255859375]
[0.80224609375, 0.031585693359375]
[0.72509765625, 0.174560546875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7158203125, 0.30712890625]
[0.63720703125, 0.2255859375]
[0.80224609375, 0.031585693359375]
[0.72509765625, 0.174560546875]
This is the real loss :  tensor(0.1659, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
356     1    NaN  ...  0.250491  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]
357     1    NaN  ...  0.174473  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
358     1    NaN  ...  0.160095  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
359     1    NaN  ...  0.133544  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
360     1    NaN  ...  0.165873  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]

[361 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.75146484375, 0.323974609375]
[0.8916015625, 0.147216796875]
[0.85107421875, 0.1051025390625]
[0.7548828125, 0.284912109375]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.75146484375, 0.323974609375]
[0.8916015625, 0.147216796875]
[0.85107421875, 0.1051025390625]
[0.7548828125, 0.284912109375]
This is the real loss :  tensor(0.0468, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
357     1    NaN  ...  0.174473  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
358     1    NaN  ...  0.160095  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
359     1    NaN  ...  0.133544  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
360     1    NaN  ...  0.165873  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
361     1    NaN  ...  0.046829  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[362 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.86328125, 0.15576171875]
[0.76904296875, 0.261962890625]
[0.818359375, 0.305419921875]
[0.91259765625, 0.1453857421875]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.86328125, 0.15576171875]
[0.76904296875, 0.261962890625]
[0.818359375, 0.305419921875]
[0.91259765625, 0.1453857421875]
This is the real loss :  tensor(0.2169, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
358     1    NaN  ...  0.160095  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
359     1    NaN  ...  0.133544  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
360     1    NaN  ...  0.165873  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
361     1    NaN  ...  0.046829  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
362     1    NaN  ...  0.216876  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]

[363 rows x 5 columns]
Logits shape before squeeze: torch.Size([4, 2])
[0.7265625, 0.19287109375]
[0.71875, 0.322021484375]
[0.93310546875, 0.06451416015625]
[0.869140625, 0.188720703125]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7265625, 0.19287109375]
[0.71875, 0.322021484375]
[0.93310546875, 0.06451416015625]
[0.869140625, 0.188720703125]
This is the real loss :  tensor(0.2617, device='cuda:0', grad_fn=<MseLossBackward0>)
train_records:     Epoch  Batch  ...      Loss                                        True Value
0       0    NaN  ...  0.749726  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
1       0    NaN  ...  0.723797  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
2       0    NaN  ...  0.634870  [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
3       0    NaN  ...  0.469201  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
4       0    NaN  ...  0.425230  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]
..    ...    ...  ...       ...                                               ...
359     1    NaN  ...  0.133544  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
360     1    NaN  ...  0.165873  [[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 0.0]]
361     1    NaN  ...  0.046829  [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
362     1    NaN  ...  0.216876  [[0.0, 1.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]
363     1    NaN  ...  0.261666  [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0]]

[364 rows x 5 columns]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.9703391790390015, 0.10071626305580139]
[1.0423606634140015, 0.05748504027724266]
[0.838593602180481, 0.1463782787322998]
[0.6760329008102417, 0.30962687730789185]
This is the real loss :  0.03305307403206825
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.744438886642456, 0.3637859523296356]
[0.7106611728668213, 0.22471392154693604]
[0.7071610689163208, 0.4125683903694153]
[0.9794442653656006, 0.083783358335495]
This is the real loss :  0.33137762919068336
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.9152745008468628, 0.1329895406961441]
[0.7842320799827576, 0.19534513354301453]
[0.9120141267776489, 0.1621440351009369]
[0.7320761680603027, 0.20885422825813293]
This is the real loss :  0.4945600666105747
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7093090415000916, 0.22028571367263794]
[0.7356954216957092, 0.3602798283100128]
[0.6743542551994324, 0.2141699194908142]
[0.7626919746398926, 0.22027677297592163]
This is the real loss :  0.8188895024359226
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7416818737983704, 0.22701817750930786]
[1.054550051689148, -0.0269180778414011]
[0.8712937831878662, 0.3100886046886444]
[0.771156370639801, 0.1835695207118988]
This is the real loss :  0.8589836545288563
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8048976063728333, 0.22056573629379272]
[0.7882082462310791, 0.17406627535820007]
[0.8669735789299011, 0.10078263282775879]
[0.745419979095459, 0.2715674638748169]
This is the real loss :  1.0915666408836842
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8729959726333618, 0.09797818958759308]
[0.6699813604354858, 0.4099329710006714]
[0.6898970007896423, 0.24117258191108704]
[0.7981094121932983, 0.21481645107269287]
This is the real loss :  1.3053800202906132
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7087931632995605, 0.40655359625816345]
[1.039980411529541, 0.0033631175756454468]
[0.6854578852653503, 0.34767311811447144]
[0.7740055918693542, 0.26257508993148804]
This is the real loss :  1.4637674130499363
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8932803869247437, 0.4190457761287689]
[1.050688624382019, 0.078736811876297]
[0.8465979099273682, 0.11586499214172363]
[0.9790703058242798, 0.08728978037834167]
This is the real loss :  1.919535044580698
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8564683198928833, 0.15638728439807892]
[0.8699896335601807, 0.14983908832073212]
[0.7494888305664062, 0.408658504486084]
[0.9229668378829956, 0.33701470494270325]
This is the real loss :  1.9737454690039158
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.9698976278305054, 0.03964613005518913]
[0.920474648475647, 0.12868590652942657]
[0.8596698045730591, 0.1272944211959839]
[1.1827764511108398, -0.033768180757761]
This is the real loss :  2.1688151098787785
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.6624321937561035, 0.3573226034641266]
[0.8676033020019531, 0.20064687728881836]
[0.7572495937347412, 0.20920884609222412]
[0.709422767162323, 0.22430536150932312]
This is the real loss :  2.5239415504038334
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.9300236701965332, 0.054322388023138046]
[0.8331695795059204, 0.1553637683391571]
[1.0179489850997925, 0.032668959349393845]
[0.8417826294898987, 0.18494492769241333]
This is the real loss :  2.708448577672243
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312], [0.9300236701965332, 0.054322388023138046, 0.8331695795059204, 0.1553637683391571], [1.0179489850997925, 0.032668959349393845, 0.8417826294898987, 0.18494492769241333]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.810563325881958, 0.1836356520652771]
[1.1081351041793823, -0.06599172949790955]
[0.6468219757080078, 0.3458862900733948]
[1.0146214962005615, 0.027930568903684616]
This is the real loss :  2.7498263753950596
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312], [0.9300236701965332, 0.054322388023138046, 0.8331695795059204, 0.1553637683391571], [1.0179489850997925, 0.032668959349393845, 0.8417826294898987, 0.18494492769241333], [0.810563325881958, 0.1836356520652771, 1.1081351041793823, -0.06599172949790955], [0.6468219757080078, 0.3458862900733948, 1.0146214962005615, 0.027930568903684616]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.9468859434127808, 0.0652921199798584]
[0.8776399493217468, 0.14643652737140656]
[0.6590524911880493, 0.27201277017593384]
[0.9705872535705566, 0.050376009196043015]
This is the real loss :  2.7794687263667583
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312], [0.9300236701965332, 0.054322388023138046, 0.8331695795059204, 0.1553637683391571], [1.0179489850997925, 0.032668959349393845, 0.8417826294898987, 0.18494492769241333], [0.810563325881958, 0.1836356520652771, 1.1081351041793823, -0.06599172949790955], [0.6468219757080078, 0.3458862900733948, 1.0146214962005615, 0.027930568903684616], [0.9468859434127808, 0.0652921199798584, 0.8776399493217468, 0.14643652737140656], [0.6590524911880493, 0.27201277017593384, 0.9705872535705566, 0.050376009196043015]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.9655159711837769, 0.11080324649810791]
[0.8375571966171265, 0.20295009016990662]
[0.9372574090957642, 0.2209758758544922]
[1.0080554485321045, 0.008454877883195877]
This is the real loss :  2.7962120082229376
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312], [0.9300236701965332, 0.054322388023138046, 0.8331695795059204, 0.1553637683391571], [1.0179489850997925, 0.032668959349393845, 0.8417826294898987, 0.18494492769241333], [0.810563325881958, 0.1836356520652771, 1.1081351041793823, -0.06599172949790955], [0.6468219757080078, 0.3458862900733948, 1.0146214962005615, 0.027930568903684616], [0.9468859434127808, 0.0652921199798584, 0.8776399493217468, 0.14643652737140656], [0.6590524911880493, 0.27201277017593384, 0.9705872535705566, 0.050376009196043015], [0.9655159711837769, 0.11080324649810791, 0.8375571966171265, 0.20295009016990662], [0.9372574090957642, 0.2209758758544922, 1.0080554485321045, 0.008454877883195877]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.6932350397109985, 0.2274811863899231]
[0.9273480176925659, 0.07786241173744202]
[1.057912826538086, 0.006586160510778427]
[1.040104627609253, 0.014642085880041122]
This is the real loss :  3.0793453361839056
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312], [0.9300236701965332, 0.054322388023138046, 0.8331695795059204, 0.1553637683391571], [1.0179489850997925, 0.032668959349393845, 0.8417826294898987, 0.18494492769241333], [0.810563325881958, 0.1836356520652771, 1.1081351041793823, -0.06599172949790955], [0.6468219757080078, 0.3458862900733948, 1.0146214962005615, 0.027930568903684616], [0.9468859434127808, 0.0652921199798584, 0.8776399493217468, 0.14643652737140656], [0.6590524911880493, 0.27201277017593384, 0.9705872535705566, 0.050376009196043015], [0.9655159711837769, 0.11080324649810791, 0.8375571966171265, 0.20295009016990662], [0.9372574090957642, 0.2209758758544922, 1.0080554485321045, 0.008454877883195877], [0.6932350397109985, 0.2274811863899231, 0.9273480176925659, 0.07786241173744202], [1.057912826538086, 0.006586160510778427, 1.040104627609253, 0.014642085880041122]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8281402587890625, 0.2791145443916321]
[0.8740947246551514, 0.1683582365512848]
[0.8653237223625183, 0.11307235062122345]
[0.8575713634490967, 0.143876314163208]
This is the real loss :  3.4229688551276922
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312], [0.9300236701965332, 0.054322388023138046, 0.8331695795059204, 0.1553637683391571], [1.0179489850997925, 0.032668959349393845, 0.8417826294898987, 0.18494492769241333], [0.810563325881958, 0.1836356520652771, 1.1081351041793823, -0.06599172949790955], [0.6468219757080078, 0.3458862900733948, 1.0146214962005615, 0.027930568903684616], [0.9468859434127808, 0.0652921199798584, 0.8776399493217468, 0.14643652737140656], [0.6590524911880493, 0.27201277017593384, 0.9705872535705566, 0.050376009196043015], [0.9655159711837769, 0.11080324649810791, 0.8375571966171265, 0.20295009016990662], [0.9372574090957642, 0.2209758758544922, 1.0080554485321045, 0.008454877883195877], [0.6932350397109985, 0.2274811863899231, 0.9273480176925659, 0.07786241173744202], [1.057912826538086, 0.006586160510778427, 1.040104627609253, 0.014642085880041122], [0.8281402587890625, 0.2791145443916321, 0.8740947246551514, 0.1683582365512848], [0.8653237223625183, 0.11307235062122345, 0.8575713634490967, 0.143876314163208]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.9846587181091309, 0.11652521789073944]
[0.7820221781730652, 0.1633124202489853]
[0.9107496738433838, 0.22016048431396484]
[1.0211989879608154, 0.08445832133293152]
This is the real loss :  3.441971058025956
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312], [0.9300236701965332, 0.054322388023138046, 0.8331695795059204, 0.1553637683391571], [1.0179489850997925, 0.032668959349393845, 0.8417826294898987, 0.18494492769241333], [0.810563325881958, 0.1836356520652771, 1.1081351041793823, -0.06599172949790955], [0.6468219757080078, 0.3458862900733948, 1.0146214962005615, 0.027930568903684616], [0.9468859434127808, 0.0652921199798584, 0.8776399493217468, 0.14643652737140656], [0.6590524911880493, 0.27201277017593384, 0.9705872535705566, 0.050376009196043015], [0.9655159711837769, 0.11080324649810791, 0.8375571966171265, 0.20295009016990662], [0.9372574090957642, 0.2209758758544922, 1.0080554485321045, 0.008454877883195877], [0.6932350397109985, 0.2274811863899231, 0.9273480176925659, 0.07786241173744202], [1.057912826538086, 0.006586160510778427, 1.040104627609253, 0.014642085880041122], [0.8281402587890625, 0.2791145443916321, 0.8740947246551514, 0.1683582365512848], [0.8653237223625183, 0.11307235062122345, 0.8575713634490967, 0.143876314163208], [0.9846587181091309, 0.11652521789073944, 0.7820221781730652, 0.1633124202489853], [0.9107496738433838, 0.22016048431396484, 1.0211989879608154, 0.08445832133293152]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.9833457469940186, 0.08856514096260071]
[0.8046919703483582, 0.2620636820793152]
[0.9705270528793335, 0.016808580607175827]
[0.6563172340393066, 0.2790708839893341]
This is the real loss :  3.5752943251281977
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312], [0.9300236701965332, 0.054322388023138046, 0.8331695795059204, 0.1553637683391571], [1.0179489850997925, 0.032668959349393845, 0.8417826294898987, 0.18494492769241333], [0.810563325881958, 0.1836356520652771, 1.1081351041793823, -0.06599172949790955], [0.6468219757080078, 0.3458862900733948, 1.0146214962005615, 0.027930568903684616], [0.9468859434127808, 0.0652921199798584, 0.8776399493217468, 0.14643652737140656], [0.6590524911880493, 0.27201277017593384, 0.9705872535705566, 0.050376009196043015], [0.9655159711837769, 0.11080324649810791, 0.8375571966171265, 0.20295009016990662], [0.9372574090957642, 0.2209758758544922, 1.0080554485321045, 0.008454877883195877], [0.6932350397109985, 0.2274811863899231, 0.9273480176925659, 0.07786241173744202], [1.057912826538086, 0.006586160510778427, 1.040104627609253, 0.014642085880041122], [0.8281402587890625, 0.2791145443916321, 0.8740947246551514, 0.1683582365512848], [0.8653237223625183, 0.11307235062122345, 0.8575713634490967, 0.143876314163208], [0.9846587181091309, 0.11652521789073944, 0.7820221781730652, 0.1633124202489853], [0.9107496738433838, 0.22016048431396484, 1.0211989879608154, 0.08445832133293152], [0.9833457469940186, 0.08856514096260071, 0.8046919703483582, 0.2620636820793152], [0.9705270528793335, 0.016808580607175827, 0.6563172340393066, 0.2790708839893341]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8406693935394287, 0.16735345125198364]
[0.8846079111099243, 0.27279847860336304]
[0.957105278968811, 0.07565213739871979]
[0.8096810579299927, 0.16623593866825104]
This is the real loss :  3.601862655952573
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312], [0.9300236701965332, 0.054322388023138046, 0.8331695795059204, 0.1553637683391571], [1.0179489850997925, 0.032668959349393845, 0.8417826294898987, 0.18494492769241333], [0.810563325881958, 0.1836356520652771, 1.1081351041793823, -0.06599172949790955], [0.6468219757080078, 0.3458862900733948, 1.0146214962005615, 0.027930568903684616], [0.9468859434127808, 0.0652921199798584, 0.8776399493217468, 0.14643652737140656], [0.6590524911880493, 0.27201277017593384, 0.9705872535705566, 0.050376009196043015], [0.9655159711837769, 0.11080324649810791, 0.8375571966171265, 0.20295009016990662], [0.9372574090957642, 0.2209758758544922, 1.0080554485321045, 0.008454877883195877], [0.6932350397109985, 0.2274811863899231, 0.9273480176925659, 0.07786241173744202], [1.057912826538086, 0.006586160510778427, 1.040104627609253, 0.014642085880041122], [0.8281402587890625, 0.2791145443916321, 0.8740947246551514, 0.1683582365512848], [0.8653237223625183, 0.11307235062122345, 0.8575713634490967, 0.143876314163208], [0.9846587181091309, 0.11652521789073944, 0.7820221781730652, 0.1633124202489853], [0.9107496738433838, 0.22016048431396484, 1.0211989879608154, 0.08445832133293152], [0.9833457469940186, 0.08856514096260071, 0.8046919703483582, 0.2620636820793152], [0.9705270528793335, 0.016808580607175827, 0.6563172340393066, 0.2790708839893341], [0.8406693935394287, 0.16735345125198364, 0.8846079111099243, 0.27279847860336304], [0.957105278968811, 0.07565213739871979, 0.8096810579299927, 0.16623593866825104]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8899181485176086, 0.18695345520973206]
[0.7108429670333862, 0.2769438326358795]
[0.9080440998077393, 0.14104416966438293]
[0.9763479232788086, 0.10297232866287231]
This is the real loss :  3.6327240746468306
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312], [0.9300236701965332, 0.054322388023138046, 0.8331695795059204, 0.1553637683391571], [1.0179489850997925, 0.032668959349393845, 0.8417826294898987, 0.18494492769241333], [0.810563325881958, 0.1836356520652771, 1.1081351041793823, -0.06599172949790955], [0.6468219757080078, 0.3458862900733948, 1.0146214962005615, 0.027930568903684616], [0.9468859434127808, 0.0652921199798584, 0.8776399493217468, 0.14643652737140656], [0.6590524911880493, 0.27201277017593384, 0.9705872535705566, 0.050376009196043015], [0.9655159711837769, 0.11080324649810791, 0.8375571966171265, 0.20295009016990662], [0.9372574090957642, 0.2209758758544922, 1.0080554485321045, 0.008454877883195877], [0.6932350397109985, 0.2274811863899231, 0.9273480176925659, 0.07786241173744202], [1.057912826538086, 0.006586160510778427, 1.040104627609253, 0.014642085880041122], [0.8281402587890625, 0.2791145443916321, 0.8740947246551514, 0.1683582365512848], [0.8653237223625183, 0.11307235062122345, 0.8575713634490967, 0.143876314163208], [0.9846587181091309, 0.11652521789073944, 0.7820221781730652, 0.1633124202489853], [0.9107496738433838, 0.22016048431396484, 1.0211989879608154, 0.08445832133293152], [0.9833457469940186, 0.08856514096260071, 0.8046919703483582, 0.2620636820793152], [0.9705270528793335, 0.016808580607175827, 0.6563172340393066, 0.2790708839893341], [0.8406693935394287, 0.16735345125198364, 0.8846079111099243, 0.27279847860336304], [0.957105278968811, 0.07565213739871979, 0.8096810579299927, 0.16623593866825104], [0.8899181485176086, 0.18695345520973206, 0.7108429670333862, 0.2769438326358795], [0.9080440998077393, 0.14104416966438293, 0.9763479232788086, 0.10297232866287231]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8038797378540039, 0.21278029680252075]
[0.6580767631530762, 0.3093200922012329]
[0.8384658694267273, 0.21153587102890015]
[0.676618218421936, 0.30802637338638306]
This is the real loss :  3.7957002501934767
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312], [0.9300236701965332, 0.054322388023138046, 0.8331695795059204, 0.1553637683391571], [1.0179489850997925, 0.032668959349393845, 0.8417826294898987, 0.18494492769241333], [0.810563325881958, 0.1836356520652771, 1.1081351041793823, -0.06599172949790955], [0.6468219757080078, 0.3458862900733948, 1.0146214962005615, 0.027930568903684616], [0.9468859434127808, 0.0652921199798584, 0.8776399493217468, 0.14643652737140656], [0.6590524911880493, 0.27201277017593384, 0.9705872535705566, 0.050376009196043015], [0.9655159711837769, 0.11080324649810791, 0.8375571966171265, 0.20295009016990662], [0.9372574090957642, 0.2209758758544922, 1.0080554485321045, 0.008454877883195877], [0.6932350397109985, 0.2274811863899231, 0.9273480176925659, 0.07786241173744202], [1.057912826538086, 0.006586160510778427, 1.040104627609253, 0.014642085880041122], [0.8281402587890625, 0.2791145443916321, 0.8740947246551514, 0.1683582365512848], [0.8653237223625183, 0.11307235062122345, 0.8575713634490967, 0.143876314163208], [0.9846587181091309, 0.11652521789073944, 0.7820221781730652, 0.1633124202489853], [0.9107496738433838, 0.22016048431396484, 1.0211989879608154, 0.08445832133293152], [0.9833457469940186, 0.08856514096260071, 0.8046919703483582, 0.2620636820793152], [0.9705270528793335, 0.016808580607175827, 0.6563172340393066, 0.2790708839893341], [0.8406693935394287, 0.16735345125198364, 0.8846079111099243, 0.27279847860336304], [0.957105278968811, 0.07565213739871979, 0.8096810579299927, 0.16623593866825104], [0.8899181485176086, 0.18695345520973206, 0.7108429670333862, 0.2769438326358795], [0.9080440998077393, 0.14104416966438293, 0.9763479232788086, 0.10297232866287231], [0.8038797378540039, 0.21278029680252075, 0.6580767631530762, 0.3093200922012329], [0.8384658694267273, 0.21153587102890015, 0.676618218421936, 0.30802637338638306]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.9408621788024902, 0.04200815036892891]
[0.7111213207244873, 0.3274080157279968]
[0.6615606546401978, 0.3074386715888977]
[0.8542405962944031, 0.18857811391353607]
This is the real loss :  3.853422263637185
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312], [0.9300236701965332, 0.054322388023138046, 0.8331695795059204, 0.1553637683391571], [1.0179489850997925, 0.032668959349393845, 0.8417826294898987, 0.18494492769241333], [0.810563325881958, 0.1836356520652771, 1.1081351041793823, -0.06599172949790955], [0.6468219757080078, 0.3458862900733948, 1.0146214962005615, 0.027930568903684616], [0.9468859434127808, 0.0652921199798584, 0.8776399493217468, 0.14643652737140656], [0.6590524911880493, 0.27201277017593384, 0.9705872535705566, 0.050376009196043015], [0.9655159711837769, 0.11080324649810791, 0.8375571966171265, 0.20295009016990662], [0.9372574090957642, 0.2209758758544922, 1.0080554485321045, 0.008454877883195877], [0.6932350397109985, 0.2274811863899231, 0.9273480176925659, 0.07786241173744202], [1.057912826538086, 0.006586160510778427, 1.040104627609253, 0.014642085880041122], [0.8281402587890625, 0.2791145443916321, 0.8740947246551514, 0.1683582365512848], [0.8653237223625183, 0.11307235062122345, 0.8575713634490967, 0.143876314163208], [0.9846587181091309, 0.11652521789073944, 0.7820221781730652, 0.1633124202489853], [0.9107496738433838, 0.22016048431396484, 1.0211989879608154, 0.08445832133293152], [0.9833457469940186, 0.08856514096260071, 0.8046919703483582, 0.2620636820793152], [0.9705270528793335, 0.016808580607175827, 0.6563172340393066, 0.2790708839893341], [0.8406693935394287, 0.16735345125198364, 0.8846079111099243, 0.27279847860336304], [0.957105278968811, 0.07565213739871979, 0.8096810579299927, 0.16623593866825104], [0.8899181485176086, 0.18695345520973206, 0.7108429670333862, 0.2769438326358795], [0.9080440998077393, 0.14104416966438293, 0.9763479232788086, 0.10297232866287231], [0.8038797378540039, 0.21278029680252075, 0.6580767631530762, 0.3093200922012329], [0.8384658694267273, 0.21153587102890015, 0.676618218421936, 0.30802637338638306], [0.9408621788024902, 0.04200815036892891, 0.7111213207244873, 0.3274080157279968], [0.6615606546401978, 0.3074386715888977, 0.8542405962944031, 0.18857811391353607]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7992740869522095, 0.2952626347541809]
[0.9778791666030884, 0.05303778871893883]
[1.000457525253296, 0.060699667781591415]
[0.9569377899169922, 0.11249220371246338]
This is the real loss :  4.229256311431527
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312], [0.9300236701965332, 0.054322388023138046, 0.8331695795059204, 0.1553637683391571], [1.0179489850997925, 0.032668959349393845, 0.8417826294898987, 0.18494492769241333], [0.810563325881958, 0.1836356520652771, 1.1081351041793823, -0.06599172949790955], [0.6468219757080078, 0.3458862900733948, 1.0146214962005615, 0.027930568903684616], [0.9468859434127808, 0.0652921199798584, 0.8776399493217468, 0.14643652737140656], [0.6590524911880493, 0.27201277017593384, 0.9705872535705566, 0.050376009196043015], [0.9655159711837769, 0.11080324649810791, 0.8375571966171265, 0.20295009016990662], [0.9372574090957642, 0.2209758758544922, 1.0080554485321045, 0.008454877883195877], [0.6932350397109985, 0.2274811863899231, 0.9273480176925659, 0.07786241173744202], [1.057912826538086, 0.006586160510778427, 1.040104627609253, 0.014642085880041122], [0.8281402587890625, 0.2791145443916321, 0.8740947246551514, 0.1683582365512848], [0.8653237223625183, 0.11307235062122345, 0.8575713634490967, 0.143876314163208], [0.9846587181091309, 0.11652521789073944, 0.7820221781730652, 0.1633124202489853], [0.9107496738433838, 0.22016048431396484, 1.0211989879608154, 0.08445832133293152], [0.9833457469940186, 0.08856514096260071, 0.8046919703483582, 0.2620636820793152], [0.9705270528793335, 0.016808580607175827, 0.6563172340393066, 0.2790708839893341], [0.8406693935394287, 0.16735345125198364, 0.8846079111099243, 0.27279847860336304], [0.957105278968811, 0.07565213739871979, 0.8096810579299927, 0.16623593866825104], [0.8899181485176086, 0.18695345520973206, 0.7108429670333862, 0.2769438326358795], [0.9080440998077393, 0.14104416966438293, 0.9763479232788086, 0.10297232866287231], [0.8038797378540039, 0.21278029680252075, 0.6580767631530762, 0.3093200922012329], [0.8384658694267273, 0.21153587102890015, 0.676618218421936, 0.30802637338638306], [0.9408621788024902, 0.04200815036892891, 0.7111213207244873, 0.3274080157279968], [0.6615606546401978, 0.3074386715888977, 0.8542405962944031, 0.18857811391353607], [0.7992740869522095, 0.2952626347541809, 0.9778791666030884, 0.05303778871893883], [1.000457525253296, 0.060699667781591415, 0.9569377899169922, 0.11249220371246338]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.9100946187973022, 0.09765534102916718]
[1.0501701831817627, -0.03090789169073105]
[0.9058859348297119, 0.21034476161003113]
[1.0467005968093872, -0.022340204566717148]
This is the real loss :  4.238865598104894
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312], [0.9300236701965332, 0.054322388023138046, 0.8331695795059204, 0.1553637683391571], [1.0179489850997925, 0.032668959349393845, 0.8417826294898987, 0.18494492769241333], [0.810563325881958, 0.1836356520652771, 1.1081351041793823, -0.06599172949790955], [0.6468219757080078, 0.3458862900733948, 1.0146214962005615, 0.027930568903684616], [0.9468859434127808, 0.0652921199798584, 0.8776399493217468, 0.14643652737140656], [0.6590524911880493, 0.27201277017593384, 0.9705872535705566, 0.050376009196043015], [0.9655159711837769, 0.11080324649810791, 0.8375571966171265, 0.20295009016990662], [0.9372574090957642, 0.2209758758544922, 1.0080554485321045, 0.008454877883195877], [0.6932350397109985, 0.2274811863899231, 0.9273480176925659, 0.07786241173744202], [1.057912826538086, 0.006586160510778427, 1.040104627609253, 0.014642085880041122], [0.8281402587890625, 0.2791145443916321, 0.8740947246551514, 0.1683582365512848], [0.8653237223625183, 0.11307235062122345, 0.8575713634490967, 0.143876314163208], [0.9846587181091309, 0.11652521789073944, 0.7820221781730652, 0.1633124202489853], [0.9107496738433838, 0.22016048431396484, 1.0211989879608154, 0.08445832133293152], [0.9833457469940186, 0.08856514096260071, 0.8046919703483582, 0.2620636820793152], [0.9705270528793335, 0.016808580607175827, 0.6563172340393066, 0.2790708839893341], [0.8406693935394287, 0.16735345125198364, 0.8846079111099243, 0.27279847860336304], [0.957105278968811, 0.07565213739871979, 0.8096810579299927, 0.16623593866825104], [0.8899181485176086, 0.18695345520973206, 0.7108429670333862, 0.2769438326358795], [0.9080440998077393, 0.14104416966438293, 0.9763479232788086, 0.10297232866287231], [0.8038797378540039, 0.21278029680252075, 0.6580767631530762, 0.3093200922012329], [0.8384658694267273, 0.21153587102890015, 0.676618218421936, 0.30802637338638306], [0.9408621788024902, 0.04200815036892891, 0.7111213207244873, 0.3274080157279968], [0.6615606546401978, 0.3074386715888977, 0.8542405962944031, 0.18857811391353607], [0.7992740869522095, 0.2952626347541809, 0.9778791666030884, 0.05303778871893883], [1.000457525253296, 0.060699667781591415, 0.9569377899169922, 0.11249220371246338], [0.9100946187973022, 0.09765534102916718, 1.0501701831817627, -0.03090789169073105], [0.9058859348297119, 0.21034476161003113, 1.0467005968093872, -0.022340204566717148]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.6906922459602356, 0.2607361674308777]
[0.9418931007385254, 0.06455519795417786]
[0.9470680952072144, 0.08296096324920654]
[0.7873597145080566, 0.31279534101486206]
This is the real loss :  4.397999151609838
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312], [0.9300236701965332, 0.054322388023138046, 0.8331695795059204, 0.1553637683391571], [1.0179489850997925, 0.032668959349393845, 0.8417826294898987, 0.18494492769241333], [0.810563325881958, 0.1836356520652771, 1.1081351041793823, -0.06599172949790955], [0.6468219757080078, 0.3458862900733948, 1.0146214962005615, 0.027930568903684616], [0.9468859434127808, 0.0652921199798584, 0.8776399493217468, 0.14643652737140656], [0.6590524911880493, 0.27201277017593384, 0.9705872535705566, 0.050376009196043015], [0.9655159711837769, 0.11080324649810791, 0.8375571966171265, 0.20295009016990662], [0.9372574090957642, 0.2209758758544922, 1.0080554485321045, 0.008454877883195877], [0.6932350397109985, 0.2274811863899231, 0.9273480176925659, 0.07786241173744202], [1.057912826538086, 0.006586160510778427, 1.040104627609253, 0.014642085880041122], [0.8281402587890625, 0.2791145443916321, 0.8740947246551514, 0.1683582365512848], [0.8653237223625183, 0.11307235062122345, 0.8575713634490967, 0.143876314163208], [0.9846587181091309, 0.11652521789073944, 0.7820221781730652, 0.1633124202489853], [0.9107496738433838, 0.22016048431396484, 1.0211989879608154, 0.08445832133293152], [0.9833457469940186, 0.08856514096260071, 0.8046919703483582, 0.2620636820793152], [0.9705270528793335, 0.016808580607175827, 0.6563172340393066, 0.2790708839893341], [0.8406693935394287, 0.16735345125198364, 0.8846079111099243, 0.27279847860336304], [0.957105278968811, 0.07565213739871979, 0.8096810579299927, 0.16623593866825104], [0.8899181485176086, 0.18695345520973206, 0.7108429670333862, 0.2769438326358795], [0.9080440998077393, 0.14104416966438293, 0.9763479232788086, 0.10297232866287231], [0.8038797378540039, 0.21278029680252075, 0.6580767631530762, 0.3093200922012329], [0.8384658694267273, 0.21153587102890015, 0.676618218421936, 0.30802637338638306], [0.9408621788024902, 0.04200815036892891, 0.7111213207244873, 0.3274080157279968], [0.6615606546401978, 0.3074386715888977, 0.8542405962944031, 0.18857811391353607], [0.7992740869522095, 0.2952626347541809, 0.9778791666030884, 0.05303778871893883], [1.000457525253296, 0.060699667781591415, 0.9569377899169922, 0.11249220371246338], [0.9100946187973022, 0.09765534102916718, 1.0501701831817627, -0.03090789169073105], [0.9058859348297119, 0.21034476161003113, 1.0467005968093872, -0.022340204566717148], [0.6906922459602356, 0.2607361674308777, 0.9418931007385254, 0.06455519795417786], [0.9470680952072144, 0.08296096324920654, 0.7873597145080566, 0.31279534101486206]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7743943929672241, 0.2555060088634491]
[0.8511923551559448, 0.18559014797210693]
[0.8840795755386353, 0.16196376085281372]
[1.015209674835205, -0.02973618172109127]
This is the real loss :  4.4246934009715915
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312], [0.9300236701965332, 0.054322388023138046, 0.8331695795059204, 0.1553637683391571], [1.0179489850997925, 0.032668959349393845, 0.8417826294898987, 0.18494492769241333], [0.810563325881958, 0.1836356520652771, 1.1081351041793823, -0.06599172949790955], [0.6468219757080078, 0.3458862900733948, 1.0146214962005615, 0.027930568903684616], [0.9468859434127808, 0.0652921199798584, 0.8776399493217468, 0.14643652737140656], [0.6590524911880493, 0.27201277017593384, 0.9705872535705566, 0.050376009196043015], [0.9655159711837769, 0.11080324649810791, 0.8375571966171265, 0.20295009016990662], [0.9372574090957642, 0.2209758758544922, 1.0080554485321045, 0.008454877883195877], [0.6932350397109985, 0.2274811863899231, 0.9273480176925659, 0.07786241173744202], [1.057912826538086, 0.006586160510778427, 1.040104627609253, 0.014642085880041122], [0.8281402587890625, 0.2791145443916321, 0.8740947246551514, 0.1683582365512848], [0.8653237223625183, 0.11307235062122345, 0.8575713634490967, 0.143876314163208], [0.9846587181091309, 0.11652521789073944, 0.7820221781730652, 0.1633124202489853], [0.9107496738433838, 0.22016048431396484, 1.0211989879608154, 0.08445832133293152], [0.9833457469940186, 0.08856514096260071, 0.8046919703483582, 0.2620636820793152], [0.9705270528793335, 0.016808580607175827, 0.6563172340393066, 0.2790708839893341], [0.8406693935394287, 0.16735345125198364, 0.8846079111099243, 0.27279847860336304], [0.957105278968811, 0.07565213739871979, 0.8096810579299927, 0.16623593866825104], [0.8899181485176086, 0.18695345520973206, 0.7108429670333862, 0.2769438326358795], [0.9080440998077393, 0.14104416966438293, 0.9763479232788086, 0.10297232866287231], [0.8038797378540039, 0.21278029680252075, 0.6580767631530762, 0.3093200922012329], [0.8384658694267273, 0.21153587102890015, 0.676618218421936, 0.30802637338638306], [0.9408621788024902, 0.04200815036892891, 0.7111213207244873, 0.3274080157279968], [0.6615606546401978, 0.3074386715888977, 0.8542405962944031, 0.18857811391353607], [0.7992740869522095, 0.2952626347541809, 0.9778791666030884, 0.05303778871893883], [1.000457525253296, 0.060699667781591415, 0.9569377899169922, 0.11249220371246338], [0.9100946187973022, 0.09765534102916718, 1.0501701831817627, -0.03090789169073105], [0.9058859348297119, 0.21034476161003113, 1.0467005968093872, -0.022340204566717148], [0.6906922459602356, 0.2607361674308777, 0.9418931007385254, 0.06455519795417786], [0.9470680952072144, 0.08296096324920654, 0.7873597145080566, 0.31279534101486206], [0.7743943929672241, 0.2555060088634491, 0.8511923551559448, 0.18559014797210693], [0.8840795755386353, 0.16196376085281372, 1.015209674835205, -0.02973618172109127]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8331042528152466, 0.18027056753635406]
[0.8329649567604065, 0.22361087799072266]
[0.9583555459976196, 0.07697772979736328]
[0.9735144376754761, 0.010621603578329086]
This is the real loss :  4.443034443072975
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312], [0.9300236701965332, 0.054322388023138046, 0.8331695795059204, 0.1553637683391571], [1.0179489850997925, 0.032668959349393845, 0.8417826294898987, 0.18494492769241333], [0.810563325881958, 0.1836356520652771, 1.1081351041793823, -0.06599172949790955], [0.6468219757080078, 0.3458862900733948, 1.0146214962005615, 0.027930568903684616], [0.9468859434127808, 0.0652921199798584, 0.8776399493217468, 0.14643652737140656], [0.6590524911880493, 0.27201277017593384, 0.9705872535705566, 0.050376009196043015], [0.9655159711837769, 0.11080324649810791, 0.8375571966171265, 0.20295009016990662], [0.9372574090957642, 0.2209758758544922, 1.0080554485321045, 0.008454877883195877], [0.6932350397109985, 0.2274811863899231, 0.9273480176925659, 0.07786241173744202], [1.057912826538086, 0.006586160510778427, 1.040104627609253, 0.014642085880041122], [0.8281402587890625, 0.2791145443916321, 0.8740947246551514, 0.1683582365512848], [0.8653237223625183, 0.11307235062122345, 0.8575713634490967, 0.143876314163208], [0.9846587181091309, 0.11652521789073944, 0.7820221781730652, 0.1633124202489853], [0.9107496738433838, 0.22016048431396484, 1.0211989879608154, 0.08445832133293152], [0.9833457469940186, 0.08856514096260071, 0.8046919703483582, 0.2620636820793152], [0.9705270528793335, 0.016808580607175827, 0.6563172340393066, 0.2790708839893341], [0.8406693935394287, 0.16735345125198364, 0.8846079111099243, 0.27279847860336304], [0.957105278968811, 0.07565213739871979, 0.8096810579299927, 0.16623593866825104], [0.8899181485176086, 0.18695345520973206, 0.7108429670333862, 0.2769438326358795], [0.9080440998077393, 0.14104416966438293, 0.9763479232788086, 0.10297232866287231], [0.8038797378540039, 0.21278029680252075, 0.6580767631530762, 0.3093200922012329], [0.8384658694267273, 0.21153587102890015, 0.676618218421936, 0.30802637338638306], [0.9408621788024902, 0.04200815036892891, 0.7111213207244873, 0.3274080157279968], [0.6615606546401978, 0.3074386715888977, 0.8542405962944031, 0.18857811391353607], [0.7992740869522095, 0.2952626347541809, 0.9778791666030884, 0.05303778871893883], [1.000457525253296, 0.060699667781591415, 0.9569377899169922, 0.11249220371246338], [0.9100946187973022, 0.09765534102916718, 1.0501701831817627, -0.03090789169073105], [0.9058859348297119, 0.21034476161003113, 1.0467005968093872, -0.022340204566717148], [0.6906922459602356, 0.2607361674308777, 0.9418931007385254, 0.06455519795417786], [0.9470680952072144, 0.08296096324920654, 0.7873597145080566, 0.31279534101486206], [0.7743943929672241, 0.2555060088634491, 0.8511923551559448, 0.18559014797210693], [0.8840795755386353, 0.16196376085281372, 1.015209674835205, -0.02973618172109127], [0.8331042528152466, 0.18027056753635406, 0.8329649567604065, 0.22361087799072266], [0.9583555459976196, 0.07697772979736328, 0.9735144376754761, 0.010621603578329086]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7160120010375977, 0.2982387840747833]
[0.9510098695755005, 0.07159459590911865]
[0.8244237899780273, 0.23431620001792908]
[0.8682907223701477, 0.1666729897260666]
This is the real loss :  4.585975202731788
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312], [0.9300236701965332, 0.054322388023138046, 0.8331695795059204, 0.1553637683391571], [1.0179489850997925, 0.032668959349393845, 0.8417826294898987, 0.18494492769241333], [0.810563325881958, 0.1836356520652771, 1.1081351041793823, -0.06599172949790955], [0.6468219757080078, 0.3458862900733948, 1.0146214962005615, 0.027930568903684616], [0.9468859434127808, 0.0652921199798584, 0.8776399493217468, 0.14643652737140656], [0.6590524911880493, 0.27201277017593384, 0.9705872535705566, 0.050376009196043015], [0.9655159711837769, 0.11080324649810791, 0.8375571966171265, 0.20295009016990662], [0.9372574090957642, 0.2209758758544922, 1.0080554485321045, 0.008454877883195877], [0.6932350397109985, 0.2274811863899231, 0.9273480176925659, 0.07786241173744202], [1.057912826538086, 0.006586160510778427, 1.040104627609253, 0.014642085880041122], [0.8281402587890625, 0.2791145443916321, 0.8740947246551514, 0.1683582365512848], [0.8653237223625183, 0.11307235062122345, 0.8575713634490967, 0.143876314163208], [0.9846587181091309, 0.11652521789073944, 0.7820221781730652, 0.1633124202489853], [0.9107496738433838, 0.22016048431396484, 1.0211989879608154, 0.08445832133293152], [0.9833457469940186, 0.08856514096260071, 0.8046919703483582, 0.2620636820793152], [0.9705270528793335, 0.016808580607175827, 0.6563172340393066, 0.2790708839893341], [0.8406693935394287, 0.16735345125198364, 0.8846079111099243, 0.27279847860336304], [0.957105278968811, 0.07565213739871979, 0.8096810579299927, 0.16623593866825104], [0.8899181485176086, 0.18695345520973206, 0.7108429670333862, 0.2769438326358795], [0.9080440998077393, 0.14104416966438293, 0.9763479232788086, 0.10297232866287231], [0.8038797378540039, 0.21278029680252075, 0.6580767631530762, 0.3093200922012329], [0.8384658694267273, 0.21153587102890015, 0.676618218421936, 0.30802637338638306], [0.9408621788024902, 0.04200815036892891, 0.7111213207244873, 0.3274080157279968], [0.6615606546401978, 0.3074386715888977, 0.8542405962944031, 0.18857811391353607], [0.7992740869522095, 0.2952626347541809, 0.9778791666030884, 0.05303778871893883], [1.000457525253296, 0.060699667781591415, 0.9569377899169922, 0.11249220371246338], [0.9100946187973022, 0.09765534102916718, 1.0501701831817627, -0.03090789169073105], [0.9058859348297119, 0.21034476161003113, 1.0467005968093872, -0.022340204566717148], [0.6906922459602356, 0.2607361674308777, 0.9418931007385254, 0.06455519795417786], [0.9470680952072144, 0.08296096324920654, 0.7873597145080566, 0.31279534101486206], [0.7743943929672241, 0.2555060088634491, 0.8511923551559448, 0.18559014797210693], [0.8840795755386353, 0.16196376085281372, 1.015209674835205, -0.02973618172109127], [0.8331042528152466, 0.18027056753635406, 0.8329649567604065, 0.22361087799072266], [0.9583555459976196, 0.07697772979736328, 0.9735144376754761, 0.010621603578329086], [0.7160120010375977, 0.2982387840747833, 0.9510098695755005, 0.07159459590911865], [0.8244237899780273, 0.23431620001792908, 0.8682907223701477, 0.1666729897260666]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8756235837936401, 0.11713951826095581]
[0.8597825765609741, 0.21806007623672485]
[0.8635329008102417, 0.17883995175361633]
[0.8532373309135437, 0.17288777232170105]
This is the real loss :  4.800401064567268
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312], [0.9300236701965332, 0.054322388023138046, 0.8331695795059204, 0.1553637683391571], [1.0179489850997925, 0.032668959349393845, 0.8417826294898987, 0.18494492769241333], [0.810563325881958, 0.1836356520652771, 1.1081351041793823, -0.06599172949790955], [0.6468219757080078, 0.3458862900733948, 1.0146214962005615, 0.027930568903684616], [0.9468859434127808, 0.0652921199798584, 0.8776399493217468, 0.14643652737140656], [0.6590524911880493, 0.27201277017593384, 0.9705872535705566, 0.050376009196043015], [0.9655159711837769, 0.11080324649810791, 0.8375571966171265, 0.20295009016990662], [0.9372574090957642, 0.2209758758544922, 1.0080554485321045, 0.008454877883195877], [0.6932350397109985, 0.2274811863899231, 0.9273480176925659, 0.07786241173744202], [1.057912826538086, 0.006586160510778427, 1.040104627609253, 0.014642085880041122], [0.8281402587890625, 0.2791145443916321, 0.8740947246551514, 0.1683582365512848], [0.8653237223625183, 0.11307235062122345, 0.8575713634490967, 0.143876314163208], [0.9846587181091309, 0.11652521789073944, 0.7820221781730652, 0.1633124202489853], [0.9107496738433838, 0.22016048431396484, 1.0211989879608154, 0.08445832133293152], [0.9833457469940186, 0.08856514096260071, 0.8046919703483582, 0.2620636820793152], [0.9705270528793335, 0.016808580607175827, 0.6563172340393066, 0.2790708839893341], [0.8406693935394287, 0.16735345125198364, 0.8846079111099243, 0.27279847860336304], [0.957105278968811, 0.07565213739871979, 0.8096810579299927, 0.16623593866825104], [0.8899181485176086, 0.18695345520973206, 0.7108429670333862, 0.2769438326358795], [0.9080440998077393, 0.14104416966438293, 0.9763479232788086, 0.10297232866287231], [0.8038797378540039, 0.21278029680252075, 0.6580767631530762, 0.3093200922012329], [0.8384658694267273, 0.21153587102890015, 0.676618218421936, 0.30802637338638306], [0.9408621788024902, 0.04200815036892891, 0.7111213207244873, 0.3274080157279968], [0.6615606546401978, 0.3074386715888977, 0.8542405962944031, 0.18857811391353607], [0.7992740869522095, 0.2952626347541809, 0.9778791666030884, 0.05303778871893883], [1.000457525253296, 0.060699667781591415, 0.9569377899169922, 0.11249220371246338], [0.9100946187973022, 0.09765534102916718, 1.0501701831817627, -0.03090789169073105], [0.9058859348297119, 0.21034476161003113, 1.0467005968093872, -0.022340204566717148], [0.6906922459602356, 0.2607361674308777, 0.9418931007385254, 0.06455519795417786], [0.9470680952072144, 0.08296096324920654, 0.7873597145080566, 0.31279534101486206], [0.7743943929672241, 0.2555060088634491, 0.8511923551559448, 0.18559014797210693], [0.8840795755386353, 0.16196376085281372, 1.015209674835205, -0.02973618172109127], [0.8331042528152466, 0.18027056753635406, 0.8329649567604065, 0.22361087799072266], [0.9583555459976196, 0.07697772979736328, 0.9735144376754761, 0.010621603578329086], [0.7160120010375977, 0.2982387840747833, 0.9510098695755005, 0.07159459590911865], [0.8244237899780273, 0.23431620001792908, 0.8682907223701477, 0.1666729897260666], [0.8756235837936401, 0.11713951826095581, 0.8597825765609741, 0.21806007623672485], [0.8635329008102417, 0.17883995175361633, 0.8532373309135437, 0.17288777232170105]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8018333911895752, 0.28720346093177795]
[0.7838162779808044, 0.1849384903907776]
[0.7613508105278015, 0.1879979372024536]
[0.6970540881156921, 0.2754090130329132]
This is the real loss :  5.00156635325402
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312], [0.9300236701965332, 0.054322388023138046, 0.8331695795059204, 0.1553637683391571], [1.0179489850997925, 0.032668959349393845, 0.8417826294898987, 0.18494492769241333], [0.810563325881958, 0.1836356520652771, 1.1081351041793823, -0.06599172949790955], [0.6468219757080078, 0.3458862900733948, 1.0146214962005615, 0.027930568903684616], [0.9468859434127808, 0.0652921199798584, 0.8776399493217468, 0.14643652737140656], [0.6590524911880493, 0.27201277017593384, 0.9705872535705566, 0.050376009196043015], [0.9655159711837769, 0.11080324649810791, 0.8375571966171265, 0.20295009016990662], [0.9372574090957642, 0.2209758758544922, 1.0080554485321045, 0.008454877883195877], [0.6932350397109985, 0.2274811863899231, 0.9273480176925659, 0.07786241173744202], [1.057912826538086, 0.006586160510778427, 1.040104627609253, 0.014642085880041122], [0.8281402587890625, 0.2791145443916321, 0.8740947246551514, 0.1683582365512848], [0.8653237223625183, 0.11307235062122345, 0.8575713634490967, 0.143876314163208], [0.9846587181091309, 0.11652521789073944, 0.7820221781730652, 0.1633124202489853], [0.9107496738433838, 0.22016048431396484, 1.0211989879608154, 0.08445832133293152], [0.9833457469940186, 0.08856514096260071, 0.8046919703483582, 0.2620636820793152], [0.9705270528793335, 0.016808580607175827, 0.6563172340393066, 0.2790708839893341], [0.8406693935394287, 0.16735345125198364, 0.8846079111099243, 0.27279847860336304], [0.957105278968811, 0.07565213739871979, 0.8096810579299927, 0.16623593866825104], [0.8899181485176086, 0.18695345520973206, 0.7108429670333862, 0.2769438326358795], [0.9080440998077393, 0.14104416966438293, 0.9763479232788086, 0.10297232866287231], [0.8038797378540039, 0.21278029680252075, 0.6580767631530762, 0.3093200922012329], [0.8384658694267273, 0.21153587102890015, 0.676618218421936, 0.30802637338638306], [0.9408621788024902, 0.04200815036892891, 0.7111213207244873, 0.3274080157279968], [0.6615606546401978, 0.3074386715888977, 0.8542405962944031, 0.18857811391353607], [0.7992740869522095, 0.2952626347541809, 0.9778791666030884, 0.05303778871893883], [1.000457525253296, 0.060699667781591415, 0.9569377899169922, 0.11249220371246338], [0.9100946187973022, 0.09765534102916718, 1.0501701831817627, -0.03090789169073105], [0.9058859348297119, 0.21034476161003113, 1.0467005968093872, -0.022340204566717148], [0.6906922459602356, 0.2607361674308777, 0.9418931007385254, 0.06455519795417786], [0.9470680952072144, 0.08296096324920654, 0.7873597145080566, 0.31279534101486206], [0.7743943929672241, 0.2555060088634491, 0.8511923551559448, 0.18559014797210693], [0.8840795755386353, 0.16196376085281372, 1.015209674835205, -0.02973618172109127], [0.8331042528152466, 0.18027056753635406, 0.8329649567604065, 0.22361087799072266], [0.9583555459976196, 0.07697772979736328, 0.9735144376754761, 0.010621603578329086], [0.7160120010375977, 0.2982387840747833, 0.9510098695755005, 0.07159459590911865], [0.8244237899780273, 0.23431620001792908, 0.8682907223701477, 0.1666729897260666], [0.8756235837936401, 0.11713951826095581, 0.8597825765609741, 0.21806007623672485], [0.8635329008102417, 0.17883995175361633, 0.8532373309135437, 0.17288777232170105], [0.8018333911895752, 0.28720346093177795, 0.7838162779808044, 0.1849384903907776], [0.7613508105278015, 0.1879979372024536, 0.6970540881156921, 0.2754090130329132]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7252262830734253, 0.3410550653934479]
[0.9039632678031921, 0.1851716786623001]
[0.8775975704193115, 0.13129699230194092]
[0.7622255086898804, 0.2576081156730652]
This is the real loss :  5.326113405637443
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312], [0.9300236701965332, 0.054322388023138046, 0.8331695795059204, 0.1553637683391571], [1.0179489850997925, 0.032668959349393845, 0.8417826294898987, 0.18494492769241333], [0.810563325881958, 0.1836356520652771, 1.1081351041793823, -0.06599172949790955], [0.6468219757080078, 0.3458862900733948, 1.0146214962005615, 0.027930568903684616], [0.9468859434127808, 0.0652921199798584, 0.8776399493217468, 0.14643652737140656], [0.6590524911880493, 0.27201277017593384, 0.9705872535705566, 0.050376009196043015], [0.9655159711837769, 0.11080324649810791, 0.8375571966171265, 0.20295009016990662], [0.9372574090957642, 0.2209758758544922, 1.0080554485321045, 0.008454877883195877], [0.6932350397109985, 0.2274811863899231, 0.9273480176925659, 0.07786241173744202], [1.057912826538086, 0.006586160510778427, 1.040104627609253, 0.014642085880041122], [0.8281402587890625, 0.2791145443916321, 0.8740947246551514, 0.1683582365512848], [0.8653237223625183, 0.11307235062122345, 0.8575713634490967, 0.143876314163208], [0.9846587181091309, 0.11652521789073944, 0.7820221781730652, 0.1633124202489853], [0.9107496738433838, 0.22016048431396484, 1.0211989879608154, 0.08445832133293152], [0.9833457469940186, 0.08856514096260071, 0.8046919703483582, 0.2620636820793152], [0.9705270528793335, 0.016808580607175827, 0.6563172340393066, 0.2790708839893341], [0.8406693935394287, 0.16735345125198364, 0.8846079111099243, 0.27279847860336304], [0.957105278968811, 0.07565213739871979, 0.8096810579299927, 0.16623593866825104], [0.8899181485176086, 0.18695345520973206, 0.7108429670333862, 0.2769438326358795], [0.9080440998077393, 0.14104416966438293, 0.9763479232788086, 0.10297232866287231], [0.8038797378540039, 0.21278029680252075, 0.6580767631530762, 0.3093200922012329], [0.8384658694267273, 0.21153587102890015, 0.676618218421936, 0.30802637338638306], [0.9408621788024902, 0.04200815036892891, 0.7111213207244873, 0.3274080157279968], [0.6615606546401978, 0.3074386715888977, 0.8542405962944031, 0.18857811391353607], [0.7992740869522095, 0.2952626347541809, 0.9778791666030884, 0.05303778871893883], [1.000457525253296, 0.060699667781591415, 0.9569377899169922, 0.11249220371246338], [0.9100946187973022, 0.09765534102916718, 1.0501701831817627, -0.03090789169073105], [0.9058859348297119, 0.21034476161003113, 1.0467005968093872, -0.022340204566717148], [0.6906922459602356, 0.2607361674308777, 0.9418931007385254, 0.06455519795417786], [0.9470680952072144, 0.08296096324920654, 0.7873597145080566, 0.31279534101486206], [0.7743943929672241, 0.2555060088634491, 0.8511923551559448, 0.18559014797210693], [0.8840795755386353, 0.16196376085281372, 1.015209674835205, -0.02973618172109127], [0.8331042528152466, 0.18027056753635406, 0.8329649567604065, 0.22361087799072266], [0.9583555459976196, 0.07697772979736328, 0.9735144376754761, 0.010621603578329086], [0.7160120010375977, 0.2982387840747833, 0.9510098695755005, 0.07159459590911865], [0.8244237899780273, 0.23431620001792908, 0.8682907223701477, 0.1666729897260666], [0.8756235837936401, 0.11713951826095581, 0.8597825765609741, 0.21806007623672485], [0.8635329008102417, 0.17883995175361633, 0.8532373309135437, 0.17288777232170105], [0.8018333911895752, 0.28720346093177795, 0.7838162779808044, 0.1849384903907776], [0.7613508105278015, 0.1879979372024536, 0.6970540881156921, 0.2754090130329132], [0.7252262830734253, 0.3410550653934479, 0.9039632678031921, 0.1851716786623001], [0.8775975704193115, 0.13129699230194092, 0.7622255086898804, 0.2576081156730652]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.969222903251648, 0.3792272210121155]
[0.6954476237297058, 0.3117865324020386]
[0.7083947062492371, 0.24843919277191162]
[0.9812209606170654, 0.017906468361616135]
This is the real loss :  5.533881383948028
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312], [0.9300236701965332, 0.054322388023138046, 0.8331695795059204, 0.1553637683391571], [1.0179489850997925, 0.032668959349393845, 0.8417826294898987, 0.18494492769241333], [0.810563325881958, 0.1836356520652771, 1.1081351041793823, -0.06599172949790955], [0.6468219757080078, 0.3458862900733948, 1.0146214962005615, 0.027930568903684616], [0.9468859434127808, 0.0652921199798584, 0.8776399493217468, 0.14643652737140656], [0.6590524911880493, 0.27201277017593384, 0.9705872535705566, 0.050376009196043015], [0.9655159711837769, 0.11080324649810791, 0.8375571966171265, 0.20295009016990662], [0.9372574090957642, 0.2209758758544922, 1.0080554485321045, 0.008454877883195877], [0.6932350397109985, 0.2274811863899231, 0.9273480176925659, 0.07786241173744202], [1.057912826538086, 0.006586160510778427, 1.040104627609253, 0.014642085880041122], [0.8281402587890625, 0.2791145443916321, 0.8740947246551514, 0.1683582365512848], [0.8653237223625183, 0.11307235062122345, 0.8575713634490967, 0.143876314163208], [0.9846587181091309, 0.11652521789073944, 0.7820221781730652, 0.1633124202489853], [0.9107496738433838, 0.22016048431396484, 1.0211989879608154, 0.08445832133293152], [0.9833457469940186, 0.08856514096260071, 0.8046919703483582, 0.2620636820793152], [0.9705270528793335, 0.016808580607175827, 0.6563172340393066, 0.2790708839893341], [0.8406693935394287, 0.16735345125198364, 0.8846079111099243, 0.27279847860336304], [0.957105278968811, 0.07565213739871979, 0.8096810579299927, 0.16623593866825104], [0.8899181485176086, 0.18695345520973206, 0.7108429670333862, 0.2769438326358795], [0.9080440998077393, 0.14104416966438293, 0.9763479232788086, 0.10297232866287231], [0.8038797378540039, 0.21278029680252075, 0.6580767631530762, 0.3093200922012329], [0.8384658694267273, 0.21153587102890015, 0.676618218421936, 0.30802637338638306], [0.9408621788024902, 0.04200815036892891, 0.7111213207244873, 0.3274080157279968], [0.6615606546401978, 0.3074386715888977, 0.8542405962944031, 0.18857811391353607], [0.7992740869522095, 0.2952626347541809, 0.9778791666030884, 0.05303778871893883], [1.000457525253296, 0.060699667781591415, 0.9569377899169922, 0.11249220371246338], [0.9100946187973022, 0.09765534102916718, 1.0501701831817627, -0.03090789169073105], [0.9058859348297119, 0.21034476161003113, 1.0467005968093872, -0.022340204566717148], [0.6906922459602356, 0.2607361674308777, 0.9418931007385254, 0.06455519795417786], [0.9470680952072144, 0.08296096324920654, 0.7873597145080566, 0.31279534101486206], [0.7743943929672241, 0.2555060088634491, 0.8511923551559448, 0.18559014797210693], [0.8840795755386353, 0.16196376085281372, 1.015209674835205, -0.02973618172109127], [0.8331042528152466, 0.18027056753635406, 0.8329649567604065, 0.22361087799072266], [0.9583555459976196, 0.07697772979736328, 0.9735144376754761, 0.010621603578329086], [0.7160120010375977, 0.2982387840747833, 0.9510098695755005, 0.07159459590911865], [0.8244237899780273, 0.23431620001792908, 0.8682907223701477, 0.1666729897260666], [0.8756235837936401, 0.11713951826095581, 0.8597825765609741, 0.21806007623672485], [0.8635329008102417, 0.17883995175361633, 0.8532373309135437, 0.17288777232170105], [0.8018333911895752, 0.28720346093177795, 0.7838162779808044, 0.1849384903907776], [0.7613508105278015, 0.1879979372024536, 0.6970540881156921, 0.2754090130329132], [0.7252262830734253, 0.3410550653934479, 0.9039632678031921, 0.1851716786623001], [0.8775975704193115, 0.13129699230194092, 0.7622255086898804, 0.2576081156730652], [0.969222903251648, 0.3792272210121155, 0.6954476237297058, 0.3117865324020386], [0.7083947062492371, 0.24843919277191162, 0.9812209606170654, 0.017906468361616135]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7311444282531738, 0.2816239595413208]
[0.8033699989318848, 0.2799971103668213]
[0.8524529933929443, 0.20604169368743896]
[0.881869912147522, 0.22198814153671265]
This is the real loss :  5.74836606066674
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312], [0.9300236701965332, 0.054322388023138046, 0.8331695795059204, 0.1553637683391571], [1.0179489850997925, 0.032668959349393845, 0.8417826294898987, 0.18494492769241333], [0.810563325881958, 0.1836356520652771, 1.1081351041793823, -0.06599172949790955], [0.6468219757080078, 0.3458862900733948, 1.0146214962005615, 0.027930568903684616], [0.9468859434127808, 0.0652921199798584, 0.8776399493217468, 0.14643652737140656], [0.6590524911880493, 0.27201277017593384, 0.9705872535705566, 0.050376009196043015], [0.9655159711837769, 0.11080324649810791, 0.8375571966171265, 0.20295009016990662], [0.9372574090957642, 0.2209758758544922, 1.0080554485321045, 0.008454877883195877], [0.6932350397109985, 0.2274811863899231, 0.9273480176925659, 0.07786241173744202], [1.057912826538086, 0.006586160510778427, 1.040104627609253, 0.014642085880041122], [0.8281402587890625, 0.2791145443916321, 0.8740947246551514, 0.1683582365512848], [0.8653237223625183, 0.11307235062122345, 0.8575713634490967, 0.143876314163208], [0.9846587181091309, 0.11652521789073944, 0.7820221781730652, 0.1633124202489853], [0.9107496738433838, 0.22016048431396484, 1.0211989879608154, 0.08445832133293152], [0.9833457469940186, 0.08856514096260071, 0.8046919703483582, 0.2620636820793152], [0.9705270528793335, 0.016808580607175827, 0.6563172340393066, 0.2790708839893341], [0.8406693935394287, 0.16735345125198364, 0.8846079111099243, 0.27279847860336304], [0.957105278968811, 0.07565213739871979, 0.8096810579299927, 0.16623593866825104], [0.8899181485176086, 0.18695345520973206, 0.7108429670333862, 0.2769438326358795], [0.9080440998077393, 0.14104416966438293, 0.9763479232788086, 0.10297232866287231], [0.8038797378540039, 0.21278029680252075, 0.6580767631530762, 0.3093200922012329], [0.8384658694267273, 0.21153587102890015, 0.676618218421936, 0.30802637338638306], [0.9408621788024902, 0.04200815036892891, 0.7111213207244873, 0.3274080157279968], [0.6615606546401978, 0.3074386715888977, 0.8542405962944031, 0.18857811391353607], [0.7992740869522095, 0.2952626347541809, 0.9778791666030884, 0.05303778871893883], [1.000457525253296, 0.060699667781591415, 0.9569377899169922, 0.11249220371246338], [0.9100946187973022, 0.09765534102916718, 1.0501701831817627, -0.03090789169073105], [0.9058859348297119, 0.21034476161003113, 1.0467005968093872, -0.022340204566717148], [0.6906922459602356, 0.2607361674308777, 0.9418931007385254, 0.06455519795417786], [0.9470680952072144, 0.08296096324920654, 0.7873597145080566, 0.31279534101486206], [0.7743943929672241, 0.2555060088634491, 0.8511923551559448, 0.18559014797210693], [0.8840795755386353, 0.16196376085281372, 1.015209674835205, -0.02973618172109127], [0.8331042528152466, 0.18027056753635406, 0.8329649567604065, 0.22361087799072266], [0.9583555459976196, 0.07697772979736328, 0.9735144376754761, 0.010621603578329086], [0.7160120010375977, 0.2982387840747833, 0.9510098695755005, 0.07159459590911865], [0.8244237899780273, 0.23431620001792908, 0.8682907223701477, 0.1666729897260666], [0.8756235837936401, 0.11713951826095581, 0.8597825765609741, 0.21806007623672485], [0.8635329008102417, 0.17883995175361633, 0.8532373309135437, 0.17288777232170105], [0.8018333911895752, 0.28720346093177795, 0.7838162779808044, 0.1849384903907776], [0.7613508105278015, 0.1879979372024536, 0.6970540881156921, 0.2754090130329132], [0.7252262830734253, 0.3410550653934479, 0.9039632678031921, 0.1851716786623001], [0.8775975704193115, 0.13129699230194092, 0.7622255086898804, 0.2576081156730652], [0.969222903251648, 0.3792272210121155, 0.6954476237297058, 0.3117865324020386], [0.7083947062492371, 0.24843919277191162, 0.9812209606170654, 0.017906468361616135], [0.7311444282531738, 0.2816239595413208, 0.8033699989318848, 0.2799971103668213], [0.8524529933929443, 0.20604169368743896, 0.881869912147522, 0.22198814153671265]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[1.0368562936782837, 0.013717826455831528]
[0.7211535573005676, 0.2732298970222473]
[0.7888091206550598, 0.1891593337059021]
[0.7968697547912598, 0.2852882444858551]
This is the real loss :  5.792989882640541
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312], [0.9300236701965332, 0.054322388023138046, 0.8331695795059204, 0.1553637683391571], [1.0179489850997925, 0.032668959349393845, 0.8417826294898987, 0.18494492769241333], [0.810563325881958, 0.1836356520652771, 1.1081351041793823, -0.06599172949790955], [0.6468219757080078, 0.3458862900733948, 1.0146214962005615, 0.027930568903684616], [0.9468859434127808, 0.0652921199798584, 0.8776399493217468, 0.14643652737140656], [0.6590524911880493, 0.27201277017593384, 0.9705872535705566, 0.050376009196043015], [0.9655159711837769, 0.11080324649810791, 0.8375571966171265, 0.20295009016990662], [0.9372574090957642, 0.2209758758544922, 1.0080554485321045, 0.008454877883195877], [0.6932350397109985, 0.2274811863899231, 0.9273480176925659, 0.07786241173744202], [1.057912826538086, 0.006586160510778427, 1.040104627609253, 0.014642085880041122], [0.8281402587890625, 0.2791145443916321, 0.8740947246551514, 0.1683582365512848], [0.8653237223625183, 0.11307235062122345, 0.8575713634490967, 0.143876314163208], [0.9846587181091309, 0.11652521789073944, 0.7820221781730652, 0.1633124202489853], [0.9107496738433838, 0.22016048431396484, 1.0211989879608154, 0.08445832133293152], [0.9833457469940186, 0.08856514096260071, 0.8046919703483582, 0.2620636820793152], [0.9705270528793335, 0.016808580607175827, 0.6563172340393066, 0.2790708839893341], [0.8406693935394287, 0.16735345125198364, 0.8846079111099243, 0.27279847860336304], [0.957105278968811, 0.07565213739871979, 0.8096810579299927, 0.16623593866825104], [0.8899181485176086, 0.18695345520973206, 0.7108429670333862, 0.2769438326358795], [0.9080440998077393, 0.14104416966438293, 0.9763479232788086, 0.10297232866287231], [0.8038797378540039, 0.21278029680252075, 0.6580767631530762, 0.3093200922012329], [0.8384658694267273, 0.21153587102890015, 0.676618218421936, 0.30802637338638306], [0.9408621788024902, 0.04200815036892891, 0.7111213207244873, 0.3274080157279968], [0.6615606546401978, 0.3074386715888977, 0.8542405962944031, 0.18857811391353607], [0.7992740869522095, 0.2952626347541809, 0.9778791666030884, 0.05303778871893883], [1.000457525253296, 0.060699667781591415, 0.9569377899169922, 0.11249220371246338], [0.9100946187973022, 0.09765534102916718, 1.0501701831817627, -0.03090789169073105], [0.9058859348297119, 0.21034476161003113, 1.0467005968093872, -0.022340204566717148], [0.6906922459602356, 0.2607361674308777, 0.9418931007385254, 0.06455519795417786], [0.9470680952072144, 0.08296096324920654, 0.7873597145080566, 0.31279534101486206], [0.7743943929672241, 0.2555060088634491, 0.8511923551559448, 0.18559014797210693], [0.8840795755386353, 0.16196376085281372, 1.015209674835205, -0.02973618172109127], [0.8331042528152466, 0.18027056753635406, 0.8329649567604065, 0.22361087799072266], [0.9583555459976196, 0.07697772979736328, 0.9735144376754761, 0.010621603578329086], [0.7160120010375977, 0.2982387840747833, 0.9510098695755005, 0.07159459590911865], [0.8244237899780273, 0.23431620001792908, 0.8682907223701477, 0.1666729897260666], [0.8756235837936401, 0.11713951826095581, 0.8597825765609741, 0.21806007623672485], [0.8635329008102417, 0.17883995175361633, 0.8532373309135437, 0.17288777232170105], [0.8018333911895752, 0.28720346093177795, 0.7838162779808044, 0.1849384903907776], [0.7613508105278015, 0.1879979372024536, 0.6970540881156921, 0.2754090130329132], [0.7252262830734253, 0.3410550653934479, 0.9039632678031921, 0.1851716786623001], [0.8775975704193115, 0.13129699230194092, 0.7622255086898804, 0.2576081156730652], [0.969222903251648, 0.3792272210121155, 0.6954476237297058, 0.3117865324020386], [0.7083947062492371, 0.24843919277191162, 0.9812209606170654, 0.017906468361616135], [0.7311444282531738, 0.2816239595413208, 0.8033699989318848, 0.2799971103668213], [0.8524529933929443, 0.20604169368743896, 0.881869912147522, 0.22198814153671265], [1.0368562936782837, 0.013717826455831528, 0.7211535573005676, 0.2732298970222473], [0.7888091206550598, 0.1891593337059021, 0.7968697547912598, 0.2852882444858551]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.949146032333374, 0.04876266047358513]
[0.910932183265686, 0.11558353900909424]
[0.8525163531303406, 0.11165854334831238]
[1.0389907360076904, 0.037465836852788925]
This is the real loss :  5.800914829596877
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312], [0.9300236701965332, 0.054322388023138046, 0.8331695795059204, 0.1553637683391571], [1.0179489850997925, 0.032668959349393845, 0.8417826294898987, 0.18494492769241333], [0.810563325881958, 0.1836356520652771, 1.1081351041793823, -0.06599172949790955], [0.6468219757080078, 0.3458862900733948, 1.0146214962005615, 0.027930568903684616], [0.9468859434127808, 0.0652921199798584, 0.8776399493217468, 0.14643652737140656], [0.6590524911880493, 0.27201277017593384, 0.9705872535705566, 0.050376009196043015], [0.9655159711837769, 0.11080324649810791, 0.8375571966171265, 0.20295009016990662], [0.9372574090957642, 0.2209758758544922, 1.0080554485321045, 0.008454877883195877], [0.6932350397109985, 0.2274811863899231, 0.9273480176925659, 0.07786241173744202], [1.057912826538086, 0.006586160510778427, 1.040104627609253, 0.014642085880041122], [0.8281402587890625, 0.2791145443916321, 0.8740947246551514, 0.1683582365512848], [0.8653237223625183, 0.11307235062122345, 0.8575713634490967, 0.143876314163208], [0.9846587181091309, 0.11652521789073944, 0.7820221781730652, 0.1633124202489853], [0.9107496738433838, 0.22016048431396484, 1.0211989879608154, 0.08445832133293152], [0.9833457469940186, 0.08856514096260071, 0.8046919703483582, 0.2620636820793152], [0.9705270528793335, 0.016808580607175827, 0.6563172340393066, 0.2790708839893341], [0.8406693935394287, 0.16735345125198364, 0.8846079111099243, 0.27279847860336304], [0.957105278968811, 0.07565213739871979, 0.8096810579299927, 0.16623593866825104], [0.8899181485176086, 0.18695345520973206, 0.7108429670333862, 0.2769438326358795], [0.9080440998077393, 0.14104416966438293, 0.9763479232788086, 0.10297232866287231], [0.8038797378540039, 0.21278029680252075, 0.6580767631530762, 0.3093200922012329], [0.8384658694267273, 0.21153587102890015, 0.676618218421936, 0.30802637338638306], [0.9408621788024902, 0.04200815036892891, 0.7111213207244873, 0.3274080157279968], [0.6615606546401978, 0.3074386715888977, 0.8542405962944031, 0.18857811391353607], [0.7992740869522095, 0.2952626347541809, 0.9778791666030884, 0.05303778871893883], [1.000457525253296, 0.060699667781591415, 0.9569377899169922, 0.11249220371246338], [0.9100946187973022, 0.09765534102916718, 1.0501701831817627, -0.03090789169073105], [0.9058859348297119, 0.21034476161003113, 1.0467005968093872, -0.022340204566717148], [0.6906922459602356, 0.2607361674308777, 0.9418931007385254, 0.06455519795417786], [0.9470680952072144, 0.08296096324920654, 0.7873597145080566, 0.31279534101486206], [0.7743943929672241, 0.2555060088634491, 0.8511923551559448, 0.18559014797210693], [0.8840795755386353, 0.16196376085281372, 1.015209674835205, -0.02973618172109127], [0.8331042528152466, 0.18027056753635406, 0.8329649567604065, 0.22361087799072266], [0.9583555459976196, 0.07697772979736328, 0.9735144376754761, 0.010621603578329086], [0.7160120010375977, 0.2982387840747833, 0.9510098695755005, 0.07159459590911865], [0.8244237899780273, 0.23431620001792908, 0.8682907223701477, 0.1666729897260666], [0.8756235837936401, 0.11713951826095581, 0.8597825765609741, 0.21806007623672485], [0.8635329008102417, 0.17883995175361633, 0.8532373309135437, 0.17288777232170105], [0.8018333911895752, 0.28720346093177795, 0.7838162779808044, 0.1849384903907776], [0.7613508105278015, 0.1879979372024536, 0.6970540881156921, 0.2754090130329132], [0.7252262830734253, 0.3410550653934479, 0.9039632678031921, 0.1851716786623001], [0.8775975704193115, 0.13129699230194092, 0.7622255086898804, 0.2576081156730652], [0.969222903251648, 0.3792272210121155, 0.6954476237297058, 0.3117865324020386], [0.7083947062492371, 0.24843919277191162, 0.9812209606170654, 0.017906468361616135], [0.7311444282531738, 0.2816239595413208, 0.8033699989318848, 0.2799971103668213], [0.8524529933929443, 0.20604169368743896, 0.881869912147522, 0.22198814153671265], [1.0368562936782837, 0.013717826455831528, 0.7211535573005676, 0.2732298970222473], [0.7888091206550598, 0.1891593337059021, 0.7968697547912598, 0.2852882444858551], [0.949146032333374, 0.04876266047358513, 0.910932183265686, 0.11558353900909424], [0.8525163531303406, 0.11165854334831238, 1.0389907360076904, 0.037465836852788925]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8955913782119751, 0.12028580904006958]
[0.7486259341239929, 0.281588077545166]
[0.9198871850967407, 0.11760495603084564]
[0.8880069851875305, 0.18898481130599976]
This is the real loss :  5.947218960151076
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312], [0.9300236701965332, 0.054322388023138046, 0.8331695795059204, 0.1553637683391571], [1.0179489850997925, 0.032668959349393845, 0.8417826294898987, 0.18494492769241333], [0.810563325881958, 0.1836356520652771, 1.1081351041793823, -0.06599172949790955], [0.6468219757080078, 0.3458862900733948, 1.0146214962005615, 0.027930568903684616], [0.9468859434127808, 0.0652921199798584, 0.8776399493217468, 0.14643652737140656], [0.6590524911880493, 0.27201277017593384, 0.9705872535705566, 0.050376009196043015], [0.9655159711837769, 0.11080324649810791, 0.8375571966171265, 0.20295009016990662], [0.9372574090957642, 0.2209758758544922, 1.0080554485321045, 0.008454877883195877], [0.6932350397109985, 0.2274811863899231, 0.9273480176925659, 0.07786241173744202], [1.057912826538086, 0.006586160510778427, 1.040104627609253, 0.014642085880041122], [0.8281402587890625, 0.2791145443916321, 0.8740947246551514, 0.1683582365512848], [0.8653237223625183, 0.11307235062122345, 0.8575713634490967, 0.143876314163208], [0.9846587181091309, 0.11652521789073944, 0.7820221781730652, 0.1633124202489853], [0.9107496738433838, 0.22016048431396484, 1.0211989879608154, 0.08445832133293152], [0.9833457469940186, 0.08856514096260071, 0.8046919703483582, 0.2620636820793152], [0.9705270528793335, 0.016808580607175827, 0.6563172340393066, 0.2790708839893341], [0.8406693935394287, 0.16735345125198364, 0.8846079111099243, 0.27279847860336304], [0.957105278968811, 0.07565213739871979, 0.8096810579299927, 0.16623593866825104], [0.8899181485176086, 0.18695345520973206, 0.7108429670333862, 0.2769438326358795], [0.9080440998077393, 0.14104416966438293, 0.9763479232788086, 0.10297232866287231], [0.8038797378540039, 0.21278029680252075, 0.6580767631530762, 0.3093200922012329], [0.8384658694267273, 0.21153587102890015, 0.676618218421936, 0.30802637338638306], [0.9408621788024902, 0.04200815036892891, 0.7111213207244873, 0.3274080157279968], [0.6615606546401978, 0.3074386715888977, 0.8542405962944031, 0.18857811391353607], [0.7992740869522095, 0.2952626347541809, 0.9778791666030884, 0.05303778871893883], [1.000457525253296, 0.060699667781591415, 0.9569377899169922, 0.11249220371246338], [0.9100946187973022, 0.09765534102916718, 1.0501701831817627, -0.03090789169073105], [0.9058859348297119, 0.21034476161003113, 1.0467005968093872, -0.022340204566717148], [0.6906922459602356, 0.2607361674308777, 0.9418931007385254, 0.06455519795417786], [0.9470680952072144, 0.08296096324920654, 0.7873597145080566, 0.31279534101486206], [0.7743943929672241, 0.2555060088634491, 0.8511923551559448, 0.18559014797210693], [0.8840795755386353, 0.16196376085281372, 1.015209674835205, -0.02973618172109127], [0.8331042528152466, 0.18027056753635406, 0.8329649567604065, 0.22361087799072266], [0.9583555459976196, 0.07697772979736328, 0.9735144376754761, 0.010621603578329086], [0.7160120010375977, 0.2982387840747833, 0.9510098695755005, 0.07159459590911865], [0.8244237899780273, 0.23431620001792908, 0.8682907223701477, 0.1666729897260666], [0.8756235837936401, 0.11713951826095581, 0.8597825765609741, 0.21806007623672485], [0.8635329008102417, 0.17883995175361633, 0.8532373309135437, 0.17288777232170105], [0.8018333911895752, 0.28720346093177795, 0.7838162779808044, 0.1849384903907776], [0.7613508105278015, 0.1879979372024536, 0.6970540881156921, 0.2754090130329132], [0.7252262830734253, 0.3410550653934479, 0.9039632678031921, 0.1851716786623001], [0.8775975704193115, 0.13129699230194092, 0.7622255086898804, 0.2576081156730652], [0.969222903251648, 0.3792272210121155, 0.6954476237297058, 0.3117865324020386], [0.7083947062492371, 0.24843919277191162, 0.9812209606170654, 0.017906468361616135], [0.7311444282531738, 0.2816239595413208, 0.8033699989318848, 0.2799971103668213], [0.8524529933929443, 0.20604169368743896, 0.881869912147522, 0.22198814153671265], [1.0368562936782837, 0.013717826455831528, 0.7211535573005676, 0.2732298970222473], [0.7888091206550598, 0.1891593337059021, 0.7968697547912598, 0.2852882444858551], [0.949146032333374, 0.04876266047358513, 0.910932183265686, 0.11558353900909424], [0.8525163531303406, 0.11165854334831238, 1.0389907360076904, 0.037465836852788925], [0.8955913782119751, 0.12028580904006958, 0.7486259341239929, 0.281588077545166], [0.9198871850967407, 0.11760495603084564, 0.8880069851875305, 0.18898481130599976]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8992315530776978, 0.08698505163192749]
[0.920428991317749, 0.07382453978061676]
[0.9235795736312866, 0.10298559069633484]
[0.8727779388427734, 0.11086001992225647]
This is the real loss :  5.956521934829652
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312], [0.9300236701965332, 0.054322388023138046, 0.8331695795059204, 0.1553637683391571], [1.0179489850997925, 0.032668959349393845, 0.8417826294898987, 0.18494492769241333], [0.810563325881958, 0.1836356520652771, 1.1081351041793823, -0.06599172949790955], [0.6468219757080078, 0.3458862900733948, 1.0146214962005615, 0.027930568903684616], [0.9468859434127808, 0.0652921199798584, 0.8776399493217468, 0.14643652737140656], [0.6590524911880493, 0.27201277017593384, 0.9705872535705566, 0.050376009196043015], [0.9655159711837769, 0.11080324649810791, 0.8375571966171265, 0.20295009016990662], [0.9372574090957642, 0.2209758758544922, 1.0080554485321045, 0.008454877883195877], [0.6932350397109985, 0.2274811863899231, 0.9273480176925659, 0.07786241173744202], [1.057912826538086, 0.006586160510778427, 1.040104627609253, 0.014642085880041122], [0.8281402587890625, 0.2791145443916321, 0.8740947246551514, 0.1683582365512848], [0.8653237223625183, 0.11307235062122345, 0.8575713634490967, 0.143876314163208], [0.9846587181091309, 0.11652521789073944, 0.7820221781730652, 0.1633124202489853], [0.9107496738433838, 0.22016048431396484, 1.0211989879608154, 0.08445832133293152], [0.9833457469940186, 0.08856514096260071, 0.8046919703483582, 0.2620636820793152], [0.9705270528793335, 0.016808580607175827, 0.6563172340393066, 0.2790708839893341], [0.8406693935394287, 0.16735345125198364, 0.8846079111099243, 0.27279847860336304], [0.957105278968811, 0.07565213739871979, 0.8096810579299927, 0.16623593866825104], [0.8899181485176086, 0.18695345520973206, 0.7108429670333862, 0.2769438326358795], [0.9080440998077393, 0.14104416966438293, 0.9763479232788086, 0.10297232866287231], [0.8038797378540039, 0.21278029680252075, 0.6580767631530762, 0.3093200922012329], [0.8384658694267273, 0.21153587102890015, 0.676618218421936, 0.30802637338638306], [0.9408621788024902, 0.04200815036892891, 0.7111213207244873, 0.3274080157279968], [0.6615606546401978, 0.3074386715888977, 0.8542405962944031, 0.18857811391353607], [0.7992740869522095, 0.2952626347541809, 0.9778791666030884, 0.05303778871893883], [1.000457525253296, 0.060699667781591415, 0.9569377899169922, 0.11249220371246338], [0.9100946187973022, 0.09765534102916718, 1.0501701831817627, -0.03090789169073105], [0.9058859348297119, 0.21034476161003113, 1.0467005968093872, -0.022340204566717148], [0.6906922459602356, 0.2607361674308777, 0.9418931007385254, 0.06455519795417786], [0.9470680952072144, 0.08296096324920654, 0.7873597145080566, 0.31279534101486206], [0.7743943929672241, 0.2555060088634491, 0.8511923551559448, 0.18559014797210693], [0.8840795755386353, 0.16196376085281372, 1.015209674835205, -0.02973618172109127], [0.8331042528152466, 0.18027056753635406, 0.8329649567604065, 0.22361087799072266], [0.9583555459976196, 0.07697772979736328, 0.9735144376754761, 0.010621603578329086], [0.7160120010375977, 0.2982387840747833, 0.9510098695755005, 0.07159459590911865], [0.8244237899780273, 0.23431620001792908, 0.8682907223701477, 0.1666729897260666], [0.8756235837936401, 0.11713951826095581, 0.8597825765609741, 0.21806007623672485], [0.8635329008102417, 0.17883995175361633, 0.8532373309135437, 0.17288777232170105], [0.8018333911895752, 0.28720346093177795, 0.7838162779808044, 0.1849384903907776], [0.7613508105278015, 0.1879979372024536, 0.6970540881156921, 0.2754090130329132], [0.7252262830734253, 0.3410550653934479, 0.9039632678031921, 0.1851716786623001], [0.8775975704193115, 0.13129699230194092, 0.7622255086898804, 0.2576081156730652], [0.969222903251648, 0.3792272210121155, 0.6954476237297058, 0.3117865324020386], [0.7083947062492371, 0.24843919277191162, 0.9812209606170654, 0.017906468361616135], [0.7311444282531738, 0.2816239595413208, 0.8033699989318848, 0.2799971103668213], [0.8524529933929443, 0.20604169368743896, 0.881869912147522, 0.22198814153671265], [1.0368562936782837, 0.013717826455831528, 0.7211535573005676, 0.2732298970222473], [0.7888091206550598, 0.1891593337059021, 0.7968697547912598, 0.2852882444858551], [0.949146032333374, 0.04876266047358513, 0.910932183265686, 0.11558353900909424], [0.8525163531303406, 0.11165854334831238, 1.0389907360076904, 0.037465836852788925], [0.8955913782119751, 0.12028580904006958, 0.7486259341239929, 0.281588077545166], [0.9198871850967407, 0.11760495603084564, 0.8880069851875305, 0.18898481130599976], [0.8992315530776978, 0.08698505163192749, 0.920428991317749, 0.07382453978061676], [0.9235795736312866, 0.10298559069633484, 0.8727779388427734, 0.11086001992225647]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[1.0485525131225586, 0.11110140383243561]
[0.7827389240264893, 0.1933434158563614]
[1.1595699787139893, -0.08905604481697083]
[0.7303086519241333, 0.2916055917739868]
This is the real loss :  6.140176526270807
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312], [0.9300236701965332, 0.054322388023138046, 0.8331695795059204, 0.1553637683391571], [1.0179489850997925, 0.032668959349393845, 0.8417826294898987, 0.18494492769241333], [0.810563325881958, 0.1836356520652771, 1.1081351041793823, -0.06599172949790955], [0.6468219757080078, 0.3458862900733948, 1.0146214962005615, 0.027930568903684616], [0.9468859434127808, 0.0652921199798584, 0.8776399493217468, 0.14643652737140656], [0.6590524911880493, 0.27201277017593384, 0.9705872535705566, 0.050376009196043015], [0.9655159711837769, 0.11080324649810791, 0.8375571966171265, 0.20295009016990662], [0.9372574090957642, 0.2209758758544922, 1.0080554485321045, 0.008454877883195877], [0.6932350397109985, 0.2274811863899231, 0.9273480176925659, 0.07786241173744202], [1.057912826538086, 0.006586160510778427, 1.040104627609253, 0.014642085880041122], [0.8281402587890625, 0.2791145443916321, 0.8740947246551514, 0.1683582365512848], [0.8653237223625183, 0.11307235062122345, 0.8575713634490967, 0.143876314163208], [0.9846587181091309, 0.11652521789073944, 0.7820221781730652, 0.1633124202489853], [0.9107496738433838, 0.22016048431396484, 1.0211989879608154, 0.08445832133293152], [0.9833457469940186, 0.08856514096260071, 0.8046919703483582, 0.2620636820793152], [0.9705270528793335, 0.016808580607175827, 0.6563172340393066, 0.2790708839893341], [0.8406693935394287, 0.16735345125198364, 0.8846079111099243, 0.27279847860336304], [0.957105278968811, 0.07565213739871979, 0.8096810579299927, 0.16623593866825104], [0.8899181485176086, 0.18695345520973206, 0.7108429670333862, 0.2769438326358795], [0.9080440998077393, 0.14104416966438293, 0.9763479232788086, 0.10297232866287231], [0.8038797378540039, 0.21278029680252075, 0.6580767631530762, 0.3093200922012329], [0.8384658694267273, 0.21153587102890015, 0.676618218421936, 0.30802637338638306], [0.9408621788024902, 0.04200815036892891, 0.7111213207244873, 0.3274080157279968], [0.6615606546401978, 0.3074386715888977, 0.8542405962944031, 0.18857811391353607], [0.7992740869522095, 0.2952626347541809, 0.9778791666030884, 0.05303778871893883], [1.000457525253296, 0.060699667781591415, 0.9569377899169922, 0.11249220371246338], [0.9100946187973022, 0.09765534102916718, 1.0501701831817627, -0.03090789169073105], [0.9058859348297119, 0.21034476161003113, 1.0467005968093872, -0.022340204566717148], [0.6906922459602356, 0.2607361674308777, 0.9418931007385254, 0.06455519795417786], [0.9470680952072144, 0.08296096324920654, 0.7873597145080566, 0.31279534101486206], [0.7743943929672241, 0.2555060088634491, 0.8511923551559448, 0.18559014797210693], [0.8840795755386353, 0.16196376085281372, 1.015209674835205, -0.02973618172109127], [0.8331042528152466, 0.18027056753635406, 0.8329649567604065, 0.22361087799072266], [0.9583555459976196, 0.07697772979736328, 0.9735144376754761, 0.010621603578329086], [0.7160120010375977, 0.2982387840747833, 0.9510098695755005, 0.07159459590911865], [0.8244237899780273, 0.23431620001792908, 0.8682907223701477, 0.1666729897260666], [0.8756235837936401, 0.11713951826095581, 0.8597825765609741, 0.21806007623672485], [0.8635329008102417, 0.17883995175361633, 0.8532373309135437, 0.17288777232170105], [0.8018333911895752, 0.28720346093177795, 0.7838162779808044, 0.1849384903907776], [0.7613508105278015, 0.1879979372024536, 0.6970540881156921, 0.2754090130329132], [0.7252262830734253, 0.3410550653934479, 0.9039632678031921, 0.1851716786623001], [0.8775975704193115, 0.13129699230194092, 0.7622255086898804, 0.2576081156730652], [0.969222903251648, 0.3792272210121155, 0.6954476237297058, 0.3117865324020386], [0.7083947062492371, 0.24843919277191162, 0.9812209606170654, 0.017906468361616135], [0.7311444282531738, 0.2816239595413208, 0.8033699989318848, 0.2799971103668213], [0.8524529933929443, 0.20604169368743896, 0.881869912147522, 0.22198814153671265], [1.0368562936782837, 0.013717826455831528, 0.7211535573005676, 0.2732298970222473], [0.7888091206550598, 0.1891593337059021, 0.7968697547912598, 0.2852882444858551], [0.949146032333374, 0.04876266047358513, 0.910932183265686, 0.11558353900909424], [0.8525163531303406, 0.11165854334831238, 1.0389907360076904, 0.037465836852788925], [0.8955913782119751, 0.12028580904006958, 0.7486259341239929, 0.281588077545166], [0.9198871850967407, 0.11760495603084564, 0.8880069851875305, 0.18898481130599976], [0.8992315530776978, 0.08698505163192749, 0.920428991317749, 0.07382453978061676], [0.9235795736312866, 0.10298559069633484, 0.8727779388427734, 0.11086001992225647], [1.0485525131225586, 0.11110140383243561, 0.7827389240264893, 0.1933434158563614], [1.1595699787139893, -0.08905604481697083, 0.7303086519241333, 0.2916055917739868]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8500824570655823, 0.17721334099769592]
[0.9416561126708984, 0.4081572890281677]
[0.8483530282974243, 0.11827713251113892]
[0.724992036819458, 0.3550989031791687]
This is the real loss :  6.197999916039407
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312], [0.9300236701965332, 0.054322388023138046, 0.8331695795059204, 0.1553637683391571], [1.0179489850997925, 0.032668959349393845, 0.8417826294898987, 0.18494492769241333], [0.810563325881958, 0.1836356520652771, 1.1081351041793823, -0.06599172949790955], [0.6468219757080078, 0.3458862900733948, 1.0146214962005615, 0.027930568903684616], [0.9468859434127808, 0.0652921199798584, 0.8776399493217468, 0.14643652737140656], [0.6590524911880493, 0.27201277017593384, 0.9705872535705566, 0.050376009196043015], [0.9655159711837769, 0.11080324649810791, 0.8375571966171265, 0.20295009016990662], [0.9372574090957642, 0.2209758758544922, 1.0080554485321045, 0.008454877883195877], [0.6932350397109985, 0.2274811863899231, 0.9273480176925659, 0.07786241173744202], [1.057912826538086, 0.006586160510778427, 1.040104627609253, 0.014642085880041122], [0.8281402587890625, 0.2791145443916321, 0.8740947246551514, 0.1683582365512848], [0.8653237223625183, 0.11307235062122345, 0.8575713634490967, 0.143876314163208], [0.9846587181091309, 0.11652521789073944, 0.7820221781730652, 0.1633124202489853], [0.9107496738433838, 0.22016048431396484, 1.0211989879608154, 0.08445832133293152], [0.9833457469940186, 0.08856514096260071, 0.8046919703483582, 0.2620636820793152], [0.9705270528793335, 0.016808580607175827, 0.6563172340393066, 0.2790708839893341], [0.8406693935394287, 0.16735345125198364, 0.8846079111099243, 0.27279847860336304], [0.957105278968811, 0.07565213739871979, 0.8096810579299927, 0.16623593866825104], [0.8899181485176086, 0.18695345520973206, 0.7108429670333862, 0.2769438326358795], [0.9080440998077393, 0.14104416966438293, 0.9763479232788086, 0.10297232866287231], [0.8038797378540039, 0.21278029680252075, 0.6580767631530762, 0.3093200922012329], [0.8384658694267273, 0.21153587102890015, 0.676618218421936, 0.30802637338638306], [0.9408621788024902, 0.04200815036892891, 0.7111213207244873, 0.3274080157279968], [0.6615606546401978, 0.3074386715888977, 0.8542405962944031, 0.18857811391353607], [0.7992740869522095, 0.2952626347541809, 0.9778791666030884, 0.05303778871893883], [1.000457525253296, 0.060699667781591415, 0.9569377899169922, 0.11249220371246338], [0.9100946187973022, 0.09765534102916718, 1.0501701831817627, -0.03090789169073105], [0.9058859348297119, 0.21034476161003113, 1.0467005968093872, -0.022340204566717148], [0.6906922459602356, 0.2607361674308777, 0.9418931007385254, 0.06455519795417786], [0.9470680952072144, 0.08296096324920654, 0.7873597145080566, 0.31279534101486206], [0.7743943929672241, 0.2555060088634491, 0.8511923551559448, 0.18559014797210693], [0.8840795755386353, 0.16196376085281372, 1.015209674835205, -0.02973618172109127], [0.8331042528152466, 0.18027056753635406, 0.8329649567604065, 0.22361087799072266], [0.9583555459976196, 0.07697772979736328, 0.9735144376754761, 0.010621603578329086], [0.7160120010375977, 0.2982387840747833, 0.9510098695755005, 0.07159459590911865], [0.8244237899780273, 0.23431620001792908, 0.8682907223701477, 0.1666729897260666], [0.8756235837936401, 0.11713951826095581, 0.8597825765609741, 0.21806007623672485], [0.8635329008102417, 0.17883995175361633, 0.8532373309135437, 0.17288777232170105], [0.8018333911895752, 0.28720346093177795, 0.7838162779808044, 0.1849384903907776], [0.7613508105278015, 0.1879979372024536, 0.6970540881156921, 0.2754090130329132], [0.7252262830734253, 0.3410550653934479, 0.9039632678031921, 0.1851716786623001], [0.8775975704193115, 0.13129699230194092, 0.7622255086898804, 0.2576081156730652], [0.969222903251648, 0.3792272210121155, 0.6954476237297058, 0.3117865324020386], [0.7083947062492371, 0.24843919277191162, 0.9812209606170654, 0.017906468361616135], [0.7311444282531738, 0.2816239595413208, 0.8033699989318848, 0.2799971103668213], [0.8524529933929443, 0.20604169368743896, 0.881869912147522, 0.22198814153671265], [1.0368562936782837, 0.013717826455831528, 0.7211535573005676, 0.2732298970222473], [0.7888091206550598, 0.1891593337059021, 0.7968697547912598, 0.2852882444858551], [0.949146032333374, 0.04876266047358513, 0.910932183265686, 0.11558353900909424], [0.8525163531303406, 0.11165854334831238, 1.0389907360076904, 0.037465836852788925], [0.8955913782119751, 0.12028580904006958, 0.7486259341239929, 0.281588077545166], [0.9198871850967407, 0.11760495603084564, 0.8880069851875305, 0.18898481130599976], [0.8992315530776978, 0.08698505163192749, 0.920428991317749, 0.07382453978061676], [0.9235795736312866, 0.10298559069633484, 0.8727779388427734, 0.11086001992225647], [1.0485525131225586, 0.11110140383243561, 0.7827389240264893, 0.1933434158563614], [1.1595699787139893, -0.08905604481697083, 0.7303086519241333, 0.2916055917739868], [0.8500824570655823, 0.17721334099769592, 0.9416561126708984, 0.4081572890281677], [0.8483530282974243, 0.11827713251113892, 0.724992036819458, 0.3550989031791687]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.7318600416183472, 0.20816242694854736]
[1.104979157447815, 0.04183315858244896]
[1.0369869470596313, 0.10709759593009949]
[0.9007052183151245, 0.06840313971042633]
This is the real loss :  6.217422121204436
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312], [0.9300236701965332, 0.054322388023138046, 0.8331695795059204, 0.1553637683391571], [1.0179489850997925, 0.032668959349393845, 0.8417826294898987, 0.18494492769241333], [0.810563325881958, 0.1836356520652771, 1.1081351041793823, -0.06599172949790955], [0.6468219757080078, 0.3458862900733948, 1.0146214962005615, 0.027930568903684616], [0.9468859434127808, 0.0652921199798584, 0.8776399493217468, 0.14643652737140656], [0.6590524911880493, 0.27201277017593384, 0.9705872535705566, 0.050376009196043015], [0.9655159711837769, 0.11080324649810791, 0.8375571966171265, 0.20295009016990662], [0.9372574090957642, 0.2209758758544922, 1.0080554485321045, 0.008454877883195877], [0.6932350397109985, 0.2274811863899231, 0.9273480176925659, 0.07786241173744202], [1.057912826538086, 0.006586160510778427, 1.040104627609253, 0.014642085880041122], [0.8281402587890625, 0.2791145443916321, 0.8740947246551514, 0.1683582365512848], [0.8653237223625183, 0.11307235062122345, 0.8575713634490967, 0.143876314163208], [0.9846587181091309, 0.11652521789073944, 0.7820221781730652, 0.1633124202489853], [0.9107496738433838, 0.22016048431396484, 1.0211989879608154, 0.08445832133293152], [0.9833457469940186, 0.08856514096260071, 0.8046919703483582, 0.2620636820793152], [0.9705270528793335, 0.016808580607175827, 0.6563172340393066, 0.2790708839893341], [0.8406693935394287, 0.16735345125198364, 0.8846079111099243, 0.27279847860336304], [0.957105278968811, 0.07565213739871979, 0.8096810579299927, 0.16623593866825104], [0.8899181485176086, 0.18695345520973206, 0.7108429670333862, 0.2769438326358795], [0.9080440998077393, 0.14104416966438293, 0.9763479232788086, 0.10297232866287231], [0.8038797378540039, 0.21278029680252075, 0.6580767631530762, 0.3093200922012329], [0.8384658694267273, 0.21153587102890015, 0.676618218421936, 0.30802637338638306], [0.9408621788024902, 0.04200815036892891, 0.7111213207244873, 0.3274080157279968], [0.6615606546401978, 0.3074386715888977, 0.8542405962944031, 0.18857811391353607], [0.7992740869522095, 0.2952626347541809, 0.9778791666030884, 0.05303778871893883], [1.000457525253296, 0.060699667781591415, 0.9569377899169922, 0.11249220371246338], [0.9100946187973022, 0.09765534102916718, 1.0501701831817627, -0.03090789169073105], [0.9058859348297119, 0.21034476161003113, 1.0467005968093872, -0.022340204566717148], [0.6906922459602356, 0.2607361674308777, 0.9418931007385254, 0.06455519795417786], [0.9470680952072144, 0.08296096324920654, 0.7873597145080566, 0.31279534101486206], [0.7743943929672241, 0.2555060088634491, 0.8511923551559448, 0.18559014797210693], [0.8840795755386353, 0.16196376085281372, 1.015209674835205, -0.02973618172109127], [0.8331042528152466, 0.18027056753635406, 0.8329649567604065, 0.22361087799072266], [0.9583555459976196, 0.07697772979736328, 0.9735144376754761, 0.010621603578329086], [0.7160120010375977, 0.2982387840747833, 0.9510098695755005, 0.07159459590911865], [0.8244237899780273, 0.23431620001792908, 0.8682907223701477, 0.1666729897260666], [0.8756235837936401, 0.11713951826095581, 0.8597825765609741, 0.21806007623672485], [0.8635329008102417, 0.17883995175361633, 0.8532373309135437, 0.17288777232170105], [0.8018333911895752, 0.28720346093177795, 0.7838162779808044, 0.1849384903907776], [0.7613508105278015, 0.1879979372024536, 0.6970540881156921, 0.2754090130329132], [0.7252262830734253, 0.3410550653934479, 0.9039632678031921, 0.1851716786623001], [0.8775975704193115, 0.13129699230194092, 0.7622255086898804, 0.2576081156730652], [0.969222903251648, 0.3792272210121155, 0.6954476237297058, 0.3117865324020386], [0.7083947062492371, 0.24843919277191162, 0.9812209606170654, 0.017906468361616135], [0.7311444282531738, 0.2816239595413208, 0.8033699989318848, 0.2799971103668213], [0.8524529933929443, 0.20604169368743896, 0.881869912147522, 0.22198814153671265], [1.0368562936782837, 0.013717826455831528, 0.7211535573005676, 0.2732298970222473], [0.7888091206550598, 0.1891593337059021, 0.7968697547912598, 0.2852882444858551], [0.949146032333374, 0.04876266047358513, 0.910932183265686, 0.11558353900909424], [0.8525163531303406, 0.11165854334831238, 1.0389907360076904, 0.037465836852788925], [0.8955913782119751, 0.12028580904006958, 0.7486259341239929, 0.281588077545166], [0.9198871850967407, 0.11760495603084564, 0.8880069851875305, 0.18898481130599976], [0.8992315530776978, 0.08698505163192749, 0.920428991317749, 0.07382453978061676], [0.9235795736312866, 0.10298559069633484, 0.8727779388427734, 0.11086001992225647], [1.0485525131225586, 0.11110140383243561, 0.7827389240264893, 0.1933434158563614], [1.1595699787139893, -0.08905604481697083, 0.7303086519241333, 0.2916055917739868], [0.8500824570655823, 0.17721334099769592, 0.9416561126708984, 0.4081572890281677], [0.8483530282974243, 0.11827713251113892, 0.724992036819458, 0.3550989031791687], [0.7318600416183472, 0.20816242694854736, 1.104979157447815, 0.04183315858244896], [1.0369869470596313, 0.10709759593009949, 0.9007052183151245, 0.06840313971042633]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.8946923017501831, 0.2996341586112976]
[0.6930220723152161, 0.25113052129745483]
[0.7908887267112732, 0.2202128767967224]
[0.6633474230766296, 0.36831367015838623]
This is the real loss :  6.441109591163695
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312], [0.9300236701965332, 0.054322388023138046, 0.8331695795059204, 0.1553637683391571], [1.0179489850997925, 0.032668959349393845, 0.8417826294898987, 0.18494492769241333], [0.810563325881958, 0.1836356520652771, 1.1081351041793823, -0.06599172949790955], [0.6468219757080078, 0.3458862900733948, 1.0146214962005615, 0.027930568903684616], [0.9468859434127808, 0.0652921199798584, 0.8776399493217468, 0.14643652737140656], [0.6590524911880493, 0.27201277017593384, 0.9705872535705566, 0.050376009196043015], [0.9655159711837769, 0.11080324649810791, 0.8375571966171265, 0.20295009016990662], [0.9372574090957642, 0.2209758758544922, 1.0080554485321045, 0.008454877883195877], [0.6932350397109985, 0.2274811863899231, 0.9273480176925659, 0.07786241173744202], [1.057912826538086, 0.006586160510778427, 1.040104627609253, 0.014642085880041122], [0.8281402587890625, 0.2791145443916321, 0.8740947246551514, 0.1683582365512848], [0.8653237223625183, 0.11307235062122345, 0.8575713634490967, 0.143876314163208], [0.9846587181091309, 0.11652521789073944, 0.7820221781730652, 0.1633124202489853], [0.9107496738433838, 0.22016048431396484, 1.0211989879608154, 0.08445832133293152], [0.9833457469940186, 0.08856514096260071, 0.8046919703483582, 0.2620636820793152], [0.9705270528793335, 0.016808580607175827, 0.6563172340393066, 0.2790708839893341], [0.8406693935394287, 0.16735345125198364, 0.8846079111099243, 0.27279847860336304], [0.957105278968811, 0.07565213739871979, 0.8096810579299927, 0.16623593866825104], [0.8899181485176086, 0.18695345520973206, 0.7108429670333862, 0.2769438326358795], [0.9080440998077393, 0.14104416966438293, 0.9763479232788086, 0.10297232866287231], [0.8038797378540039, 0.21278029680252075, 0.6580767631530762, 0.3093200922012329], [0.8384658694267273, 0.21153587102890015, 0.676618218421936, 0.30802637338638306], [0.9408621788024902, 0.04200815036892891, 0.7111213207244873, 0.3274080157279968], [0.6615606546401978, 0.3074386715888977, 0.8542405962944031, 0.18857811391353607], [0.7992740869522095, 0.2952626347541809, 0.9778791666030884, 0.05303778871893883], [1.000457525253296, 0.060699667781591415, 0.9569377899169922, 0.11249220371246338], [0.9100946187973022, 0.09765534102916718, 1.0501701831817627, -0.03090789169073105], [0.9058859348297119, 0.21034476161003113, 1.0467005968093872, -0.022340204566717148], [0.6906922459602356, 0.2607361674308777, 0.9418931007385254, 0.06455519795417786], [0.9470680952072144, 0.08296096324920654, 0.7873597145080566, 0.31279534101486206], [0.7743943929672241, 0.2555060088634491, 0.8511923551559448, 0.18559014797210693], [0.8840795755386353, 0.16196376085281372, 1.015209674835205, -0.02973618172109127], [0.8331042528152466, 0.18027056753635406, 0.8329649567604065, 0.22361087799072266], [0.9583555459976196, 0.07697772979736328, 0.9735144376754761, 0.010621603578329086], [0.7160120010375977, 0.2982387840747833, 0.9510098695755005, 0.07159459590911865], [0.8244237899780273, 0.23431620001792908, 0.8682907223701477, 0.1666729897260666], [0.8756235837936401, 0.11713951826095581, 0.8597825765609741, 0.21806007623672485], [0.8635329008102417, 0.17883995175361633, 0.8532373309135437, 0.17288777232170105], [0.8018333911895752, 0.28720346093177795, 0.7838162779808044, 0.1849384903907776], [0.7613508105278015, 0.1879979372024536, 0.6970540881156921, 0.2754090130329132], [0.7252262830734253, 0.3410550653934479, 0.9039632678031921, 0.1851716786623001], [0.8775975704193115, 0.13129699230194092, 0.7622255086898804, 0.2576081156730652], [0.969222903251648, 0.3792272210121155, 0.6954476237297058, 0.3117865324020386], [0.7083947062492371, 0.24843919277191162, 0.9812209606170654, 0.017906468361616135], [0.7311444282531738, 0.2816239595413208, 0.8033699989318848, 0.2799971103668213], [0.8524529933929443, 0.20604169368743896, 0.881869912147522, 0.22198814153671265], [1.0368562936782837, 0.013717826455831528, 0.7211535573005676, 0.2732298970222473], [0.7888091206550598, 0.1891593337059021, 0.7968697547912598, 0.2852882444858551], [0.949146032333374, 0.04876266047358513, 0.910932183265686, 0.11558353900909424], [0.8525163531303406, 0.11165854334831238, 1.0389907360076904, 0.037465836852788925], [0.8955913782119751, 0.12028580904006958, 0.7486259341239929, 0.281588077545166], [0.9198871850967407, 0.11760495603084564, 0.8880069851875305, 0.18898481130599976], [0.8992315530776978, 0.08698505163192749, 0.920428991317749, 0.07382453978061676], [0.9235795736312866, 0.10298559069633484, 0.8727779388427734, 0.11086001992225647], [1.0485525131225586, 0.11110140383243561, 0.7827389240264893, 0.1933434158563614], [1.1595699787139893, -0.08905604481697083, 0.7303086519241333, 0.2916055917739868], [0.8500824570655823, 0.17721334099769592, 0.9416561126708984, 0.4081572890281677], [0.8483530282974243, 0.11827713251113892, 0.724992036819458, 0.3550989031791687], [0.7318600416183472, 0.20816242694854736, 1.104979157447815, 0.04183315858244896], [1.0369869470596313, 0.10709759593009949, 0.9007052183151245, 0.06840313971042633], [0.8946923017501831, 0.2996341586112976, 0.6930220723152161, 0.25113052129745483], [0.7908887267112732, 0.2202128767967224, 0.6633474230766296, 0.36831367015838623]]
Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[1.063140630722046, -0.009017307311296463]
[0.9030185341835022, 0.31602734327316284]
[0.716485857963562, 0.2444121241569519]
[0.882757306098938, 0.24701711535453796]
This is the real loss :  6.482138038612902
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312], [0.9300236701965332, 0.054322388023138046, 0.8331695795059204, 0.1553637683391571], [1.0179489850997925, 0.032668959349393845, 0.8417826294898987, 0.18494492769241333], [0.810563325881958, 0.1836356520652771, 1.1081351041793823, -0.06599172949790955], [0.6468219757080078, 0.3458862900733948, 1.0146214962005615, 0.027930568903684616], [0.9468859434127808, 0.0652921199798584, 0.8776399493217468, 0.14643652737140656], [0.6590524911880493, 0.27201277017593384, 0.9705872535705566, 0.050376009196043015], [0.9655159711837769, 0.11080324649810791, 0.8375571966171265, 0.20295009016990662], [0.9372574090957642, 0.2209758758544922, 1.0080554485321045, 0.008454877883195877], [0.6932350397109985, 0.2274811863899231, 0.9273480176925659, 0.07786241173744202], [1.057912826538086, 0.006586160510778427, 1.040104627609253, 0.014642085880041122], [0.8281402587890625, 0.2791145443916321, 0.8740947246551514, 0.1683582365512848], [0.8653237223625183, 0.11307235062122345, 0.8575713634490967, 0.143876314163208], [0.9846587181091309, 0.11652521789073944, 0.7820221781730652, 0.1633124202489853], [0.9107496738433838, 0.22016048431396484, 1.0211989879608154, 0.08445832133293152], [0.9833457469940186, 0.08856514096260071, 0.8046919703483582, 0.2620636820793152], [0.9705270528793335, 0.016808580607175827, 0.6563172340393066, 0.2790708839893341], [0.8406693935394287, 0.16735345125198364, 0.8846079111099243, 0.27279847860336304], [0.957105278968811, 0.07565213739871979, 0.8096810579299927, 0.16623593866825104], [0.8899181485176086, 0.18695345520973206, 0.7108429670333862, 0.2769438326358795], [0.9080440998077393, 0.14104416966438293, 0.9763479232788086, 0.10297232866287231], [0.8038797378540039, 0.21278029680252075, 0.6580767631530762, 0.3093200922012329], [0.8384658694267273, 0.21153587102890015, 0.676618218421936, 0.30802637338638306], [0.9408621788024902, 0.04200815036892891, 0.7111213207244873, 0.3274080157279968], [0.6615606546401978, 0.3074386715888977, 0.8542405962944031, 0.18857811391353607], [0.7992740869522095, 0.2952626347541809, 0.9778791666030884, 0.05303778871893883], [1.000457525253296, 0.060699667781591415, 0.9569377899169922, 0.11249220371246338], [0.9100946187973022, 0.09765534102916718, 1.0501701831817627, -0.03090789169073105], [0.9058859348297119, 0.21034476161003113, 1.0467005968093872, -0.022340204566717148], [0.6906922459602356, 0.2607361674308777, 0.9418931007385254, 0.06455519795417786], [0.9470680952072144, 0.08296096324920654, 0.7873597145080566, 0.31279534101486206], [0.7743943929672241, 0.2555060088634491, 0.8511923551559448, 0.18559014797210693], [0.8840795755386353, 0.16196376085281372, 1.015209674835205, -0.02973618172109127], [0.8331042528152466, 0.18027056753635406, 0.8329649567604065, 0.22361087799072266], [0.9583555459976196, 0.07697772979736328, 0.9735144376754761, 0.010621603578329086], [0.7160120010375977, 0.2982387840747833, 0.9510098695755005, 0.07159459590911865], [0.8244237899780273, 0.23431620001792908, 0.8682907223701477, 0.1666729897260666], [0.8756235837936401, 0.11713951826095581, 0.8597825765609741, 0.21806007623672485], [0.8635329008102417, 0.17883995175361633, 0.8532373309135437, 0.17288777232170105], [0.8018333911895752, 0.28720346093177795, 0.7838162779808044, 0.1849384903907776], [0.7613508105278015, 0.1879979372024536, 0.6970540881156921, 0.2754090130329132], [0.7252262830734253, 0.3410550653934479, 0.9039632678031921, 0.1851716786623001], [0.8775975704193115, 0.13129699230194092, 0.7622255086898804, 0.2576081156730652], [0.969222903251648, 0.3792272210121155, 0.6954476237297058, 0.3117865324020386], [0.7083947062492371, 0.24843919277191162, 0.9812209606170654, 0.017906468361616135], [0.7311444282531738, 0.2816239595413208, 0.8033699989318848, 0.2799971103668213], [0.8524529933929443, 0.20604169368743896, 0.881869912147522, 0.22198814153671265], [1.0368562936782837, 0.013717826455831528, 0.7211535573005676, 0.2732298970222473], [0.7888091206550598, 0.1891593337059021, 0.7968697547912598, 0.2852882444858551], [0.949146032333374, 0.04876266047358513, 0.910932183265686, 0.11558353900909424], [0.8525163531303406, 0.11165854334831238, 1.0389907360076904, 0.037465836852788925], [0.8955913782119751, 0.12028580904006958, 0.7486259341239929, 0.281588077545166], [0.9198871850967407, 0.11760495603084564, 0.8880069851875305, 0.18898481130599976], [0.8992315530776978, 0.08698505163192749, 0.920428991317749, 0.07382453978061676], [0.9235795736312866, 0.10298559069633484, 0.8727779388427734, 0.11086001992225647], [1.0485525131225586, 0.11110140383243561, 0.7827389240264893, 0.1933434158563614], [1.1595699787139893, -0.08905604481697083, 0.7303086519241333, 0.2916055917739868], [0.8500824570655823, 0.17721334099769592, 0.9416561126708984, 0.4081572890281677], [0.8483530282974243, 0.11827713251113892, 0.724992036819458, 0.3550989031791687], [0.7318600416183472, 0.20816242694854736, 1.104979157447815, 0.04183315858244896], [1.0369869470596313, 0.10709759593009949, 0.9007052183151245, 0.06840313971042633], [0.8946923017501831, 0.2996341586112976, 0.6930220723152161, 0.25113052129745483], [0.7908887267112732, 0.2202128767967224, 0.6633474230766296, 0.36831367015838623], [1.063140630722046, -0.009017307311296463, 0.9030185341835022, 0.31602734327316284], [0.716485857963562, 0.2444121241569519, 0.882757306098938, 0.24701711535453796]]checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([0., 1.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3
checkpoint 1
Label is:  tensor([1., 0.])
feature count: 11
checkpoint 3

Logits shape: torch.Size([4, 2])
Labels shape: torch.Size([4, 2])
[0.9329142570495605, 0.0986853539943695]
[0.8481824994087219, 0.15331369638442993]
[0.6938360929489136, 0.20301750302314758]
[0.7464874386787415, 0.2771446406841278]
This is the real loss :  6.815293819643557
val_targets: [array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32), array([1., 0.], dtype=float32), array([0., 1.], dtype=float32)]
val_preds: [[0.9703391790390015, 0.10071626305580139, 1.0423606634140015, 0.05748504027724266], [0.838593602180481, 0.1463782787322998, 0.6760329008102417, 0.30962687730789185], [0.744438886642456, 0.3637859523296356, 0.7106611728668213, 0.22471392154693604], [0.7071610689163208, 0.4125683903694153, 0.9794442653656006, 0.083783358335495], [0.9152745008468628, 0.1329895406961441, 0.7842320799827576, 0.19534513354301453], [0.9120141267776489, 0.1621440351009369, 0.7320761680603027, 0.20885422825813293], [0.7093090415000916, 0.22028571367263794, 0.7356954216957092, 0.3602798283100128], [0.6743542551994324, 0.2141699194908142, 0.7626919746398926, 0.22027677297592163], [0.7416818737983704, 0.22701817750930786, 1.054550051689148, -0.0269180778414011], [0.8712937831878662, 0.3100886046886444, 0.771156370639801, 0.1835695207118988], [0.8048976063728333, 0.22056573629379272, 0.7882082462310791, 0.17406627535820007], [0.8669735789299011, 0.10078263282775879, 0.745419979095459, 0.2715674638748169], [0.8729959726333618, 0.09797818958759308, 0.6699813604354858, 0.4099329710006714], [0.6898970007896423, 0.24117258191108704, 0.7981094121932983, 0.21481645107269287], [0.7087931632995605, 0.40655359625816345, 1.039980411529541, 0.0033631175756454468], [0.6854578852653503, 0.34767311811447144, 0.7740055918693542, 0.26257508993148804], [0.8932803869247437, 0.4190457761287689, 1.050688624382019, 0.078736811876297], [0.8465979099273682, 0.11586499214172363, 0.9790703058242798, 0.08728978037834167], [0.8564683198928833, 0.15638728439807892, 0.8699896335601807, 0.14983908832073212], [0.7494888305664062, 0.408658504486084, 0.9229668378829956, 0.33701470494270325], [0.9698976278305054, 0.03964613005518913, 0.920474648475647, 0.12868590652942657], [0.8596698045730591, 0.1272944211959839, 1.1827764511108398, -0.033768180757761], [0.6624321937561035, 0.3573226034641266, 0.8676033020019531, 0.20064687728881836], [0.7572495937347412, 0.20920884609222412, 0.709422767162323, 0.22430536150932312], [0.9300236701965332, 0.054322388023138046, 0.8331695795059204, 0.1553637683391571], [1.0179489850997925, 0.032668959349393845, 0.8417826294898987, 0.18494492769241333], [0.810563325881958, 0.1836356520652771, 1.1081351041793823, -0.06599172949790955], [0.6468219757080078, 0.3458862900733948, 1.0146214962005615, 0.027930568903684616], [0.9468859434127808, 0.0652921199798584, 0.8776399493217468, 0.14643652737140656], [0.6590524911880493, 0.27201277017593384, 0.9705872535705566, 0.050376009196043015], [0.9655159711837769, 0.11080324649810791, 0.8375571966171265, 0.20295009016990662], [0.9372574090957642, 0.2209758758544922, 1.0080554485321045, 0.008454877883195877], [0.6932350397109985, 0.2274811863899231, 0.9273480176925659, 0.07786241173744202], [1.057912826538086, 0.006586160510778427, 1.040104627609253, 0.014642085880041122], [0.8281402587890625, 0.2791145443916321, 0.8740947246551514, 0.1683582365512848], [0.8653237223625183, 0.11307235062122345, 0.8575713634490967, 0.143876314163208], [0.9846587181091309, 0.11652521789073944, 0.7820221781730652, 0.1633124202489853], [0.9107496738433838, 0.22016048431396484, 1.0211989879608154, 0.08445832133293152], [0.9833457469940186, 0.08856514096260071, 0.8046919703483582, 0.2620636820793152], [0.9705270528793335, 0.016808580607175827, 0.6563172340393066, 0.2790708839893341], [0.8406693935394287, 0.16735345125198364, 0.8846079111099243, 0.27279847860336304], [0.957105278968811, 0.07565213739871979, 0.8096810579299927, 0.16623593866825104], [0.8899181485176086, 0.18695345520973206, 0.7108429670333862, 0.2769438326358795], [0.9080440998077393, 0.14104416966438293, 0.9763479232788086, 0.10297232866287231], [0.8038797378540039, 0.21278029680252075, 0.6580767631530762, 0.3093200922012329], [0.8384658694267273, 0.21153587102890015, 0.676618218421936, 0.30802637338638306], [0.9408621788024902, 0.04200815036892891, 0.7111213207244873, 0.3274080157279968], [0.6615606546401978, 0.3074386715888977, 0.8542405962944031, 0.18857811391353607], [0.7992740869522095, 0.2952626347541809, 0.9778791666030884, 0.05303778871893883], [1.000457525253296, 0.060699667781591415, 0.9569377899169922, 0.11249220371246338], [0.9100946187973022, 0.09765534102916718, 1.0501701831817627, -0.03090789169073105], [0.9058859348297119, 0.21034476161003113, 1.0467005968093872, -0.022340204566717148], [0.6906922459602356, 0.2607361674308777, 0.9418931007385254, 0.06455519795417786], [0.9470680952072144, 0.08296096324920654, 0.7873597145080566, 0.31279534101486206], [0.7743943929672241, 0.2555060088634491, 0.8511923551559448, 0.18559014797210693], [0.8840795755386353, 0.16196376085281372, 1.015209674835205, -0.02973618172109127], [0.8331042528152466, 0.18027056753635406, 0.8329649567604065, 0.22361087799072266], [0.9583555459976196, 0.07697772979736328, 0.9735144376754761, 0.010621603578329086], [0.7160120010375977, 0.2982387840747833, 0.9510098695755005, 0.07159459590911865], [0.8244237899780273, 0.23431620001792908, 0.8682907223701477, 0.1666729897260666], [0.8756235837936401, 0.11713951826095581, 0.8597825765609741, 0.21806007623672485], [0.8635329008102417, 0.17883995175361633, 0.8532373309135437, 0.17288777232170105], [0.8018333911895752, 0.28720346093177795, 0.7838162779808044, 0.1849384903907776], [0.7613508105278015, 0.1879979372024536, 0.6970540881156921, 0.2754090130329132], [0.7252262830734253, 0.3410550653934479, 0.9039632678031921, 0.1851716786623001], [0.8775975704193115, 0.13129699230194092, 0.7622255086898804, 0.2576081156730652], [0.969222903251648, 0.3792272210121155, 0.6954476237297058, 0.3117865324020386], [0.7083947062492371, 0.24843919277191162, 0.9812209606170654, 0.017906468361616135], [0.7311444282531738, 0.2816239595413208, 0.8033699989318848, 0.2799971103668213], [0.8524529933929443, 0.20604169368743896, 0.881869912147522, 0.22198814153671265], [1.0368562936782837, 0.013717826455831528, 0.7211535573005676, 0.2732298970222473], [0.7888091206550598, 0.1891593337059021, 0.7968697547912598, 0.2852882444858551], [0.949146032333374, 0.04876266047358513, 0.910932183265686, 0.11558353900909424], [0.8525163531303406, 0.11165854334831238, 1.0389907360076904, 0.037465836852788925], [0.8955913782119751, 0.12028580904006958, 0.7486259341239929, 0.281588077545166], [0.9198871850967407, 0.11760495603084564, 0.8880069851875305, 0.18898481130599976], [0.8992315530776978, 0.08698505163192749, 0.920428991317749, 0.07382453978061676], [0.9235795736312866, 0.10298559069633484, 0.8727779388427734, 0.11086001992225647], [1.0485525131225586, 0.11110140383243561, 0.7827389240264893, 0.1933434158563614], [1.1595699787139893, -0.08905604481697083, 0.7303086519241333, 0.2916055917739868], [0.8500824570655823, 0.17721334099769592, 0.9416561126708984, 0.4081572890281677], [0.8483530282974243, 0.11827713251113892, 0.724992036819458, 0.3550989031791687], [0.7318600416183472, 0.20816242694854736, 1.104979157447815, 0.04183315858244896], [1.0369869470596313, 0.10709759593009949, 0.9007052183151245, 0.06840313971042633], [0.8946923017501831, 0.2996341586112976, 0.6930220723152161, 0.25113052129745483], [0.7908887267112732, 0.2202128767967224, 0.6633474230766296, 0.36831367015838623], [1.063140630722046, -0.009017307311296463, 0.9030185341835022, 0.31602734327316284], [0.716485857963562, 0.2444121241569519, 0.882757306098938, 0.24701711535453796], [0.9329142570495605, 0.0986853539943695, 0.8481824994087219, 0.15331369638442993], [0.6938360929489136, 0.20301750302314758, 0.7464874386787415, 0.2771446406841278]]
Batch size (logits): 4, Batch size (labels): 4
Total Predictions: 90, Total Targets: 180
